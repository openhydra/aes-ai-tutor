MMEdu安装和下载
快速了解安装方式
根据对算力的依赖程度，MMEdu分为两个版本，分别是：基础版和完整版。
基础版既可以使用pip方式安装，也可以使用"XEdu一键安装包"。本版本集成了MMClassification（图像分类）和MMDetection（物体检测）两个模块，采用CPU训练（不需要GPU环境）。
完整版对算力要求较高，建议装在有英伟达显卡的电脑上，安装方式有pip安装和容器镜像安装两种。本版本集成了MMClassification（图像分类）、MMDetection（物体检测）、MMEditing（图像编辑）等模块，需要GPU环境支持。
1. 基础版的安装
1.1 一键安装包
为方便中小学教学，MMEdu团队提供了XEdu一键安装包。只要下载并解压即可直接使用。
现MMEdu一键安装包已升级为XEdu一键安装包，下文提到的MMEdu文件，应理解为XEdu一键安装包。
第一步：下载MMEdu（XEdu）最新版文件，并安装到本地，双击exe文件，选择安装路径后，点击Extract即可，安装后自动生成XEdu文件夹，文件夹内目录结构如下图所示。
1）下载方式（即XEdu下载方式）
飞书网盘：XEdu v1.6.7d.exe
第二步：您可以根据个人喜好，选择自己习惯的IDE。
1）使用XEdu自带的Thonny。
Thonny是一款好用的Python轻量级IDE。其最突出的两个特点便是是简洁性和交互性。打开根目录下的Thonny.bat文件快捷方式即可打开。使用Thonny打开"demo"文件夹中的py文件，如"MMEdu_cls_demo.py"，点击"运行"的"将文件作为脚本运行"即可运行代码，界面如下图所示。
2）使用XEdu自带的Jupyter。
Jupyter
Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。它相对简单，对用户也更加友好，适合初学者。打开根目录下的"jupyter编辑器.bat"，即自动启动浏览器并显示界面，如下图所示。
使用常用工具栏对代码进行操作，如"运行"，可以在单元格中编写文本或者代码（如下图中写了print("hello")代码的位置），执行代码的结果也将会在每个单元下方呈现。可以逐个运行单元格，每点击一次，仅运行一个单元格。单元格左侧[*]内的星号变为数字，表示该单元格运行完成。此时可打开"demo"文件夹中的ipynb文件，如"cls_notebook.ipynb"。
3）使用其他IDE。
如果您需要使用其他IDE，那么需要您自己配置Python编译器，配置方法如下。
配置环境路径
①打开您的IDE，如PyCharm、Thonny等。
②配置Python编译器，路径为解压路径下的"MMEdu"文件夹下的"mmedu"文件夹中的"python.exe"文件。
PyCharm环境路径配置如下图所示。
执行demo文件
用IDE打开解压路径下的py文件，如"cls_demo.py"，点击"运行"。运行效果应和pyzo一样。
4）使用cmd安装用户库。
python中最常用的库管理工具pip，可以使用cmd命令行来运行，打开根目录下的"启动cmd.bat"可以打开cmd命令行界面，如下图所示。
在其中输入想要安装的库文件即可，如"pip install rarfile"。
在2022年9月后，不再维护MMEdu一键安装包，统一更新为XEdu。XEdu于22年9月在世界人工智能大会正式发布，分MMEdu、BaseML、BaseNN三个功能模块，除了一键安装包，发布会上线了以pip方式安装的XEdu。
如pip install BaseML BaseNN BaseDT BaseDeploy MMEdu 
1.2 pip安装
当前，我们提供的MMEdu的安装是基础版的pip安装包，可以使用如下命令安装：
pip install mmedu或pip install MMEdu。
最新安装说明可以参见：这里
在这一步中，可能会有运行失败的情况，通常是由于依赖库安装失败导致的。
如果提示
mmcv相关的错误，可以尝试用：pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8.0/index.html
推荐使用Python3.8 （64位）来进行安装 。
如果仍然没有解决，请先卸载mmcv-full，然后再次安装。
第一步：pip uninstall mmcv-full -y
第二步：
pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8.0/index.html
该版本
不支持CPU训练检测模型，要想支持，需要升级mmdet库至2.23.0，具体步骤如下：
第一步：pip uninstall mmdet pycocotools-windows -y
第二步：pip install pycocotools
如果失败，可以考虑以下两种方案 ①下载whl进行本地安装（见
）；
②安装Visual Studio进行编译安装。
第三步：pip install mmdet==2.23.0
如果想要
升级为GPU版本，可以将torch、torchvision和mmcv-full做一次升级。步骤如下：
最新安装说明可以参见：这里
第一步：确认安装好与电脑GPU适配的CUDA驱动（这里以cuda10.1为例）
第二步：卸载当前安装的版本：pip uninstall torch torchvision mmcv-full -y
第三步：安装合适的版本：pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html
第四步：安装合适的mmcv-full：pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.8.0/index.html
至此，已经完成了对MMEdu的GPU升级。
2. GPU完整版安装之pip安装
最新安装说明可以参见：这里
视频演示参见：B站
3.1.2 安装python编辑器
若您已经安装好合适的python编辑器，该步骤可跳过。
此处以安装Thonny为例，其他编辑器例如Pycharm，VScode等也支持，用户自行配置好Python编译器即可。
下载
首先打开Thonny官网：https://thonny.org/
右上角选择合适的操作系统点击下载，此处以windows为例
安装
双击exe文件即可开始安装（一般下载完成后会自动打开安装界面无需点击exe文件，若没有自动打开安装页面再点击此exe文件）
打开安装界面后，依次选择Install for me only -> Next -> Next ->
Next -> Next -> Next -> Install -> Finish
运行
在安装好Thonny之后，在第一次运行的时候，会提示选择界面语言和初始设置，选择'Standard'模式即可。
配置Thonny的Python解释器
点击Thonny主界面右下角的Python版本号，可以选择对应的Python解释器，第一次配置点击Configure inter preter，弹出的窗口中，第一个下拉栏选择可选的python3解释器或虚拟环境，
第二个下拉栏找到自己之前安装的anaconda环境中的python解释器位置。点击确认即可使用该python解释器。
2.2 安装MMEdu(CPU版本)
最新安装说明可以参见：这里
2.2.1 Linux安装MMEdu
点击鼠标右键，打开终端。
终端中输入pip install MMEdu即可安装。
{.powershell}
$ pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8.0/index.html
注：为避免出现版本冲突，建议新建一个conda环境，并在新环境中执行以上命令（注：要求python\<3.9）。
{.powershell}
$ conda create -n your_env_name python=3.8
$ conda activate your_env_name
$ pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8.0/index.html
注：请将命令中的"your_env_name"换成你喜欢的名称，如"mmedu"。
2.2.2 Windows安装MMEdu
最新安装说明可以参见：这里
同时按下win+r，输入cmd，回车，打开一个命令行窗口。
在命令行中使用pip安装即可。
{.powershell}
$ pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8.0/index.html
注：为避免出现版本冲突，建议新建一个conda环境，并在新环境中执行以上命令（注：要求python\<3.9）。
{.powershell}
$ conda create -n your_env_name python=3.8
$ conda activate your_env_name
$ pip install MMEdu -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.8.0/index.html
2.3 安装MMEdu(GPU版本)
最新安装说明可以参见：这里
视频演示参见：B站
3. 完整版安装之docker容器
请参考XEdu的docker容器：https://xedu.readthedocs.io/zh/master/about/installation.html#docker
4. 查看MMEdu版本
打开python终端，执行以下命令即可查看MMEdu版本。当前最新版本是0.1.21。
5. 卸载MMEdu库
如果MMEdu库出现异常情况，可以尝试使用如下命令卸载MMEdu然后再使用install命令安装。
$ pip uninstall MMEdu -y

MMEdu基本功能
MMEdu功能简介
MMEdu是一个计算机视觉方向的深度学习开发工具，是一个用来训练AI模型的工具。
MMEdu和常见AI框架的比较
1）MMEdu和OpenCV的比较
OpenCV是一个开源的计算机视觉框架，MMEdu的核心模块MMCV基于OpenCV，二者联系紧密。
OpenCV虽然是一个很常用的工具，但是普通用户很难在OpenCV的基础上训练自己的分类器。MMEdu则是一个入门门槛很低的深度学习开发工具，借助MMEdu和经典的网络模型，只要拥有一定数量的数据，连小学生都能训练出自己的个性化模型。
2）MMEdu和MediaPipe的比较
MediaPipe 是一款由 Google Research 开发并开源的多媒体机器学习模型应用框架，支持人脸识别、手势识别和表情识别等，功能非常强大。MMEdu中的MMPose模块关注的重点也是手势识别，功能类似。但MediaPipe是应用框架，而不是开发框架。换句话说，用MediaPipe只能完成其提供的AI识别功能，没办法训练自己的个性化模型。
3）MMEdu和Keras的比较
Keras是一个高层神经网络API，是对Tensorflow、Theano以及CNTK的进一步封装。OpenMMLab和Keras一样，都是为支持快速实验而生。MMEdu则源于OpenMMLab，其语法设计借鉴过Keras。
相当而言，MMEdu的语法比Keras更加简洁，对中小学生来说也更友好。目前MMEdu的底层框架是Pytorch，而Keras的底层是TensorFlow（虽然也有基于Pytorch的Keras）。
4）MMEdu和FastAI的比较
FastAI（Fast.ai）最受学生欢迎的MOOC课程平台，也是一个PyTorch的顶层框架。和OpenMMLab的做法一样，为了让新手快速实施深度学习，FastAI团队将知名的SOTA模型封装好供学习者使用。
FastAI同样基于Pytorch，但是和OpenMMLab不同的是，FastAI只能支持GPU。考虑到中小学的基础教育中很难拥有GPU环境，MMEdu特意将OpenMMLab中支持CPU训练的工具筛选出来，供中小学生使用。
MMEdu基于OpenMMLab的基础上开发，因为面向中小学，优先选择支持CPU训练的模块。
MMEdu的内置模块概述
模块名称 
简称
功能
MMClassification
MMEduCls
图片分类 
MMDetection
MMEduDet
图片中的物体检测
MMGeneration
MMEduGen
GAN，风格化
MMPose
MMEduPose
骨架
MMEduEditing
MMEduSegmentation
像素级识别
MMEdu的内置SOTA模型
MMEdu内置了常见的SOTA模型，我们还在不断更新中。如需查看所有支持的SOTA模型，可使用model.sota()代码进行查看。
模块名称
内置模型
功能
MMClassification(MMEduCls)
LeNet、ResNet18、ResNet50、MobileNet
图片分类
MMDetection(MMEduDet)
FastRCNN、SSD_Lite、YOLO
目标检测
注：关于MMClassification支持的SOTA模型的比较可参考“解锁图像分类模块：MMClassification”中关于“支持的SOTA模型”的介绍，关于MMDetection支持的SOTA模型的比较可参考“揭秘目标检测模块：MMDetection”中关于“支持的SOTA模型”的介绍。关于这些SOTA模型更具体的介绍，请参考本文档的“深度学习知识库”部分的“经典网络模型介绍”。当然，通过“AI模型 + 关键词”的形式，你在很多搜索引擎中都能找到资料。
数据集支持
MMEdu系列提供了包括分类、检测等任务的若干数据集，存储在XEdu一键安装包中的dataset文件夹下。MMEdu支持的数据集格式如下：
1）ImageNet
ImageNet是斯坦福大学提出的一个用于视觉对象识别软件研究的大型可视化数据库，目前大部分模型的性能基准测试都在ImageNet上完成。MMEdu的图像分类模块支持的数据集类型是ImageNet，如需训练自己创建的数据集，数据集需整理成ImageNet格式。
ImageNet格式数据集文件夹结构如下所示，包含三个文件夹和三个文本文件，图像数据文件夹内，不同类别图片按照文件夹分门别类排好，通过trainning_set、val_set、test_set区分训练集、验证集和测试集。文本文件classes.txt说明类别名称与序号的对应关系，val.txt说明验证集图片路径与类别序号的对应关系，test.txt说明测试集图片路径与类别序号的对应关系。
plain
imagenet
├── training_set
│   ├── class_0
│   │   ├── filesname_0.JPEG
│   │   ├── filesname_1.JPEG
│   │   ├── ...
│   ├── ...
│   ├── class_n
│   │   ├── filesname_0.JPEG
│   │   ├── filesname_1.JPEG
│   │   ├── ...
├── classes.txt
├── val_set
│   ├── ...
├── val.txt
├── test_set
│   ├── ...
├── test.txt
如上所示训练数据根据图片的类别，存放至不同子目录下，子目录名称为类别名称。
classes.txt包含数据集类别标签信息，每行包含一个类别名称，按照字母顺序排列。
plain
class_0
class_1
...
class_n
为了验证和测试，我们建议划分训练集、验证集和测试集，因此另外包含“val.txt”和“test.txt”这两个标签文件，要求是每一行都包含一个文件名和其相应的真实标签。格式如下所示：
plain
filesname_0.jpg 0
filesname_1.jpg 0
...
filesname_a.jpg n
filesname_b.jpg n
注：真实标签的值应该位于[0,类别数目-1]之间。
如果您觉得整理规范格式数据集有点困难，您只需收集完图片按照类别存放，然后完成训练集（trainning_set）、验证集（val_set）和测试集（test_set）等的拆分，整理在一个大的文件夹下作为你的数据集。此时指定数据集路径后同样可以训练模型，因为XEdu拥有检查数据集的功能，如您的数据集缺失txt文件，会自动帮您生成“classes.txt”，“val.txt”等（如存在对应的数据文件夹）开始训练。这些txt文件会生成在您指定的数据集路径下，即帮您补齐数据集。完整的从零开始制作一个ImageNet格式的数据集的步骤详见深度学习知识库。
2）COCO
COCO数据集是微软于2014年提出的一个大型的、丰富的检测、分割和字幕数据集，包含33万张图像，针对目标检测和实例分割提供了80个类别的物体的标注，一共标注了150万个物体。MMEdu的MMDetection支持的数据集类型是COCO，如需训练自己创建的数据集，数据集需转换成COCO格式。
MMEdu的目标检测模块设计的COCO格式数据集文件夹结构如下所示，“annotations”文件夹存储标注文件，“images”文件夹存储用于训练、验证、测试的图片。
plain
coco
├── annotations
│   ├── train.json
│   ├── ...
├── images
│   ├── train
│   │   ├── filesname_0.JPEG
│   │   ├── filesname_1.JPEG
│   │   ├── ...
│   ├── ...
如果您的文件夹结构和上方不同，则需要在“Detection_Edu.py”文件中修改load_dataset方法中的数据集和标签加载路径。
COCO数据集的标注信息存储在“annotations”文件夹中的json文件中，需满足COCO标注格式，基本数据结构如下所示。
```plain
全局信息
{
    "images": [image],
    "annotations": [annotation],
    "categories": [category]
}
图像信息标注，每个图像一个字典
image {
    "id": int,  # 图像id编号，可从0开始
    "width": int, # 图像的宽
    "height": int,  # 图像的高
    "file_name": str, # 文件名
}
检测框标注，图像中所有物体及边界框的标注，每个物体一个字典
annotation {
    "id": int,  # 注释id编号
    "image_id": int,  # 图像id编号
    "category_id": int,   # 类别id编号
    "segmentation": RLE or [polygon],  # 分割具体数据，用于实例分割
    "area": float,  # 目标检测的区域大小
    "bbox": [x,y,width,height],  # 目标检测框的坐标详细位置信息
    "iscrowd": 0 or 1,  # 目标是否被遮盖，默认为0
}
类别标注
categories [{
    "id": int, # 类别id编号
    "name": str, # 类别名称
    "supercategory": str, # 类别所属的大类，如哈巴狗和狐狸犬都属于犬科这个大类
}]
```
为了验证和测试，我们建议划分训练集、验证集和测试集，需要生成验证集valid和标注文件valid.json，测试集test和标注文件test.json，json文件的基本数据结构依然是COCO格式。制作一个COCO格式的数据集的步骤详见深度学习知识库。
使用示例
文档涉及的部分代码见XEdu帮助文档配套项目集：https://www.openinnolab.org.cn/pjlab/project?id=64f54348e71e656a521b0cb5&sc=645caab8a8efa334b3f0eb24#public
模型推理：
此处展示的是图像分类模型的模型推理的示例代码，如需了解更多模块的示例代码或想了解更多使用说明请看后文。
python
from MMEdu import MMClassification as cls
img = './img.png'
model = cls(backbone='ResNet18')
checkpoint = './latest.pth'
result = model.inference(image=img, show=True, checkpoint = checkpoint)
model.print_result(result)
从零开始训练：
此处展示的是图像分类模型的从零开始训练的示例代码，如需了解更多模块的示例代码或想了解更多使用说明请看后文。
python
from MMEdu import MMClassification as cls
model = cls(backbone='ResNet18')
model.num_classes = 3
model.load_dataset(path='./dataset')
model.save_fold = './my_model'
model.train(epochs=10,validate=True)
继续训练：
此处展示的是图像分类模型的继续训练的示例代码，如需了解更多模块的示例代码或想了解更多使用说明请看后文。
python
from MMEdu import MMClassification as cls
model = cls(backbone='ResNet18')
model.num_classes = 3
model.load_dataset(path='./dataset')
model.save_fold = './my_model'
checkpoint = './latest.pth'
model.train(epochs=10, validate=True, checkpoint=checkpoint)
更多示例：
1.查看MMEdu库所在的目录
进入Python终端，然后依次输入如下代码即可查看Python库所在的目录（site-packages）。
python
import MMEdu
print(MMEdu.__path__)
2.查看权重文件信息
模型训练好后生成了日志文件和（.pth）权重文件，可以使用如下代码查看权重文件信息。
python
pth_info(checkpoint) # 指定为pth权重文件路径
3.返回日志信息
如需返回日志信息，可在训练时使用如下代码：
python
log = model.train(xxx)
print(log)
返回的是日志文件中各行信息组成的列表。
4.库文件源代码可以从PyPi下载，选择tar.gz格式下载，可查看库文件原码和更多示例程序。

图像处理模块：MMEditing
开发中，敬请期待

解锁图像分类模块：MMEduCls
初识MMEduCls
MMEduCls（简称cls）的主要功能是对图像进行分类。其支持的SOTA模型有LeNet、MobileNet、ResNet18、ResNet50等，具体介绍详见后文。如需查看所有支持的SOTA模型，可导入模块后使用cls.sota()代码进行查看。
文档涉及的部分代码见XEdu帮助文档配套项目集：https://www.openinnolab.org.cn/pjlab/project?id=64f54348e71e656a521b0cb5&sc=645caab8a8efa334b3f0eb24#public
使用说明
XEdu一键安装包中预置了MMEdu的图像分类模块的示例代码（路径：/demo）、常用小数据集（路径：/dataset/cls）,并且已经预训练了一些权重（路径：/checkpoints/cls_model）。在demo文件夹中，还提供了一张测试图片，OpenInnoLab平台也公开了非常多图像分类任务的项目，体验了几个之后相信会对此模块有一定理解。
下面我们将以“石头剪刀布”手势识别这个任务为例，介绍一下图像分类模块示例代码的用法，在解锁图像分类模块的同时也一起完成一个新的图像分类项目吧！
0. 导入模块
```python
用别名让代码变得简洁
from MMEdu import MMClassification as mmeducls
用更加简单的别名
from MMEdu import MMClassification as cls
```
1. 模型训练
使用下面的代码即可简单体验图像分类模型的训练过程，接下来就开始详细的介绍。
在运行代码之前，我们首先需要拥有一个数据集，这里提供了经典的石头剪刀布分类数据集。
数据集文件结构如下:
hand_gray数据集符合MMEdu图像分类模块支持的数据集要求，文件夹中包含三个图片文件夹，test_set,training_set,val_set分别存储测试集，训练集和验证集的图片；还有三个txt文件，其中classes.txt记录该数据集的类别，test.txt和val.txt分别记录测试集和验证集的图片名。若想要了解更多数据集格式的内容，可参考数据集支持部分。
训练代码如下：
python
model = cls('LeNet') # 实例化模型，不指定参数即使用默认参数。
model.num_classes = 3 # 指定数据集中的类别数量
model.load_dataset(path='../dataset/cls/hand_gray') # 从指定数据集路径中加载数据
model.save_fold = '../checkpoints/cls_model/hand_gray' # 设置模型的保存路径
model.train(epochs=10, validate=True) # 设定训练的epoch次数以及是否进行评估
通过注释，我们可以清晰的理解每句代码的功能，模型训练过程可以概括为5步骤，每个步骤分别对应一行代码：
实例化模型
python
model = mmeducls('LeNet') # 实例化模型，'LeNet'是sota模型的名称
这里对于MMEdu图像分类模块提供的参数进行解释，支持传入的参数是backbone（骨干网络）。也可以写成“backbone='LeNet'”，强化一下，这是一个网络的名称。
backbone：指定使用的图像分类模型。可选的有LeNet、MobileNet、ResNet18、ResNet50等，具体介绍详见后文。
指定类别数量
python
model.num_classes = 3 # 指定数据集中的类别数量
加载数据集
python
model.load_dataset(path='../dataset/cls/hand_gray') # 从指定数据集路径中加载数据
load_dataset()的作用是修改模型中关于数据集路径的配置文件，从而确保我们在训练时不会找错文件，该函数可传入的参数有两个：
path：训练数据集的路径。
check：布尔值，默认为True，控制是否检查数据集中每张图像是否损坏。由于检查需要花费一定时间，若已经在前面步骤中确认数据集没有问题，可以设置为False来省去检查的时间。
指定模型参数存储位置
python
model.save_fold = '../checkpoints/cls_model/hand_gray'
模型训练
python
model.train(epochs=10, validate=True) # 设定训练的epoch次数以及是否进行评估
epochs=10表示训练10个轮次，validate=True表示在训练结束后用验证集（val_set）进行评估。
参数详解
train函数支持很多参数，为了降低难度，MMEdu已经给绝大多数的参数设置了默认值。根据具体的情况修改参数，可能会得到更好的训练效果。下面来详细说明train函数的各个参数。
epochs：默认参数为100，用于指定训练的轮次，而在上述代码中我们设置为10。
batch_size：批大小，一次训练所选取的样本数，指每次训练在训练集中取batch_size个样本训练。默认参数为None，如为None则默认为对应网络配置文件中设置的samples_per_gpu的值，用于指定一次训练所选取的样本数。当训练集样本非常多时，直接将这些数据输入到神经网络的话会导致计算量非常大，容易因内存不足导致内核挂掉，因此可引入batch_size参数的设置。关于batch_size的取值范围，应该大于类别数，小于样本数，且由于GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优。
validate：布尔值，只能为True或者False，默认参数为True，在训练结束后，设定是否需要在验证集上进行评估，True则是需要进行评估。
random_seed：随机种子策略，默认为0即不使用，使用随机种子策略会减小模型算法结果的随机性。
save_fold：模型的保存路径，参数为None，默认保存路径为./checkpoints/cls_model/，如果不想模型保存在该目录下，可自己指定路径。
distributed：布尔值，表示是否在分布式环境中训练该模型，默认为False。
device：训练时所使用的设备，默认为'cpu'，如果电脑支持GPU，也可以将参数修改为'cuda'，使用GPU进行推理。
optimizer：进行迭代时的优化器，默认参数为SGD，SGD会在训练的过程中迭代计算mini-bath的梯度。可选参数：['SGD','Adam','Adagrad']。
lr：学习率，默认参数为1e-2即0.01，指定模型进行梯度下降时的步长。简单解释就是，学习率过小，训练过程会很缓慢，学习率过大时，模型精度会降低。可以根据需要设置不同的lr大小。
checkpoint：指定使用的模型权重文件，默认参数为None，如果没有指定模型权重文件，那么我们将会使用默认的模型权重文件进行推理。
topk：元组，验证top-k准确率的策略。validate=True时有效。topk准确率的详细介绍见下图。当类别数量class<=5时，默认topk=(1,)，class>5时，默认topk=(5,)。这里我们可以自定义参数值，传入一个元组，如(1,2,3)，表示同时验证top-1、top-2、top-3的准确率，这样可以有效帮助我们评估模型的总体效果。
执行上述代码之后的运行结果如下图：
而在checkpoints\cls_model文件夹中我们会发现多了两种文件，一个是***.log.json文件，它记录了我们模型在训练过程中的一些参数，比如说学习率lr，所用时间time，以及损失loss等；另一个文件是***.pth文件，这个是我们在训练过程中所保存的模型。
准确率怎么看？
方式一：通过训练输出（如上图），运行训练代码时输出项里会出现学习率lr，所用时间time，以及损失loss，每一轮在验证集上的accuracy_top-**等。
方式二：通过日志文件，在训练过程中我们会发现模型保存路径下（代码中指定指定）出现一个***.log.json文件，这就是日志文件，它记录了我们模型在训练过程中的一些信息。
当您启动验证集验证，即设置validate=True，表示每轮（每个epoch）训练后，在验证集（val_set）上测试一次准确率。那么每一轮训练结束时会呈现一次准确率，并且会生成best_accuracy_top-*.pth权重文件即最佳准确率权重文件。
accuracy_top-1：对一张图片，如果你的预测结果中概率最大的那个分类正确，则认为正确，再根据分类正确的样本数除以所有的样本数计算得到的准确率。
accuracy_top-5：对一张图片，如果预测概率前五名的答案中出现了正确答案，便认为正确，再根据分类正确的样本数除以所有的样本数计算得到的准确率，在MMEdu的图像分类模块中，如果类别数量大于5会启动accuracy_top-5准确率。
日志文件解读
Epoch[1][10/838]: 1表示当前是第1个epoch，而10/838表示当前正在处理第10个批次，一共有838个批次。在深度学习模型的训练过程中，通常会将训练数据集分成若干个批次，每个批次包含一定数量的样本（每批次样本数和batch_size设置相关），训练时会使用这些批次逐步迭代来更新模型的参数。
lr: 学习率。
eta: 表示预计完成整个训练所需要的时间。
time: 表示本批次训练需要的时间。
data_time: 数据预处理的时间。
memory: 训练时占据内存或现存的大小。
loss: 本批次模型在训练集上计算的损失值。loss是衡量模型在训练集上预测结果与真实结果之间差异的指标。不同类型的模型（如分类、回归、生成等）使用不同的loss函数来优化模型，MMEdu的图像分类模型一般使用交叉熵损失函数。通常情况下，训练过程中的loss会逐渐下降，表示模型在逐步学习优化。
2. 模型推理
训练完模型后，我们就可使用该模型对新图片进行模型推理。当然如果想快速上手体验MMEdu的图像分类，可直接使用我们已经预训练好的模型和权重文件体验图片推理。
示例代码如下:
python
img = 'testrock01-02.png' # 指定待推理的图片路径
model = cls('LeNet') # 实例化图像分类模型
model.checkpoint='../checkpoints/cls_model/hand_gray/latest.pth' # 指定使用的模型权重文件
result = model.inference(image=img, show=True, checkpoint=checkpoint) # 在CPU上进行推理
model.print_result() # 输出结果，可以修改参数show的值来决定是否需要显示结果图片，默认显示结果图片
运行结果如图：
标签: 1， 置信度: 0.69， 预测结果: 'rock'
推理结果图片（带标签的图片）会以原来的文件名称保存在代码文件的同级目录下的cls_result文件夹下，如果运行代码前没有发现该文件夹，不用担心，系统会自动建立。当然，我们可以自己指定保存文件夹的名称。
此外，我们还可以对一组图片进行批量推理，只需将收集的图片放在一个文件夹下，如在demo文件夹下新建一个cls_testIMG文件夹放图片。批量推理的示例代码如下。
python
img = 'cls_testIMG/' # 指定进行推理的一组图片的路径
model = cls('LeNet') # 实例化MMEdu图像分类模型
model.checkpoint='../checkpoints/cls_model/hand_gray/latest.pth' # 指定使用的模型权重文件
result = model.inference(image=img, show=True, checkpoint=checkpoint) # 在CPU上进行推理
model.print_result(result) # 输出结果，可以修改参数show的值来决定是否需要显示结果图片，默认显示结果图片
运行上述代码之后，‘cls_result’文件夹里就会出现这组图片的推理结果图，推理图片名称和原图片同名。
模型推理的过程也可以概括为6步骤，每个步骤分别对应一行代码：
- 图片准备
python
img = 'testrock01-02.png' # 指定推理图片的路径，直接在代码所在的demo文件夹中选择图片
如果使用自己的图片的话，只需要修改img的路径即可（绝对路径和相对路径均可）
实例化模型
python
model = cls('LeNet') # 实例化MMEdu图像分类模型
这里对于MMEdu图像分类模型提供的参数进行解释，支持传入的参数是backbone。
backbone：指定使用的sota模型名称，默认参数是'LeNet'，当然读者可以自行修改该参数以使用不同模型。
指定模型权重文件
python
model.checkpoint='../checkpoints/cls_model/hand_gray/latest.pth' # 指定使用的模型权重文件
此时指定的模型权重文件首先需存在，并且需和实例化模型对应，训练时实例化的网络是什么，推理时也需实例化同一个网络。如果没有指定模型权重文件，那么这两句代码可以不修改，即使用默认的模型。
模型推理
python
model.inference(image=img, show=True, checkpoint=checkpoint) # 在cpu上进行推理
将所需要推理图片的路径传入inference函数中即可进行推理，我们这里传入了四个参数，image代表的就是推理图片的路径，show代表是否需要显示结果图片，class_path代表训练集的路径，checkpoint代表指定使用的模型权重文件。
- 图片准备
python
img = 'testrock01-02.png' # 指定推理图片的路径，直接在代码所在的demo文件夹中选择图片
参数详解
在MMEdu中对于inference函数还有其他的传入参数，在这里进行说明：
device：推理所用的设备，默认为'cpu'，如果电脑支持GPU，也可以将参数修改为'cuda'，使用GPU进行推理。
checkpoint：指定使用的模型权重文件，默认参数为None，如果没有指定模型权重文件，那么我们将会使用默认的模型权重文件进行推理。
image：推理图片的路径。
show：布尔值，默认为True，表示推理后是否显示推理结果
save_fold：保存的图片名，数据结构为字符串，默认参数为'cls_result'，用户也可以定义为自己想要的名字。
快速推理
针对部分用户希望加快推理速度的需求，设计了fast_inference函数，主要方法是使用load_checkpoint提前加载权重文件。
python
model.load_checkpoint(checkpoint=checkpoint)
result = model.fast_inference(image=img)
参数详解
load_checkpoint函数的传入参数：
device：推理所用的设备，默认为'cpu'，如果电脑支持GPU，也可以将参数修改为'cuda'，使用GPU进行推理。
checkpoint：指定使用的模型权重文件，默认参数为None，如果没有指定模型权重文件，那么我们将会使用默认的模型权重文件进行推理。
fast_inference函数的传入参数：
image：推理图片的路径。
show：布尔值，默认为True，表示推理后是否显示推理结果。
save_fold：保存的图片名，数据结构为字符串，默认参数为'cls_result'，用户也可以定义为自己想要的名字。
verbose：布尔值，默认为True，控制是否输出推理进度提示信息。如果循环执行时，不想要看到进度信息，而是用你自己的变量和条件判断来实现，可以设置verbose为False。
3. 继续训练
在这一步中，我们将学习如何加载之前训练过的模型接着训练。如果觉得之前训练的模型epoch数不够的话或者因为一些客观原因而不得不提前结束训练，相信下面的代码会帮到您。
python
model = cls('LeNet') # 初始化实例模型
model.num_classes = 3 # 指定数据集中的类别数量
model.load_dataset(path='../dataset/cls/hand_gray') # 配置数据集路径
model.save_fold = '../checkpoints/cls_model/hand_gray' # 设置模型的保存路径
checkpoint = '../checkpoints/cls_model/hand_gray/latest.pth' # 指定使用的模型权重文件
model.train(epochs=50, validate=True, checkpoint=checkpoint) # 进行再训练
这里我们有一个参数在之前的训练模型过程中没有详细说明，那就是train函数中的checkpoint参数，这个放到这里就比较好理解，它的意思是指定需要进行再训练的模型路径，当然您也可以根据你需要训练的不同模型而调整参数。
我们还可以指定网上下载的某个预训练模型。通过借助在大型数据集上训练的预训练模型，来对新的任务进行训练，而无需从头开始训练。它可以将一个大型数据集中的知识和技能转移到另一个任务上，从而大大节省训练时间。 
全新开始训练一个模型一般要花较长时间，所以我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样，基于预训练模型继续训练可起到加速训练的作用。在学习资源下载处也提供了一些预训练模型和权重文件下载。
4. 支持的SOTA模型
目前MMEdu的图像分类模块支持的SOTA模型有LeNet、MobileNet、ResNet18、ResNet50等，如需查看所有支持的SOTA模型，可导入模块后使用cls.sota()代码进行查看。这些模型的作用和适用场景简介如下。
LeNet
适用于灰度图像识别。
MobileNet
适用于绝大多数的图像识别，支持1000个分类。
ResNet
广泛应用于分类、分割、检测等问题，结构简单，效果拔群。
各个SOTA模型的比较：
LeNet是一种简单的深度卷积神经网络，他的特色就是参数量少、计算小，训练模型很快，确定层数少，不能充分学习数据的特征，LeNet比较适合图像比较简单的图像分类，通常像素值超过224的图片或者彩色图片分类建议选择MobileNet和ResNet。点击下方SOTA模型介绍链接可以学习更多模型知识。
序号
SOTA模型介绍
1
LeNet
2
MobileNet
3
ResNet
4
更多
MMEduCls的高级操作
源代码的修改（以修改验证策略为例）
所有库文件均以py文件形式开源，用户可以自行编辑修改。这样极大提升了代码的可玩性。这里以修改源码中的验证策略为例，展示如果修改源代码，实现自定义功能或功能修改。
由于源码限制，分类类别超过5类之后，默认的验证方式变为了top-5的准确率(accuracy_top-5)，无法看到top1的准确率。如下图所示：
为此，我们可以在train函数中加入topk参数修改验证top-k准确率的策略（见上文），设置topk=(1,)或topk=(1,2,5)。也可以使用修改源码的方式，在源码中将其改为'topk': (1,)，这样就可以看到top1的准确率，也可以改为其他数字，同时也支持(1,2,5)这样的写法。然后将修改之后的源码覆盖原始源码后重新导入库文件。
查看库文件所在路径
    python
    import MMEdu
    print(MMEdu.__path__)
覆盖原来的库文件
   python
    !cp ./Classification_Edu.py /usr/local/python3.8/site-packages/MMEdu/Classification/Classification_Edu.py
重新导入库（重启内核）

揭秘目标检测模块：MMEduDet
初识目标检测和MMEduDet
初识目标检测任务
相信你已经体验并完成了一些图像分类任务，但是图像分类任务在真实的场景中应用中，像只出现一只猫的图片情况较少，如果一张图片里有两只猫，图像分类的模型可能可以识别出是猫。但是如果是这张图又有猫又有狗，那图像分类模型就肯定识别不出来了。为了解决这一种问题，就出现了目标检测算法。对于图像分类任务来讲，AI仅需正确识别图片的类别即可。
目标检测(Detection)任务有两个主要的子任务：定位和分类。
定位任务是什么呢？ 定位是找到图像主体或者某个物体的位置，然后用一个框将其框起来。这个框可以是矩形的，也可以是多边形或者圆形的，通常用得最多的是矩形（如下图）。
总的来说，目标检测任务是图像分类任务的进阶任务，图像分类任务只有一个子任务：分类，而目标检测任务有两个任务：定位和分类，按照图像中目标的数量可分为单目标检测和多目标检测。目标检测任务比起分类任务多出来了一个定位，即将目标框起来的步骤。
初识MMEduDet
那么MMEdu的目标检测模块MMEduDet（简称det）的主要功能便是输出图片或视频中出现的多个对象名称，同时用方框框出对象所在方形区域。
其支持的SOTA模型有FasterRCNN、YOLO、SSD_Lite等，具体介绍详见后文。如需查看所有支持的SOTA模型，可导入模块后使用det.sota()代码进行查看，网络名称不区分大小写，YOLO网络所指版本为v3版本。
文档涉及的部分代码见XEdu帮助文档配套项目集：https://www.openinnolab.org.cn/pjlab/project?id=64f54348e71e656a521b0cb5&sc=645caab8a8efa334b3f0eb24#public
使用说明
XEdu一键安装包中预置了MMEdu的det模块的示例代码（路径：/demo）、常用小数据集（路径：/dataset/det）,并且已经预训练了一些权重（路径：/checkpoints/det_model）。在demo文件夹中，还提供了一张测试图片，OpenInnoLab平台也公开了非常多目标检测任务的项目，体验了几个之后相信会对此模块有一定理解。
下面我们将以车牌检测这个任务为例，介绍一下目标检测模块示例代码的用法，一起解锁目标检测模块吧！
0. 导入模块
```python
用别名让代码变得简洁
from MMEdu import MMDetection as mmedudet
用更加简单的别名
from MMEdu import MMDetection as det
```
1.模型训练
使用下面的代码即可简单体验目标检测模块的训练过程，接下来就开始详细的介绍。
在运行代码之前，我们同样需要先拥有一个数据集，这里我们提供了车牌检测数据集plate。
数据集文件结构如下：
车牌数据集符合MMEdu目标检测模块支持的数据集要求，文件夹中包含两个文件夹annotations和images，分别存储标注信息以及图片数据，每个文件夹下面有train和valid两个json文件。了解更多数据集格式的内容，可参考数据集支持部分。
训练代码如下：
~~~python
model = det('FasterRCNN') # 实例化模型，不指定参数即使用默认参数
model.num_classes = 1 # 进行车牌识别，此时只有一个类别。
model.load_dataset(path='../dataset/det/coco') # 从指定数据集路径中加载数据
model.save_fold = '../checkpoints/det_model/plate' # 设置模型的保存路径
model.train(epochs=3, validate=True) # 设定训练的epoch次数以及是否进行评估
~~~
接下来逐句讲述训练代码：
实例化模型
python
model = det('FasterRCNN') # 初始化实例模型
这里对于MMEdu目标检测模块提供的参数进行解释，支持传入的参数是backbone`。也可以写成“backbone='FasterRCNN'”，强化一下，这是一个网络的名称。
backbone：指定使用的目标检测模型。可选的有FasterRCNN、YOLO、SSD_Lite等，具体介绍详见后文。
指定类别数量
~~~python
model.num_classes = 1 # 进行车牌识别，此时只有一个类别
~~~
加载数据集
~~~python
model.load_dataset(path='../dataset/det/coco') # 从指定数据集路径中加载数据
~~~
load_dataset()的作用是修改模型中关于数据集路径的配置文件，从而确保我们在训练时不会找错文件，该函数可传入的参数有两个：
path：训练数据集的路径。
check：布尔值，默认为True，控制是否检查数据集中每张图像是否损坏。由于检查需要花费一定时间，若已经在前面步骤中确认数据集没有问题，可以设置为False来省去检查的时间。
指定模型参数存储位置
~~~python
model.save_fold = '../checkpoints/det_model/plate' # 设置模型的保存路径
~~~
模型训练
~~~python
model.train(epochs=10, validate=True) # 设定训练的epoch次数以及是否进行评估
~~~
epochs=10表示训练10个轮次，validate=True表示在训练结束后用验证集进行评估。
参数详解
train函数支持很多参数，为了降低难度，MMEdu已经给绝大多数的参数设置了默认值。根据具体的情况修改参数，可能会得到更好的训练效果。下面来详细说明train函数的各个参数。
random_seed：随机种子策略，默认为0即不使用，使用随机种子策略会减小模型算法结果的随机性。
save_fold：模型的保存路径，默认参数为./checkpoints/det_model/，如果不想模型保存在该目录下，可自己指定路径。
distributed：布尔值，只能为True或者False，默认参数为False，设为True时即使用分布式训练。
epochs：默认参数为100，用于指定训练的轮次，而在上述代码中我们设置为10。
batch_size：批大小，一次训练所选取的样本数，指每次训练在训练集中取batch_size个样本训练。默认参数为None，如为None则默认为对应网络配置文件中设置的samples_per_gpu的值，用于指定一次训练所选取的样本数。当训练集样本非常多时，直接将这些数据输入到神经网络的话会导致计算量非常大，容易因内存不足导致内核挂掉，因此可引入batch_size参数的设置。关于batch_size的取值范围，应该大于类别数，小于样本数，且由于GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优。
validate：布尔值，只能为True或者False，默认参数为True，在训练结束后，设定是否需要在校验集上进行评估，True则是需要进行评估。
optimizer：进行迭代时的优化器，默认参数为SGD，SGD会在训练的过程中迭代计算mini-batch的梯度。可选参数：['SGD','Adam','Adagrad']
lr：学习率，默认参数为1e-3即0.001，指定模型进行梯度下降时的步长。简单解释就是，学习率过小，训练过程会很缓慢，学习率过大时，模型精度会降低。
weight_decay：权值衰减参数，用来调节模型复杂度对损失函数的影响，防止过拟合，默认值为1e-3即0.001。
checkpoint: 默认为'None'，表示在训练过程中使用初始化权重。如果使用训练得到的模型（或预训练模型），此参数传入一个模型路径，我们的训练将基于传入的模型参数继续训练。
执行上述代码之后的运行结果如下图:
此时，打开model.save_fold指定的checkpoints/det_model/plate文件夹，我们会发现多了两种文件，一种是**.log.json日志文件，它记录了我们模型在训练过程中的一些参数，比如说学习率lr，所用时间time，损失loss，以及评估指标bbox_mAP等；另一种文件是.pth文件，这个是我们在训练过程中所保存的模型权重文件，分为按照训练轮次生成的权重文件epoch_x.pth和一个best_bbox_mAP_epoch_**.pth权重文件，best_bbox_mAP_epoch_**.pth权重文件即目前为止准确率最高的权重。
日志解读：
Epoch[1][10/838]: 1表示当前是第1个epoch，而10/838表示当前正在处理第10个批次，一共有838个批次。在深度学习模型的训练过程中，通常会将训练数据集分成若干个批次，每个批次包含一定数量的样本，训练时会使用这些批次逐步迭代来更新模型的参数。
eta: 表示预计完成整个训练所需要的时间。
time: 表示本批次训练需要的时间。
data_time: 数据预处理的时间。
memory: 训练时占据内存或现存的大小。
loss_bbox：评估模型预测边界框的精度的指标，通常loss_bbox越小表示预测出的边框和标注的越接近。
loss_cls：衡量目标检测任务中分类性能的损失函数，一般用于衡量模型预测的类别与真实类别之间的差距。先对每个预测的边界框分别预测一个类别，然后使用 loss_cls 计算每个框内分类预测的损失，通常loss_cls 越小，每个框内的分类结果越准确。
bbox_mAP：对目标检测任务，bbox_mAP可用来衡量检测准确度，指模型预测的边界框和真实边界框之间的重合度。
loss: 本批次模型在训练集上计算的损失值。loss是衡量模型在训练集上预测结果与真实结果之间差异的指标。
2.模型推理
完成模型训练后，我们就可使用训练好的模型对新图片进行模型推理。若想快速上手体验MMEdu的目标检测，可直接使用我们已经预训练好的模型和权重文件进行图片推理。
示例代码如下:
```python
img = 'car_plate.png' # 指定进行推理的图片路径，我们使用demo文件夹中提供的图片
model = det('FasterRCNN') # 初始化实例模型
checkpoint = '../checkpoints/det_model/plate/latest.pth' # 指定使用的模型权重文件
result = model.inference(image=img, show=True, checkpoint = checkpoint) # 在CPU上进行推理
model.print_result() # 输出结果
同时您可以修改show的值来决定是否需要显示结果图片，此处默认显示结果图片
```
运行结果如图：
推理结果图片（带标签的图片）会以原本的文件名称保存在代码文件的同级目录下的det_result文件夹下，如果运行前没有发现该文件夹，不用担心，系统会自动建立。当然我们也可以自己指定保存文件夹的名称。
此外，我们还可以对一组图片进行批量推理，只需将收集的图片放在一个文件夹下，如在demo文件夹下新建一个cls_testIMG文件夹放图片。批量推理的示例代码如下。
~~~python
img = 'det_testIMG/' # 指定进行推理的一组图片的路径
model = det('FasterRCNN') # 初始化实例模型
checkpoint = '../checkpoints/det_model/plate/latest.pth' # 指定使用的模型权重文件
result = model.inference(image=img, show=True, checkpoint = checkpoint) # 在CPU上进行推理
model.print_result() # 输出结果
同时您可以修改show的值来决定是否需要显示结果图片，此处默认显示结果图片
~~~
推理结果如下：
返回的数据类型是一个字典列表（很多个字典组成的列表）类型的变量，内置的字典表示分类的结果和边框的位置，如“[{'类别标签': 0, '置信度': 0.6336591, '坐标': {'x1': 97, 'y1': 150, 'x2': 230, 'y2': 188}}]”，我们可以用字典访问其中的元素。同时您会发现当前目录下‘det_result’文件夹里出现了这组图片的推理结果图，每张图片的结果与您收集的图片同名，您可以到这个文件夹下查看推理结果。
接下来讲述推理代码规则：
图片准备
python
img = 'car_plate.png' # 指定推理图片的路径，直接在代码所在的demo文件夹中选择图片
如果使用自己的图片的话，只需要修改img的路径即可（绝对路径和相对路径均可）。
实例化模型并指定模型权重文件
python
model = det('FasterRCNN') # 初始化实例模型
checkpoint = '../checkpoints/det_model/plate/latest.pth' # 指定使用的模型权重文件
实例化模型的代码在前面说过就不再赘述。推理时需实例化模型并指定已有的模型权重文件，两句代码需相互匹配，训练时实例化的网络是什么，推理时也需实例化同一个网络。如果没有指定模型权重文件，那么这两句代码可以不修改，即使用默认的模型。
模型推理
python
model.inference(image=img, show=True, checkpoint = checkpoint) # 在CPU上进行推理
将所需要推理图片的路径传入inference函数中即可进行推理，我们这里传入了四个参数，image代表的就是推理图片的路径，show代表是否需要显示结果图片，checkpoint代表指定使用的模型权重文件。
参数详解
在MMEdu中对于inference函数还有其他的传入参数，在这里进行说明：
device：推理所用的设备，默认为'cpu'，如果电脑支持GPU，也可以将参数修改为'cuda:0'，使用GPU进行推理。
checkpoint：指定使用的模型权重文件，默认参数为None，如果没有指定模型权重文件，那么我们将会使用默认的模型权重文件进行推理。 
image：推理图片的路径。
show：布尔值，默认为True，表示推理后是否显示推理结果。
rpn_threshold & rcnn_threshold: 0～1之间的数值。由于FasterRCNN为一个两阶段的检测模型，这两个参数分别表示两个阶段对于检测框的保留程度，高于这个数值的框将会被保留（这里如果同学们设置过低，也可能会发现图中出现了多个框）。
save_fold：保存的图片名，数据结构为字符串，默认参数为'det_result'，用户也可以定义为自己想要的名字。
（最后两个参数的使用，我们将在下一部分进行详细举例解释）。
快速推理
针对部分用户希望加快推理速度的需求，设计了fast_inference函数，主要方法是使用load_checkpoint提前加载权重文件。
python
model.load_checkpoint(checkpoint=checkpoint)
result = model.fast_inference(image=img)
参数详解
load_checkpoint函数的传入参数：
device：推理所用的设备，默认为'cpu'，如果电脑支持GPU，也可以将参数修改为'cuda'，使用GPU进行推理。
checkpoint：指定使用的模型权重文件，默认参数为None，如果没有指定模型权重文件，那么我们将会使用默认的模型权重文件进行推理。
fast_inference函数的传入参数：
image：推理图片的路径。
show：布尔值，默认为True，表示推理后是否显示推理结果。
save_fold：保存的图片名，数据结构为字符串，默认参数为'cls_result'，用户也可以定义为自己想要的名字。
verbose：布尔值，默认为True，控制是否输出推理进度提示信息。如果循环执行时，不想要看到进度信息，而是用你自己的变量和条件判断来实现，可以设置verbose为False。
3.继续训练
在这一步中，我们将学习如何加载之前训练过的模型接着训练。如果觉得之前训练的模型epoch数不够的话或者因为一些客观原因而不得不提前结束训练，相信下面的代码会帮到您。
~~~python
model = det('FasterRCNN') # 初始化实例模型
model.num_classes = 1  # 进行车牌识别，此时只有一个类别。
model.load_dataset(path='../dataset/det/coco') # 配置数据集路径
model.save_fold = '../checkpoints/det_model/plate' # 设置模型的保存路径
checkpoint='../checkpoints/det_model/plate/latest.pth' # 指定使用的模型权重文件
model.train(epochs=3, validate=True, checkpoint=checkpoint) # 进行再训练
~~~
这里我们有一个参数在之前的训练模型过程中没有详细说明，那就是train函数中的checkpoint参数，这个放到这里就比较好理解，它的意思是指定需要进行再训练的模型路径，当然您也可以根据你需要训练的不同模型而调整参数。
我们还可以指定网上下载的某个预训练模型。通过借助在大型数据集上训练的预训练模型，来对新的任务进行训练，而无需从头开始训练。它可以将一个大型数据集中的知识和技能转移到另一个任务上，从而大大节省训练时间。 
全新开始训练一个模型一般要花较长时间，所以我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样，基于预训练模型继续训练可起到加速训练的作用。在学习资源下载处也提供了一些预训练模型和权重文件下载。
另外为初学者准备了一个快速入门MMEdu的目标检测项目的基础项目《用MMEdu实现车牌目标检测》，项目地址是：https://openinnolab.org.cn/pjlab/project?id=641426fdcb63f030544017a2&sc=62f34141bf4f550f3e926e0e#public（用Chrome浏览器打开体验最佳）
4.支持的SOTA模型
目前MMEdu的目标检测模块支持的SOTA模型有SSD_Lite、FaterRCNN、YOLO等，如需查看所有支持的SOTA模型，可导入模块后使用det.sota()代码进行查看，网络名称不区分大小写，YOLO网络所指版本为v3版本。这些模型的作用和适用场景简介如下。
SSD_Lite
相比Faster RCNN有明显的速度优势，相比YOLO又有明显的mAP优势。
FasterRCNN
采用双阶检测方法，可以解决多尺度、小目标问题，通用性强。
YOLO
只进行一次检测，相比于FasterRCNN速度较快，适用于复杂的目标检测问题。
序号
SOTA模型介绍
1
SSD_Lite
2
  FasterRCNN
3
YOLO
4
更多
内容补充：单目标检测和多目标检测是什么？
根据图像中目标的数量，目标检测分为单目标检测和多目标检测，单目标检测顾名思义，就是识别图像中的单独的一个目标。如下图，就用红框框出了一只猫。
多目标检测就像下面这张图，识别图中的多个目标，在下图中分别用红色的框框出来了猫和绿色的框框出来了狗。多目标检测通常比单目标检测更难，因为它需要同时处理多个目标。但是，多目标检测在很多场景中都是必要的。
小问题：单目标检测一定是只有一个类别吗？多目标检测一定是多个类别吗？
答：单目标检测和多目标检测的主要区别是图像中目标的数量，而与类别数量是无关的。

最后一步：模型转换
训练好了模型该如何快速应用呢？在别的设备安装MMEdu似乎很麻烦。得益于OpenMMLab系列工具的不断进步与发展。MMEdu通过集成OpenMMLab开源的模型部署工具箱MMDeploy和模型压缩工具包MMRazor，打通了从算法模型到应用程序这 “最后一公里”！ 
我们也希望通过本系列教程，带领大家学会如何把自己使用MMEdu训练的计算机视觉任务SOTA模型部署到ONNXRuntime、NCNN等各个推理引擎上。
借助MMEdu完成模型转换
MMEdu内置了一个convert函数，来实现了一键式模型转换，转换前先了解一下转换要做的事情吧。
转换准备：
待转换的模型权重文件（用MMEdu训练）。
需要配置两个信息：
待转换的模型权重文件（checkpoint）和输出的文件（out_file）。
模型转换的典型代码：
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog/best_accuracy_top-1_epoch_2.pth'
out_file="catdog.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
model.convert函数中有四个参数可以设置：
checkpoint(string) - 必需参数，选择想要进行模型转换的权重文件，以.pth为后缀。
out_file(string) - 必需参数，指定模型转换后的输出文件路径。
opset_version(int) - 可选参数，设置模型算子集的版本，默认为11。
ir_version(int) - 可选参数，设置模型转化中间表示的版本。指定中间表示（Intermediate Representation, 简称 IR）规范的版本，一个整数（int）类型的参数。在计算机编程中，中间表示是一种数据结构或代码，它介于原始代码和机器码之间。它通常用于编译器或类似工具中，作为转换和优化代码的一个步骤。指定中间表示的版本，可方便根据不同的需求和优化目标选择最合适的 IR 规范。当前可选范围为1～12，默认为6。
类似的，目标检测模型转换的示例代码如下：
python
from MMEdu import MMDetection as det
model = det(backbone='SSD_Lite')
checkpoint = 'checkpoints/COCO-80/ssdlite.pth'
out_file="COCO-80.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
参考项目：MMEdu模型转换
全流程体验
现在，让我们从“从零开始训练猫狗识别模型并完成模型转换”项目入手，见识一下使用MMEdu工具完成从模型训练到模型部署的基本流程吧！
1.准备数据集
思考自己想要解决的分类问题后，首先要收集数据并整理好数据集。例如想要解决猫狗识别问题，则需准备猫狗数据集。
2.模型训练
训练一个全新的模型，一般要花较长时间。因此我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样。
如下代码是使用基于MobileNet网络训练的猫狗识别预训练模型，模型训练是在预训练模型基础上继续训练。基于预训练模型继续训练可起到加速训练的作用，并且通常会使得模型达到更好的效果。
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
model.load_dataset(path='/data/TC4V0D/CatsDogsSample') 
model.save_fold = 'checkpoints/cls_model/CatsDog1' 
model.train(epochs=5, checkpoint='checkpoints/pretrain_model/mobilenet_v2.pth' ,batch_size=4, lr=0.001, validate=True,device='cuda')
3.推理测试
使用MMEdu图像分类模块模型推理的示例代码完成模型推理。返回的数据类型是一个字典列表（很多个字典组成的列表）类型的变量，内置的字典表示分类的结果，如“{'标签': 0, '置信度': 0.9417100548744202, '预测结果': 'cat'}”，我们可以用字典访问其中的元素。巧用预测结果设置一些输出。如：
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog1/best_accuracy_top-1_epoch_1.pth'
img_path = '/data/TC4V0D/CatsDogsSample/test_set/cat/cat0.jpg'
result = model.inference(image=img_path, show=True, checkpoint = checkpoint,device='cuda')
x = model.print_result(result)
print('标签（序号）为：',x[0]['标签'])
if x[0]['标签'] == 0:
    print('这是小猫，喵喵喵！')
else:
    print('这是小猫，喵喵喵！')
4.模型转换
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog1/best_accuracy_top-1_epoch_1.pth'
out_file='out_file/cats_dogs.onnx'
model.convert(checkpoint=checkpoint, out_file=out_file)
此时项目文件中的out_file文件夹下便生成了模型转换后生成的两个文件，可打开查看。一个是ONNX模型权重，一个是示例代码，示例代码稍作改动即可运行（XEduHub）。
硬件上需安装的库：
XEduHub（pip install xedu-python）
需上传到硬件的文件：
1）out_file文件夹（内含模型转换生成的两个文件）。
新建一个代码文件，将out_file文件夹中的py文件中的代码稍作修改用于代码运行（当然也可以直接运行）。
生成的示例代码：
```python
from XEdu.hub import Workflow as wf
import numpy as np
模型声明
mm = wf(task='mmedu',checkpoint='cats_dogs.onnx')
待推理图像，此处仅以随机数组为例
image = 'cat0.jpg'
模型推理
res,img = mm.inference(data=image,img_type='cv2')
标准化推理结果
result = mm.format_output(lang="zh")
可视化结果图像
mm.show(img)
```
拓展：模型转换在线版
MMDeploy还推出了模型转换工具网页版本，支持更多后端推理框架，具体使用步骤如下。
点击MMDeploy硬件模型库，后选择模型转换
点击新建转换任务
选择需要转换的模型类型、模型训练配置，并点击上传模型上传本地训练好的.pth权重文件，具体的选项如下表所示
MMEdu模型名称
功能
OpenMMlab算法
模型训练配置
MobileNet
图像分类
mmcls v1.0.0rc5
configs/mobilenet_v2/mobilenet-v2_8xb32_in1k.py
RegNet
图像分类
mmcls v1.0.0rc5
configs/regnet/regnetx-400mf_8xb128_in1k.py
RepVGG
图像分类
mmcls v1.0.0rc5
configs/repvgg/deploy/repvgg-A0_deploy_4xb64-coslr-120e_in1k.py
ResNeXt
图像分类
mmcls v1.0.0rc5
configs/resnext/resnext50-32x4d_8xb32_in1k.py
ResNet18
图像分类
mmcls v1.0.0rc5
configs/resnet/resnet18_8xb32_in1k.py
ResNet50
图像分类
mmcls v1.0.0rc5
configs/resnet/resnet50_8xb32_in1k.py
ShuffleNet_v2
图像分类
mmcls v1.0.0rc5
configs/shufflenet_v2/shufflenet-v2-1x_16xb64_in1k.py
VGG
图像分类
mmcls v1.0.0rc5
configs/vgg/vgg19_8xb32_in1k.py
FasterRCNN
目标检测
mmdet-det v3.0.0rc5
configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py
Mask_RCNN
目标检测
mmdet-det v3.0.0rc5
configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py
SSD_Lite
目标检测
mmdet-det v3.0.0rc5
configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py
Yolov3
目标检测
mmdet-det v3.0.0rc5
configs/yolo/yolov3_d53_320_273e_coco.py
选择需要的目标runtime，可选的有ncnn,ort1.8.1(onnxruntime),openvino等，点击提交任务
点击提交任务后，状态会变为排队中，或处理中，如果转换失败会提示错误日志，根据错误日志提示修改，像下图错误的原因是使用ResNet50（分类）的权重，可对应的OpenMMLab算法误选为了mmdet（检测）的，所以提示的错误是找不到配置文件
转换成功后，点击下载模型即可使用
What：什么现象与成果
精度测试结果
软硬件环境
操作系统：Ubuntu 16.04
系统位数：64
处理器：Intel i7-11700 @ 2.50GHz * 16
显卡：GeForce GTX 1660Ti
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理
精度测试结果汇总
图像分类
模型
数据集
权重大小
精度（TOP-1）
精度（TOP-5）
FP32
INT8
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
70.94%
68.30%
89.99%
88.44%
ResNet18
ImageNet
44.7 MB
69.93%
89.29%
ResNet50
ImageNet
97.8 MB
24.6 MB
74.93%
74.77%
92.38%
92.32%
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
69.36%
66.15%
88.32%
86.34%
VGG
ImageNet
527.8 MB
101.1 MB
72.62%
72.32%
91.14%
90.97%
ImageNet 数据集：ImageNet项目是一个用于视觉对象识别软件研究的大型可视化数据库。ImageNet项目每年举办一次软件比赛，即ImageNet大规模视觉识别挑战赛（ILSVRC），软件程序竞相正确分类检测物体和场景。 ImageNet挑战使用了一个“修剪”的1000个非重叠类的列表。2012年在解决ImageNet挑战方面取得了巨大的突破
准确度（Top-1）：排名第一的类别与实际结果相符的准确率
准确度（Top-5）：排名前五的类别包含实际结果的准确率
目标检测
模型
数据集
权重大小
精度（mAP）
FP32
INT8
FP32
INT8
SSD_Lite
COCO
28.1 MB
8.5 MB 
0.2303
0.2285
FasterRCNN
COCO
168.5 MB
42.6 MB
0.3437
0.3399
Mask_RCNN
COCO
169.7 MB
45.9 MB
0.3372
0.3340
Yolov3
COCO
237 MB
61 MB
0.2874
0.2688
COCO 数据集: MS COCO的全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。 COCO数据集是一个大型的、丰富的物体检测，分割和字幕数据集。这个数据集以scene understanding为目标，目前为止有语义分割的最大数据集，提供的类别有80类，有超过33万张图片，其中20万张有标注，整个数据集中个体的数目超过150万个。
AP (average Precision)：平均精度，在不同recall下的最高precision的均值(一般会对各类别分别计算各自的AP)
mAP（mean AP）:平均精度的均值，各类别的AP的均值
边、端设备测试结果
PC机测试
用于模型训练的机器，性能较优，常见的操作系统有Windows和Linux
软硬件环境
操作系统：Ubuntu 16.04
系统位数：64
处理器：Intel i7-11700 @ 2.50GHz * 16
显卡：GeForce GTX 1660Ti
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理 
测试时，计算各个数据集中 10 张图片的平均耗时
下面是我们环境中的测试结果：
图像分类
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
201
217
ResNet18
ImageNet
44.7 MB
62
ResNet50
ImageNet
97.8 MB
24.6 MB
29
43
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
244
278
VGG
ImageNet
527.8 MB
101.1 MB
6
15
吞吐量 (图片数/每秒)：表示每秒模型能够识别的图片总数，常用来评估模型的表现
*：不建议部署，单张图片推理的时间超过30s
目标检测
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
SSD_Lite*
COCO
28.1 MB
8.5 MB 
37
53
SSD_Lite**
COCO
28.1 MB
8.5 MB 
FasterRCNN
COCO
168.5 MB
42.6 MB
Mask_RCNN
COCO
169.7 MB
45.9 MB
Yolov3
COCO
237 MB
61 MB
3
6
*：后端支持网络为MobileNetv1，性能弱于以MobileNetv2为后端推理框架的版本
**：后端支持网络为MobileNetv2，即MMEdu中SSD_Lite选用的版本，可从参数对比中得出其精度、准确度、模型大小均优于以MobileNetv1为后端推理框架的SSD_Lite
行空板测试
行空板, 青少年Python教学用开源硬件，解决Python教学难和使用门槛高的问题，旨在推动Python教学在青少年中的普及。官网：https://www.dfrobot.com.cn/
软硬件环境
操作系统：Linux
系统位数：64
处理器：4核单板AArch64 1.20GHz
内存：512MB
硬盘：16GB
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理 
测试时，计算各个数据集中 10 张图片的平均耗时
下面是我们环境中的测试结果：
图像分类
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
1.77
4.94
ResNet18
ImageNet
44.7 MB
0.46
ResNet50
ImageNet
97.8 MB
24.6 MB
0.22
0.58
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
3.97
8.51
VGG
ImageNet
527.8 MB
101.1 MB
*
*
吞吐量 (图片数/每秒)：表示每秒模型能够识别的图片总数，常用来评估模型的表现
*：不建议部署，单张图片推理的时间超过30s
目标检测
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
SSD_Lite*
COCO
28.1 MB
8.5 MB 
0.55
1.30
SSD_Lite**
COCO
28.1 MB
8.5 MB 
FasterRCNN
COCO
168.5 MB
42.6 MB
Mask_RCNN
COCO
169.7 MB
45.9 MB
Yolov3
COCO
237 MB
61 MB
0.026
0.066
*：后端支持网络为MobileNetv1，性能弱于以MobileNetv2为后端推理框架的版本
**：后端支持网络为MobileNetv2，即MMEdu中SSD_Lite选用的版本，可从参数对比中得出其精度、准确度、模型大小均优于以MobileNetv1为后端推理框架的SSD_Lite
树莓派（4b）测试
Raspberry Pi。中文名为“树莓派”,简写为RPi，或者RasPi（RPi）是为学生计算机编程教育而设计，卡片式电脑，其系统基于Linux。
软硬件环境
操作系统：Linux
系统位数：32
处理器：BCM2711 四核 Cortex-A72(ARM v8) @1.5GHz
内存：4G
硬盘：16G
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理 
测试时，计算各个数据集中 10 张图片的平均耗时
下面是我们环境中的测试结果：
图像分类
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
6.45
ResNet18
ImageNet
44.7 MB
3.20
ResNet50
ImageNet
97.8 MB
24.6 MB
1.48
2.91
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
19.11
10.85*
VGG
ImageNet
527.8 MB
101.1 MB
0.43
0.44
吞吐量 (图片数/每秒)：表示每秒模型能够识别的图片总数，常用来评估模型的表现
*：量化后在树莓派上推理速度变慢
目标检测
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
SSD_Lite*
COCO
28.1 MB
8.5 MB 
2.55
SSD_Lite**
COCO
FasterRCNN
COCO
168.5 MB
42.6 MB
Mask_RCNN
COCO
169.7 MB
45.9 MB
Yolov3
COCO
237 MB
61 MB
0.21
0.34
*：后端支持网络为MobileNetv1，性能弱于以MobileNetv2为后端推理框架的版本
**：后端支持网络为MobileNetv2，即MMEdu中SSD_Lite选用的版本，可从参数对比中得出其精度、准确度、模型大小均优于以MobileNetv1为后端推理框架的SSD_Lite
注：硬件测试模块持续更新中，如有更多硬件测试需求，请联系我们
多模态交互
回顾用AI解决真实问题的流程图，我们已经介绍了收集数据、训练模型、模型推理和应用部署。结合项目设计，我们还会去思考如何通过摄像头获得图像，如何控制灯光发亮，如何操纵舵机，如何设计显示界面UI等需要使用输入设备和输出设备等来实现的交互设计，即对多模态交互的考量。
更多传感器、执行器使用教程参见：DFRobot
更多模型转换和应用的教程详见后文。

最后一步：AI模型转换
得益于OpenMMLab系列工具的不断进步与发展。MMEdu通过集成OpenMMLab开源的模型部署工具箱MMDeploy和模型压缩工具包MMRazor，打通了从算法模型到应用程序这 “最后一公里”！ 
今天我们将开启AI模型部署入门系列教程，在面向中小学AI教育的开发和学习工具 XEdu 的辅助下，介绍以下内容：
模型转换
模型量化（MMEdu暂未合并该功能）
多模态交互
MMEdu已经可以帮助我们DIY自己的AI模型了，为什么还要多此一举、徒增难度，来学习需要更多编程知识的模型部署模块？
面对新知识，我们总会有困惑与畏难的时候，无法确定这项技术能否为我所用、产生效益，担心付出与收获不成正比。但是没有关系！我们将用黄金圈法则：Why、How、What，来带领大家一起寻找答案。
我们也希望通过本系列教程，带领大家学会如何把自己使用MMEdu训练的计算机视觉任务SOTA模型部署到ONNXRuntime、NCNN等各个推理引擎上。
行空板上部署MMEdu训练模型效果示例：
Why：为什么
为什么要进行模型转换
用MMEdu训练的模型，只能运行在安装了MMEdu环境的电脑吗？
不。借助模型转换，MMEdu（包括BaseNN）训练的模型都可以转换为ONNX模型，然后部署在很多常见的电脑（开源硬件）上。
模型转换是为了模型能在不同框架间流转。在实际应用时，模型转换几乎都用于工业部署，负责模型从训练框架到部署侧推理框架的连接。 这是因为随着深度学习应用和技术的演进，训练框架和推理框架的职能已经逐渐分化。 分布式、自动求导、混合精度……训练框架往往围绕着易用性，面向设计算法的研究员，以研究员能更快地生产高性能模型为目标。 硬件指令集、预编译优化、量化算法……推理框架往往围绕着硬件平台的极致优化加速，面向工业落地，以模型能更快执行为目标。由于职能和侧重点不同，没有一个深度学习框架能面面俱到，完全一统训练侧和推理侧，而模型在各个框架内部的表示方式又千差万别，所以模型转换就被广泛需要了。
概括： 训练框架大，塞不进两三百块钱买的硬件设备中，推理框架小，能在硬件设备上安装。要把训练出的模型翻译成推理框架能读懂的语言，才能在硬件设备上运行
为什么要进行模型量化
模型量化是指将深度学习模型中的参数、激活值等数据转化为更小的数据类型（通常是8位整数或者浮点数），以达到模型大小减小、计算速度加快、内存占用减小等优化目的的技术手段。模型量化有以下几个优点：减小模型大小、加速模型推理、减少内存占用等。因此，模型量化可以帮助提高深度学习模型的效率和性能，在实际应用中具有重要的价值和意义。
概括： 对模型采用合适的量化，能在对准确率忽略不计的情况下，让模型更小、更快、更轻量。比如原先168 MB的模型量化后大小变为了42.6 MB，推理速度提高了两倍。
为什么要进行多模态交互
多模态交互是指利用多个感知通道（例如语音、图像、触觉、姿态等）进行交互的技术。多模态交互在人机交互、智能交通、健康医疗、教育培训等领域都有广泛的应用、在提高交互效率、用户体验、解决单模态限制和实现智能化交互等方面具有重要的作用和价值。
概括： 多模态交互给你的AI作品加点创客料
什么是推理框架
深度学习推理框架是一种让深度学习算法在实时处理环境中提高性能的框架。常见的有ONNXRuntime、NCNN、TensorRT、OpenVINO等。
ONNXRuntime是微软推出的一款推理框架，支持多种运行后端包括CPU，GPU，TensorRT，DML等，是对ONNX模型最原生的支持。
NCNN是腾讯公司开发的移动端平台部署工具，一个为手机端极致优化的高性能神经网络前向计算框架。NCNN仅用于推理，不支持学习。
值得注意的是，包括Pytorch、Tensorflow，以及国内的百度PaddlePaddle、华为的MindSpore等主流的深度学习框架都开发了工具链来回应这个Why。我们采用业界主流的方法，以更高代码封装度的形式来解决这一问题。接下来，且听我们对利用XEdu进行How：怎么做的流程娓娓道来。
How：怎么做
总结一下Why中的回应，在软件工程中，部署指把开发完毕的软件投入使用的过程，包括环境配置、软件安装等步骤。类似地，对于深度学习模型来说，模型部署指让训练好的模型在特定环境中运行的过程。相比于软件部署，模型部署会面临更多的难题：
运行模型所需的环境难以配置。深度学习模型通常是由一些框架编写，比如 PyTorch、TensorFlow。由于框架规模、依赖环境的限制，这些框架不适合在手机、开发板等生产环境中安装。 
深度学习模型的结构通常比较庞大，需要大量的算力才能满足实时运行的需求。模型的运行效率需要优化。 因为这些难题的存在，模型部署不能靠简单的环境配置与安装完成。
经过工业界和学术界数年的探索，结合XEdu的工具，下图展示了模型部署的一条流行流水线：
这条流水线解决了模型部署中的两大问题：使用对接深度学习框架和推理引擎的中间表示。开发者不必担心如何在新环境中运行各个复杂的框架；通过中间表示的网络结构优化和推理引擎对运算的底层优化，使模型的运算效率大幅提升。
借助MMEdu完成模型转换
MMEdu内置了一个convert函数，来实现了一键式模型转换，转换前先了解一下转换要做的事情吧。
转换准备：
待转换的模型权重文件（用MMEdu训练）。
需要配置两个信息：
待转换的模型权重文件（checkpoint）和输出的文件（out_file）。
模型转换的典型代码：
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog/best_accuracy_top-1_epoch_2.pth'
out_file="catdog.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
这段代码是完成分类模型的转换，接下来对为您model.convert函数的各个参数：
checkpoint：选择想要进行模型转换的权重文件，以.pth为后缀。
out_file：模型转换后的输出文件路径。
类似的，目标检测模型转换的示例代码如下：
from MMEdu import MMDetection as det
model = det(backbone='SSD_Lite')
checkpoint = 'checkpoints/COCO-80/ssdlite.pth'
out_file="COCO-80.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
参考项目：MMEdu模型转换
借助BaseDeploy完成模型部署
XEdu团队推出了模型部署工具BaseDeploy，该工具可以轻松完成模型部署。对MMEdu训练的模型完成转换后，生成ONNX模型，可借助BaseDeploy库部署到硬件上。
示例代码如下：
import cv2
import BaseDeploy as bd
model_path = 'cls.onnx'
cap = cv2.VideoCapture(0)
ret, img = cap.read()
model = bd(model_path)
result = model.inference(img)
model.print_result(result)
cap.release()
更多关于BaseDePloy库的介绍和使用说明可见BaseDeploy：服务于XEdu的模型部署工具。
现在，让我们从“从零开始训练猫狗识别模型并完成模型转换”项目入手，见识一下使用MMEdu工具完成从模型训练到模型部署的基本流程吧！
1.准备数据集
思考自己想要解决的分类问题后，首先要收集数据并整理好数据集。例如想要解决猫狗识别问题，则需准备猫狗数据集。
2.模型训练
训练一个全新的模型，一般要花较长时间。因此我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样。
如下代码是使用基于MobileNet网络训练的猫狗识别预训练模型，模型训练是在预训练模型基础上继续训练。基于预训练模型继续训练可起到加速训练的作用，并且通常会使得模型达到更好的效果。
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
model.load_dataset(path='/data/TC4V0D/CatsDogsSample') 
model.save_fold = 'checkpoints/cls_model/CatsDog1' 
model.train(epochs=5, checkpoint='checkpoints/pretrain_model/mobilenet_v2.pth' ,batch_size=4, lr=0.001, validate=True,device='cuda')
3.推理部署
使用MMEdu图像分类模块模型推理的示例代码完成模型推理。返回的数据类型是一个字典列表（很多个字典组成的列表）类型的变量，内置的字典表示分类的结果，如“{'标签': 0, '置信度': 0.9417100548744202, '预测结果': 'cat'}”，我们可以用字典访问其中的元素。巧用预测结果设置一些输出。如：
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog1/best_accuracy_top-1_epoch_1.pth'
img_path = '/data/TC4V0D/CatsDogsSample/test_set/cat/cat0.jpg'
result = model.inference(image=img_path, show=True, checkpoint = checkpoint,device='cuda')
x = model.print_result(result)
print('标签（序号）为：',x[0]['标签'])
if x[0]['标签'] == 0:
    print('这是小猫，喵喵喵！')
else:
    print('这是小猫，喵喵喵！')
4.模型转换
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog1/best_accuracy_top-1_epoch_1.pth'
out_file='out_file/cats_dogs.onnx'
model.convert(checkpoint=checkpoint, out_file=out_file)
此时项目文件中的out_file文件夹下便生成了模型转换后生成的两个文件，可打开查看。一个是ONNX模型权重，一个是示例代码，示例代码稍作改动即可运行（需配合BaseData.py的BaseDT库）。
硬件上需安装的库：
BaseDeploy
需上传到硬件的文件：
1）out_file文件夹（内含模型转换生成的两个文件）。
新建一个代码文件，将out_file文件夹中的py文件中的代码稍作修改用于代码运行（当然也可以直接运行）。
示例代码：
```
import cv2
import BaseDeploy as bd
model_path = 'out_file/cats_dogs.onnx'
cap = cv2.VideoCapture(0)
ret, img = cap.read()
model = bd(model_path)
result = model.inference(img)
result = model.print_result(result)
if result['预测结果'] == 'dog':
    print('这是小狗，汪汪汪！')
else:
    print('这是小猫，喵喵喵！')
cap.release()
```
拓展：模型转换在线版
MMDeploy还推出了模型转换工具网页版本，支持更多后端推理框架，具体使用步骤如下。
点击MMDeploy硬件模型库，后选择模型转换
点击新建转换任务
选择需要转换的模型类型、模型训练配置，并点击上传模型上传本地训练好的.pth权重文件，具体的选项如下表所示
MMEdu模型名称
功能
OpenMMlab算法
模型训练配置
MobileNet
图像分类
mmcls v1.0.0rc5
configs/mobilenet_v2/mobilenet-v2_8xb32_in1k.py
RegNet
图像分类
mmcls v1.0.0rc5
configs/regnet/regnetx-400mf_8xb128_in1k.py
RepVGG
图像分类
mmcls v1.0.0rc5
configs/repvgg/deploy/repvgg-A0_deploy_4xb64-coslr-120e_in1k.py
ResNeXt
图像分类
mmcls v1.0.0rc5
configs/resnext/resnext50-32x4d_8xb32_in1k.py
ResNet18
图像分类
mmcls v1.0.0rc5
configs/resnet/resnet18_8xb32_in1k.py
ResNet50
图像分类
mmcls v1.0.0rc5
configs/resnet/resnet50_8xb32_in1k.py
ShuffleNet_v2
图像分类
mmcls v1.0.0rc5
configs/shufflenet_v2/shufflenet-v2-1x_16xb64_in1k.py
VGG
图像分类
mmcls v1.0.0rc5
configs/vgg/vgg19_8xb32_in1k.py
FasterRCNN
目标检测
mmdet-det v3.0.0rc5
configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py
Mask_RCNN
目标检测
mmdet-det v3.0.0rc5
configs/mask_rcnn/mask_rcnn_r50_fpn_1x_coco.py
SSD_Lite
目标检测
mmdet-det v3.0.0rc5
configs/ssd/ssdlite_mobilenetv2_scratch_600e_coco.py
Yolov3
目标检测
mmdet-det v3.0.0rc5
configs/yolo/yolov3_d53_320_273e_coco.py
选择需要的目标runtime，可选的有ncnn,ort1.8.1(onnxruntime),openvino等，点击提交任务
点击提交任务后，状态会变为排队中，或处理中，如果转换失败会提示错误日志，根据错误日志提示修改，像下图错误的原因是使用ResNet50（分类）的权重，可对应的OpenMMLab算法误选为了mmdet（检测）的，所以提示的错误是找不到配置文件
转换成功后，点击下载模型即可使用
What：什么现象与成果
精度测试结果
软硬件环境
操作系统：Ubuntu 16.04
系统位数：64
处理器：Intel i7-11700 @ 2.50GHz * 16
显卡：GeForce GTX 1660Ti
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理
精度测试结果汇总
图像分类
模型
数据集
权重大小
精度（TOP-1）
精度（TOP-5）
FP32
INT8
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
70.94%
68.30%
89.99%
88.44%
ResNet18
ImageNet
44.7 MB
69.93%
89.29%
ResNet50
ImageNet
97.8 MB
24.6 MB
74.93%
74.77%
92.38%
92.32%
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
69.36%
66.15%
88.32%
86.34%
VGG
ImageNet
527.8 MB
101.1 MB
72.62%
72.32%
91.14%
90.97%
ImageNet 数据集：ImageNet项目是一个用于视觉对象识别软件研究的大型可视化数据库。ImageNet项目每年举办一次软件比赛，即ImageNet大规模视觉识别挑战赛（ILSVRC），软件程序竞相正确分类检测物体和场景。 ImageNet挑战使用了一个“修剪”的1000个非重叠类的列表。2012年在解决ImageNet挑战方面取得了巨大的突破
准确度（Top-1）：排名第一的类别与实际结果相符的准确率
准确度（Top-5）：排名前五的类别包含实际结果的准确率
目标检测
模型
数据集
权重大小
精度（mAP）
FP32
INT8
FP32
INT8
SSD_Lite
COCO
28.1 MB
8.5 MB 
0.2303
0.2285
FasterRCNN
COCO
168.5 MB
42.6 MB
0.3437
0.3399
Mask_RCNN
COCO
169.7 MB
45.9 MB
0.3372
0.3340
Yolov3
COCO
237 MB
61 MB
0.2874
0.2688
COCO 数据集: MS COCO的全称是Microsoft Common Objects in Context，起源于微软于2014年出资标注的Microsoft COCO数据集，与ImageNet竞赛一样，被视为是计算机视觉领域最受关注和最权威的比赛之一。 COCO数据集是一个大型的、丰富的物体检测，分割和字幕数据集。这个数据集以scene understanding为目标，目前为止有语义分割的最大数据集，提供的类别有80类，有超过33万张图片，其中20万张有标注，整个数据集中个体的数目超过150万个。
AP (average Precision)：平均精度，在不同recall下的最高precision的均值(一般会对各类别分别计算各自的AP)
mAP（mean AP）:平均精度的均值，各类别的AP的均值
边、端设备测试结果
PC机测试
用于模型训练的机器，性能较优，常见的操作系统有Windows和Linux
软硬件环境
操作系统：Ubuntu 16.04
系统位数：64
处理器：Intel i7-11700 @ 2.50GHz * 16
显卡：GeForce GTX 1660Ti
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理 
测试时，计算各个数据集中 10 张图片的平均耗时
下面是我们环境中的测试结果：
图像分类
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
201
217
ResNet18
ImageNet
44.7 MB
62
ResNet50
ImageNet
97.8 MB
24.6 MB
29
43
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
244
278
VGG
ImageNet
527.8 MB
101.1 MB
6
15
吞吐量 (图片数/每秒)：表示每秒模型能够识别的图片总数，常用来评估模型的表现
*：不建议部署，单张图片推理的时间超过30s
目标检测
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
SSD_Lite*
COCO
28.1 MB
8.5 MB 
37
53
SSD_Lite**
COCO
28.1 MB
8.5 MB 
FasterRCNN
COCO
168.5 MB
42.6 MB
Mask_RCNN
COCO
169.7 MB
45.9 MB
Yolov3
COCO
237 MB
61 MB
3
6
*：后端支持网络为MobileNetv1，性能弱于以MobileNetv2为后端推理框架的版本
**：后端支持网络为MobileNetv2，即MMEdu中SSD_Lite选用的版本，可从参数对比中得出其精度、准确度、模型大小均优于以MobileNetv1为后端推理框架的SSD_Lite
行空板测试
行空板, 青少年Python教学用开源硬件，解决Python教学难和使用门槛高的问题，旨在推动Python教学在青少年中的普及。官网：https://www.dfrobot.com.cn/
软硬件环境
操作系统：Linux
系统位数：64
处理器：4核单板AArch64 1.20GHz
内存：512MB
硬盘：16GB
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理 
测试时，计算各个数据集中 10 张图片的平均耗时
下面是我们环境中的测试结果：
图像分类
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
1.77
4.94
ResNet18
ImageNet
44.7 MB
0.46
ResNet50
ImageNet
97.8 MB
24.6 MB
0.22
0.58
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
3.97
8.51
VGG
ImageNet
527.8 MB
101.1 MB
*
*
吞吐量 (图片数/每秒)：表示每秒模型能够识别的图片总数，常用来评估模型的表现
*：不建议部署，单张图片推理的时间超过30s
目标检测
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
SSD_Lite*
COCO
28.1 MB
8.5 MB 
0.55
1.30
SSD_Lite**
COCO
28.1 MB
8.5 MB 
FasterRCNN
COCO
168.5 MB
42.6 MB
Mask_RCNN
COCO
169.7 MB
45.9 MB
Yolov3
COCO
237 MB
61 MB
0.026
0.066
*：后端支持网络为MobileNetv1，性能弱于以MobileNetv2为后端推理框架的版本
**：后端支持网络为MobileNetv2，即MMEdu中SSD_Lite选用的版本，可从参数对比中得出其精度、准确度、模型大小均优于以MobileNetv1为后端推理框架的SSD_Lite
树莓派（4b）测试
Raspberry Pi。中文名为“树莓派”,简写为RPi，或者RasPi（RPi）是为学生计算机编程教育而设计，卡片式电脑，其系统基于Linux。
软硬件环境
操作系统：Linux
系统位数：32
处理器：BCM2711 四核 Cortex-A72(ARM v8) @1.5GHz
内存：4G
硬盘：16G
推理框架：ONNXRuntime == 1.13.1
数据处理工具：BaseDT == 0.0.1
配置
静态图导出
batch大小为1
BaseDT内置ImageData工具进行数据预处理 
测试时，计算各个数据集中 10 张图片的平均耗时
下面是我们环境中的测试结果：
图像分类
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
MobileNet
ImageNet
13.3 MB
3.5 MB 
6.45
ResNet18
ImageNet
44.7 MB
3.20
ResNet50
ImageNet
97.8 MB
24.6 MB
1.48
2.91
ShuffleNet_v2
ImageNet
9.2 MB
2.28 MB
19.11
10.85*
VGG
ImageNet
527.8 MB
101.1 MB
0.43
0.44
吞吐量 (图片数/每秒)：表示每秒模型能够识别的图片总数，常用来评估模型的表现
*：量化后在树莓派上推理速度变慢
目标检测
模型
数据集
权重大小
吞吐量 (图片数/每秒) 
FP32
INT8
FP32
INT8
SSD_Lite*
COCO
28.1 MB
8.5 MB 
2.55
SSD_Lite**
COCO
FasterRCNN
COCO
168.5 MB
42.6 MB
Mask_RCNN
COCO
169.7 MB
45.9 MB
Yolov3
COCO
237 MB
61 MB
0.21
0.34
*：后端支持网络为MobileNetv1，性能弱于以MobileNetv2为后端推理框架的版本
**：后端支持网络为MobileNetv2，即MMEdu中SSD_Lite选用的版本，可从参数对比中得出其精度、准确度、模型大小均优于以MobileNetv1为后端推理框架的SSD_Lite
注：硬件测试模块持续更新中，如有更多硬件测试需求，请联系我们
多模态交互
回顾用AI解决真实问题的流程图，我们已经介绍了收集数据、训练模型、模型推理和应用部署。结合项目设计，我们还会去思考如何通过摄像头获得图像，如何控制灯光发亮，如何操纵舵机，如何设计显示界面UI等需要使用输入设备和输出设备等来实现的交互设计，即对多模态交互的考量。
更多传感器、执行器使用教程参见：DFRobot
更多模型转换相关项目
猫狗分类小助手：https://www.openinnolab.org.cn/pjlab/project?id=641039b99c0eb14f2235e3d5&backpath=/pjedu/userprofile%3FslideKey=project#public
千物识别小助手：https://www.openinnolab.org.cn/pjlab/project?id=641be6d479f259135f1cf092&backpath=/pjlab/projects/list#public
有无人检测小助手：https://www.openinnolab.org.cn/pjlab/project?id=641d3eb279f259135f870fb1&backpath=/pjlab/projects/list#public
MMEdu模型在线转换：https://www.openinnolab.org.cn/pjlab/project?id=63c756ad2cf359369451a617&sc=62f34141bf4f550f3e926e0e#public

走向OpenMMLab
OpenMMLab简介
当你打开这个页面，说明MMEdu已经入门，并期望使用更多的参数，训练出更好的模型。
MMEdu源自OpenMMLab。OpenMMLab（浦视）是上海人工智能实验室的计算机视觉算法开源体系 OpenMMLab是深度学习时代全球领域最全面、最具影响力的视觉算法开源项目，全球最大最全的开源计算机视觉算法库，为学术和产业界提供一个可跨方向、结构精良、易复现的统一算法工具库。
OpenMMLab 已经累计开源了超过 30 个算法库，涵盖分类、检测、分割、视频理解等众多研究领域，拥有超过 300 种算法、2,400 多个预训练模型。在 GitHub 上获得超过 72,000 个标星，同时吸引了超过 1,500 名社区开发者参与项目贡献，用户遍及超过 110 个国家和地区，覆盖全国全球顶尖高校、研究机构和企业。
MMEdu保留了OpenMMLab的各种参数，尤其是模型训练的所有常见参数。如果要训练出更加专业的模型，你需要先了解OpenMMLab。
OpenMMLab官方地址：https://openmmlab.com/
训练参数详解
train函数支持很多参数，为了降低难度，MMEdu已经给绝大多数的参数设置了默认值。根据具体的情况修改参数，可能会得到更好的训练效果。下面来详细说明train函数的各个参数。
epochs：默认参数为100，用于指定训练的轮次，而在上述代码中我们设置为10。
batch_size：批大小，一次训练所选取的样本数，指每次训练在训练集中取batch_size个样本训练。默认参数为None，如为None则默认为对应网络配置文件中设置的samples_per_gpu的值，用于指定一次训练所选取的样本数。当训练集样本非常多时，直接将这些数据输入到神经网络的话会导致计算量非常大，容易因内存不足导致内核挂掉，因此可引入batch_size参数的设置。关于batch_size的取值范围，应该大于类别数，小于样本数，且由于GPU对2的幂次的batch可以发挥更佳的性能，因此设置成16、32、64、128…时往往要比设置为整10、整100的倍数时表现更优。
validate：布尔值，只能为True或者False，默认参数为True，在训练结束后，设定是否需要在验证集上进行评估，True则是需要进行评估。
random_seed：随机种子策略，默认为0即不使用，使用随机种子策略会减小模型算法结果的随机性。
save_fold：模型的保存路径，参数为None，默认保存路径为./checkpoints/cls_model/，如果不想模型保存在该目录下，可自己指定路径。
distributed：布尔值，表示是否在分布式环境中训练该模型，默认为False。
device：训练时所使用的设备，默认为'cpu'，如果电脑支持GPU，也可以将参数修改为'cuda'，使用GPU进行推理。
metric：验证指标，默认参数为'accuracy'，在进行模型评估时会计算分类准确率，数值越高说明模型性能越好，我们在运行完程序之后也会看到这个结果。
save_best：验证指标，默认参数为'auto'，在进行模型评估时会计算分类准确率，数值越高说明模型性能越好，运行完程序之后会将这个结果保存。
optimizer：进行迭代时的优化器，默认参数为SGD，SGD会在训练的过程中迭代计算mini-bath的梯度。
lr：学习率，默认参数为1e-2即0.01，指定模型进行梯度下降时的步长。简单解释就是，学习率过小，训练过程会很缓慢，学习率过大时，模型精度会降低。
checkpoint：指定使用的模型权重文件，默认参数为None，如果没有指定模型权重文件，那么我们将会使用默认的模型权重文件进行推理。
执行上述代码之后的运行结果如下图：
而在checkpoints\cls_model文件夹中我们会发现多了两种文件，一个是***.log.json文件，它记录了我们模型在训练过程中的一些参数，比如说学习率lr，所用时间time，以及损失loss等；另一个文件是.pth文件，这个是我们在训练过程中所保存的模型。
如果查看准确率
方式一：通过训练输出（如上图），运行训练代码时输出项里会出现学习率lr，所用时间time，以及损失loss，每一轮在验证上的accuracy_top-**等。
方式二：通过日志文件，在训练过程中我们会发现模型保存路径下（代码中指定指定）出现一个*.log.json文件，这就是日志文件，它记录了我们模型在训练过程中的一些信息。
当您启动验证集验证，即设置“validate=True”，表示每轮（每个epoch）训练后，在验证集（val_set）上测试一次准确率。那么每一轮训练结束时会呈现一次准确率，并且会生成best_accuracy_top-*.pth权重文件即最佳准确率权重文件。
accuracy_top-1：对一张图片，如果你的预测结果中概率最大的那个分类正确，则认为正确，再根据分类正确的样本数除以所有的样本数计算得到的准确率。
accuracy_top-5：对一张图片，如果预测概率前五名的答案中出现了正确答案，便认为正确，再根据分类正确的样本数除以所有的样本数计算得到的准确率，在MMClassification中，如果类别数量大于5会启动accuracy_top-5准确率。
日志文件解读
Epoch[1][10/838]: 1表示当前是第1个epoch，而10/838表示当前正在处理第10个批次，一共有838个批次。在深度学习模型的训练过程中，通常会将训练数据集分成若干个批次，每个批次包含一定数量的样本（每批次样本数和batch_size设置相关），训练时会使用这些批次逐步迭代来更新模型的参数。
lr: 学习率。
eta: 表示预计完成整个训练所需要的时间。
time: 表示本批次训练需要的时间。
data_time: 数据预处理的时间。
memory: 训练时占据内存或现存的大小。
loss: 本批次模型在训练集上计算的损失值。loss是衡量模型在训练集上预测结果与真实结果之间差异的指标。不同类型的模型（如分类、回归、生成等）使用不同的loss函数来优化模型，MMEdu的图像分类模型一般使用交叉熵损失函数。通常情况下，训练过程中的loss会逐渐下降，表示模型在逐步学习优化。

MMEdu项目案例集
解决图像分类问题——以猫狗识别为例
项目地址：https://openinnolab.org.cn/pjlab/project?id=64055263cb63f03054eca2df&sc=62f34141bf4f550f3e926e0e#public
实现效果：
实施步骤：
1）数据准备
首先需要根据要解决的问题收集数据并整理好数据集，如想要解决猫狗识别问题需准备猫狗数据集。
MMEdu图像分类模块要求的数据集格式为ImageNet格式，包含三个文件夹和三个文本文件，文件夹内，不同类别图片按照文件夹分门别类排好，通过trainning_set、val_set、test_set区分训练集、验证集和测试集。文本文件classes.txt说明类别名称与序号的对应关系，val.txt说明验证集图片路径与类别序号的对应关系，test.txt说明测试集图片路径与类别序号的对应关系。如何从零开始制作符合要求的数据集详见后文。
为了能够在平台完成训练，还需完成引用数据集的操作，可在左侧工具栏选择模型与数据集，上传并创建自己的数据集，再引用，如本项目已经引用我在平台上传并公开的经典版猫狗二分类数据集。
猫狗分类数据集地址：https://openinnolab.org.cn/pjlab/dataset/63ac17c7c9fa8b6b82070697
2）建立模型并模型训练
示例项目使用的LeNet网络比较适合一些简单图像的分类，不适合彩色图，因此我们现在训练猫狗分类模型可以换个网络，例如ResNet18、MobileNet。另外如果你的数据集图片比较多，又是全新训练的，一般都需要100多轮才会有较好的表现，你要有心理准备哦。如需启动GPU训练，在参数中加入device='cuda'即可，若无，则去掉【浦育平台可启动GPU服务器，可上传数据集到浦育平台进行模型训练】。
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
model.num_classes = 2 # 猫和狗共2类
model.load_dataset(path='/data/6P6SGH/CatsDogs') 
model.save_fold = 'checkpoints/cls_model/catsdogs' 
model.train(epochs=10 ,lr=0.001,batch_size=4, validate=True,device='cuda')
3）基于预训练模型继续训练
全新开始训练一个模型，一般要花较长时间。因此我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样。
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
model.num_classes = 2
model.load_dataset(path='/data/6P6SGH/CatsDogs') 
model.save_fold = 'checkpoints/cls_model/CatsDog1' 
model.train(epochs=5, checkpoint='checkpoints/Pre-trained_mobilenet_v2.pth' ,batch_size=4, lr=0.001, validate=True,device='cuda')
预训练模型下载地址：https://p6bm2if73b.feishu.cn/drive/folder/fldcnxios44vrIOV9Je3wPLmExf
4）模型测试
可用一些新的图片进行测试。
python
img = '/data/6P6SGH/CatsDogs/test_set/cat/cat100.jpg'
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/catsdogs/best_accuracy_top-1_epoch_8.pth'
result = model.inference(image=img, show=True, checkpoint = checkpoint, device='cuda')
model.print_result(result)
5）模型转换和应用
参考项目：https://openinnolab.org.cn/pjlab/project?id=641039b99c0eb14f2235e3d5&backpath=/pjlab/projects/list#public
模型转换：
模型转换所需要的文件：待转换的模型权重文件。本项目以猫狗分类为例，我们使用在猫狗分类模型训练过程中生成的最佳权重文件（已传入项目文件），您也可以上传您的图像分类模型。实例化模型时选择自己准备的模型权重文件在训练时选择的网络。我使用的是MobileNet，因此我指定MobileNet。
python
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/best_accuracy_top-1_epoch_5.pth'
out_file='cats_dogs.onnx' # 指定输出的文件即转换后的文件
model.convert(checkpoint=checkpoint, backend="ONNX", out_file=out_file)
模型应用的基础代码：
python
from XEdu.hub import Workflow as wf
mmcls = wf(task='mmedu',checkpoint='cats_dogs.onnx')# 指定使用的onnx模型
result, result_img =  mmcls.inference(data='/data/6P6SGH/CatsDogs/test_set/cat/cat0.jpg',img_type='cv2')# 进行模型推理
format_result = mmcls.format_output(lang="zh")# 推理结果格式化输出
mmcls.show(result_img)# 展示推理结果图片
mmcls.save(result_img,'new.jpg')# 保存推理结果图片
6）部署到硬件
此时您可以挑选自己熟悉的硬件，去做自己训练并完成转换的模型部署啦，只需要下载转换的ONNX模型，在硬件上安装库即可。更多模型应用与部署的介绍详见后文。
解决目标检测问题——以猫狗检测为例
项目地址：https://openinnolab.org.cn/pjlab/project?id=64055f119c0eb14f22db647c&sc=62f34141bf4f550f3e926e0e#public
实现效果：
实施步骤：
1）数据准备
首先需要根据要解决的问题收集数据并整理好数据集，如想要解决猫狗目标检测问题需准备猫狗目标检测数据集。
XEdu中MMEdu的MMDetection模块支持的数据集类型是COCO，如需训练自己创建的数据集，数据集需转换成COCO格式。如何从零开始制作符合要求的数据集详见后文。
浦育平台的猫狗检测数据集：https://openinnolab.org.cn/pjlab/dataset/6407fdcd9c0eb14f2297218d
2）建立模型并模型训练
```python
导入库、实例化模型
from MMEdu import MMDetection as det 
model = det(backbone='SSD_Lite')
model.num_classes = 2 # 猫和狗共2类
model.load_dataset(path='/data/H47U12/cat_dog_det') 
model.save_fold = 'checkpoints/det_model/catdogs' 
model.train(epochs=10 ,lr=0.001,batch_size=4, validate=True,device='cuda')
```
3）基于预训练模型继续训练
全新开始训练一个模型，一般要花较长时间。因此我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样。
```python
model.num_classes = 2 # 猫和狗共2类
model.load_dataset(path='/data/H47U12/cat_dog_det') 
预训练模型权重路线
checkpoint = 'checkpoints/pretrain_ssdlite_mobilenetv2.pth'
model.save_fold = 'checkpoints/det_model/catdogs_pretrain' 
启动cpu容器将device='cpu'，启动GPU容器将device='cuda'
model.train(epochs=10, lr=0.001, validate=True, batch_size = 4, device='cuda', checkpoint=checkpoint)
```
预训练模型下载地址：https://p6bm2if73b.feishu.cn/drive/folder/fldcnxios44vrIOV9Je3wPLmExf
注：浦育平台可启动GPU服务器，可上传数据集到浦育平台进行模型训练。
4）模型测试
可用一些新的图片进行测试。
```python
img = "/data/H47U12/cat_dog_det/images/valid/001.jpg"
checkpoint = "checkpoints/det_model/catdogs_pretrain/best_bbox_mAP_epoch_7.pth"
推理，“show=True”表示弹出识别结果窗口
result = model.inference(image=img, show=True, checkpoint = checkpoint,device='cuda')#启动cpu容器将device='cpu'，启动GPU容器将device='cuda'
输出结果，将inference函数输出的结果修饰后输出具体信息
r=model.print_result(result)
```
5）模型转换和应用
参考项目：https://openinnolab.org.cn/pjlab/project?id=64128fd87c99492cf1bd579e&sc=62f34141bf4f550f3e926e0e#public
模型转换：
模型转换所需要的文件：待转换的模型权重文件。本项目以猫狗检测为例，我们使用在猫狗检测模型训练过程中生成的最佳权重文件（已传入项目文件），您也可以上传您的目标检测模型。
实例化模型时选择自己准备的模型权重文件在训练时选择的网络。我使用的是SSD_Lite，因此我指定SSD_Lite。
python
from MMEdu import MMDetection as det
model = det(backbone='SSD_Lite')
checkpoint = 'checkpoints/best_bbox_mAP_epoch_7.pth'
out_file='cats_dogs_det.onnx' # 指定输出的文件即转换后的文件
model.convert(checkpoint=checkpoint, backend="ONNX", out_file=out_file)
模型应用的基础代码：
python
from XEdu.hub import Workflow as wf
mmdet = wf(task='mmedu',checkpoint='cats_dogs_det.onnx')# 指定使用的onnx模型
result, result_img =  mmdet.inference(data='/data/H47U12/cat_dog_det/images/valid/001.jpg',img_type='cv2')# 进行模型推理
format_result = mmdet.format_output(lang="zh")# 推理结果格式化输出
mmdet.show(result_img)# 展示推理结果图片
mmdet.save(result_img,'new.jpg')# 保存推理结果图片
6）部署到硬件
此时您可以挑选自己熟悉的硬件，去做自己训练并完成转换的模型部署啦，只需要下载转换的ONNX模型，在硬件上安装库即可。更多模型应用与部署的介绍详见后文。

快速体验MMEdu，开始！
MMEdu是什么？
MMEdu源于国产人工智能视觉（CV）算法集成框架OpenMMLab，是一个“开箱即用”的深度学习开发工具。在继承OpenMMLab强大功能的同时，MMEdu简化了神经网络模型搭建和训练的参数，降低了编程的难度，并实现一键部署编程环境，让初学者通过简洁的代码完成各种SOTA模型（state-of-the-art，指在该项研究任务中目前最好/最先进的模型）的训练，并能够快速搭建出AI应用系统。 
官方地址：OpenInnoLab
GitHub：https://github.com/OpenXLab-Edu/OpenMMLab-Edu
国内镜像：https://gitee.com/openxlab-edu/OpenMMLab-Edu
库文件源代码可以从PyPi下载，选择tar.gz格式下载，可用常见解压软件查看源码。
文档涉及的部分代码见XEdu帮助文档配套项目集：https://www.openinnolab.org.cn/pjlab/project?id=64f54348e71e656a521b0cb5&sc=645caab8a8efa334b3f0eb24#public
小目标：使用MMEdu训练一个手写体数字识别模型！
MMEdu有多种安装方式，可以通过pip方式安装，也可以使用一键安装包。体验MMEdu的最快速方式是通过OpenInnoLab平台在线体验MMEdu。
1. 在OpenInnoLab创建MMEdu项目
OpenInnoLab平台为上海人工智能实验室推出的青少年AI学习平台，满足青少年的AI学习和创作需求，支持在线编程。在“工具 - 在线工具- 人工智能工坊”中，可直接创建XEdu项目。OpenInnoLab平台网址：https://www.openinnolab.org.cn/（用Chrome浏览器打开效果最佳）。
在“项目-更多”中，搜索“MMEdu”，可找到所有与MMEdu相关的项目直接“克隆”，项目所有人为XEdu的项目是XEdu研发团队原创并公开的项目，其他项目是MMEdu使用者公开的项目，如初次使用MMEdu，建议直接克隆XEdu名下的MMEdu相关项目。
下面以“手写体识别”为例来介绍从零开始训练一个AI模型的过程。
2. 克隆项目
强烈建议直接“克隆”《用MMEdu实现MNIST手写体数字识别（XEdu官方版）》项目。
项目链接：https://www.openinnolab.org.cn/pjlab/project?id=64a3c64ed6c5dc7310302853&sc=635638d69ed68060c638f979#public
（用Chrome浏览器打开效果最佳）
3. 加载数据集
默认情况下，“克隆”的项目中已经导入了数据集。你也可以重新“导入”一次。步骤如下：
点击“+”，输入“mnist“，找到相应数据集，勾选“☑️”，点击导入；
在数据集文件图标上方点击“右键”，即可“复制文件路径”。接下来，你就可以通过这个路径来访问你的数据集。比如，我得到的文件路径是：/data/MELLBZ/mnist。
新手提问1： 我要使用自己的数据集怎么办？为什么会这么麻烦？
解答：因为项目的空间容量是有限的，同时数据集是公用的，经常在多个项目中使用。因而OpenInnoLab将数据集放在一个公用区域，和项目的文件分离。如果要使用自己的数据集，请在“我的数据集”中增加一个新的数据集，OpenInnoLab支持上传压缩包的方式来建立数据集。数据集加好后，同样需要“导入”后才能访问。
如果你的数据集很小（比如100M内），那么也可以像使用正常的项目文件一下，通过浏览器上传即可。
新手提问2： 我应该怎么制作自己的数据集？
解答：制作合适的数据集是机器学习任务成功的关键。而数据集的质量会直接影响到模型的性能。因此，制定合适的数据集策略至关重要。以下是关于如何做数据集的建议：
​       明确目标：首先，你需要根据你的任务来确定你的数据需求。你想要解决的问题是什么？你需要什么样的数据来支持这个问题的解答？
​       数据量与质量：许多人关心的问题是：“我需要多少图片？”以及“图片的尺寸应该是多大？”答案取决于你期望模型的识别精度。一般来说，数据越丰富、越多样，模型的表现就越好。
​       多样性：只有数量还不够，你还需要确保数据的多样性。这意味着你需要考虑各种光线、拍摄角度、背景等变化条件。
​       格式要求：不同的AI开发工具或框架可能对数据集格式有特定的要求。MMEdu工具对数据集的格式也有自己的要求，如图像分类模块要求ImageNet格式，目标检测模块要求COCO格式，后文有如何从零开始制作ImageNet格式数据集和如何从零开始制作COCO格式数据集的详细说明。
4. 训练模型
一个典型的模型训练代码：
python
from MMEdu import MMClassification as cls # 导入库
model = cls(backbone='LeNet') # 实例化模型
model.num_classes = 3 # 配置基本信息（类别数量）
model.load_dataset(path='./dataset') # 指定数据集路径
model.save_fold = './my_model' # 指定模型保存集路径
model.train(epochs=10, validate=True) # 训练模型
打开《用MMEdu实现MNIST手写体数字识别（XEdu官方版）》项目中的"1.mnist手写体数字识别-训练.ipynb"文件跟随说明完成训练代码运行。示例代码中共6行代码，完成了导入库、实例化模型、配置基本信息（图片分类的类别数量（model.num_classes），指定数据集的路径（model.load_dataset）和指定模型保存的路径（model.save_fold）），开始训练模型。
5. 继续训练
如果觉得效果不够好，请继续训练（实际上就是“迁移学习”）：
python
from MMEdu import MMClassification as cls
model = cls(backbone='LeNet')
model.num_classes = 3
model.load_dataset(path='./dataset')
model.save_fold = './my_model'
checkpoint = './latest.pth' # 指定原有预训练模型路径
model.train(epochs=10, validate=True, checkpoint=checkpoint) # 训练模型
注意：“继续训练”和“普通训练”的区别就在于model.train()函数中多了一个参数，即checkpoint=checkpoint。checkpoint的路径就来自之前训练的权重文件。
推荐使用“继续训练”的方式，因为全新开始训练一个模型，一般要花较长时间。如果使用CPU训练，需要的时间就更长了。因此我们强烈建议在预训练模型的基础上继续训练。可以指定网上下载的某个预训练模型。借助在大型数据集上训练的预训练模型可以根据一系列任务的历史数据来对新的任务进行训练，而无需从头开始训练。它可以将一个大型数据集中的知识和技能转移到另一个任务上，从而大大节省训练时间。 
6. 模型推理
模型训练好后，就可以测试效果了。代码中img的路径就是用于测试的新图片。
python
from MMEdu import MMClassification as cls # 导入库
img = './img.png' # 指定图片
model = cls(backbone='LeNet') # 实例化模型
checkpoint = './latest.pth' # 指定模型保存路径
result = model.inference(image=img, show=True, checkpoint = checkpoint) # 开始推理
model.print_result(result) # 输出推理结果
打开《用MMEdu实现MNIST手写体数字识别（NOTEBOOK）》项目中的"2.mnist手写体数字识别-推理.ipynb"文件跟随说明完成推理代码运行。示例代码中共7行代码，完成了导入库、指定图片、实例化模型、指定模型保存的路径、开始推理、输出推理结果。根据推理结果可以完成各种其他输出。
如需了解代码规则和参数信息，以及图像分类模块的更多使用说明，请看"解锁图像分类模块：MMClassification"。
下一个目标：使用MMEdu训练各种模型
我们的小目标就达成了！一个手写体数字识别模型就训练好了，此时你肯定还不过瘾，还想使用MMEdu基于各种数据集来训练模型去解决各种分类问题也很方便！只需要对手写体识别训练的代码稍作改动即可。首先思考自己想要解决的分类问题，收集数据并整理好数据集，如想要解决猫狗识别问题需准备猫狗数据集。
可以通过《用MMEdu实现MNIST手写体数字识别（NOTEBOOK）》项目中的"3.从mnist手写体数字识别到猫狗分类.ipynb"文件进行案例学习。更多案例可以参考详见项目集区域，这里的AI特色工具专区展示了一些有助于深度学习和机器学习入门和进阶的案例。 当然你也可以在AI项目工坊创建自己的项目，做自己的人工智能案例。MMEdu工具有图像分类模块、目标检测模块等模块，请继续阅读文档，了解并掌握MMEdu的各个模块。
MMEdu的简要总结
MMEdu是针对青少年学习AI技术而设计的，其前身就是大名鼎鼎的OpenMMLab。MMEdu的语法非常简单，几句话就能完成训练和推理。如果你下载了XEdu一键安装包，还可以使用Easytrain等可视化辅助工具。有了MMEdu，你会发现AI模型训练原来这么简单。

MMEdu快速入门
1.MMEdu是什么？
MMEdu源于国产人工智能视觉（CV）算法集成框架OpenMMLab，是一个“开箱即用”的深度学习开发工具。在继承OpenMMLab强大功能的同时，MMEdu简化了神经网络模型搭建和训练的参数，降低了编程的难度，并实现一键部署编程环境，让初学者通过简洁的代码完成各种SOTA模型（state-of-the-art，指在该项研究任务中目前最好/最先进的模型）的训练，并能够快速搭建出AI应用系统。 
官方地址：OpenInnoLab
GitHub：https://github.com/OpenXLab-Edu/OpenMMLab-Edu 
国内镜像：https://gitee.com/openxlab-edu/OpenMMLab-Edu
2.MMEdu安装
MMEdu有多种安装方式，可以通过Pip方式安装，也可以使用一键安装包。
2.1 一键安装包
为方便中小学教学，MMEdu团队提供了一键安装包。只要下载并解压MMEdu的Project文件，即可直接使用。
第一步：下载MMEdu最新版文件，并解压到本地，文件夹目录结构如下图所示。
 
图1 目录结构图
​    1）下载方式一
飞书网盘： https://p6bm2if73b.feishu.cn/drive/folder/fldcnfDtXSQx0PDuUGLWZlnVO3g
​    2）下载方式二
百度网盘：https://pan.baidu.com/s/19lu12-T2GF_PiI3hMbtX-g?pwd=2022 
​       提取码：2022
第二步：运行根目录的“steup.bat”文件，完成环境部署（如下图所示）。
图2 环境部署界面
第三步：您可以根据个人喜好，选择自己习惯的IDE。一键安装包内置了pyzo和jupyter，您也可以使用其他您习惯使用的编辑器。
3.体验demo代码
1）使用默认IDE
双击pyzo.exe，打开demo文件夹中的cls_demo.py，运行并体验相关功能，也可以查看其他的Demo文件。详细说明可以在HowToStart文件夹看到。
2）使用第三方IDE
环境支持任意的Python编辑器，如：Thonny、PyCharm、Sublime等。
只要配置其Python解释器地址为{你的安装目录}+{\MMEdu\mmedu\python.exe}。
体验入门Demo后，我们还准备了一系列的入门课程供您参考。将在稍晚发布。
4.模型训练
典型训练：
python
from MMEdu import MMClassification as cls
model = cls(backbone='LeNet')
model.num_classes = 3
model.load_dataset(path='./dataset')
model.save_fold = './my_model'
model.train(epochs=10, validate=True)
继续训练：
python
from MMEdu import MMClassification as cls
model = cls(backbone='LeNet')
model.num_classes = 3
model.load_dataset(path='./dataset')
model.save_fold = './my_model'
checkpoint = './latest.pth'
model.train(epochs=10, validate=True, checkpoint=checkpoint)
5.模型推理
python
from MMEdu import MMClassification as cls
img = './img.png'
model = cls(backbone='LeNet')
checkpoint = './latest.pth'
class_path = './classes.txt'
result = model.inference(image=img, show=True, class_path=class_path,checkpoint = checkpoint)
model.print_result(result)
6.部署AI应用
1.准备工作
所谓准备工作就是先训练好一个效果不错的模型。
2.借助OpenCV识别摄像头画面
1）代码编写
python
import cv2
from time import sleep
cap = cv2.VideoCapture(0)
print("一秒钟后开始拍照......")
sleep(1)
ret, frame = cap.read()
cv2.imshow("my_hand.jpg", frame)
cv2.waitKey(1000) # 显示1秒（这里单位是毫秒）
cv2.destroyAllWindows()
cv2.imwrite("my_hand.jpg", frame)
print("成功保存 my_hand.jpg")
cap.release()
2）运行效果
3.借助PyWebIO部署Web应用
1）编写代码
```python
from base import *
from MMEdu import MMBase
import numpy as np
from pywebio.input import input, FLOAT,input_group
from pywebio.output import put_text
鸢尾花的分类
flower = ['iris-setosa','iris-versicolor','iris-virginica']
声明模型
model = MMBase()
导入模型
model.load('./checkpoints/mmbase_net.pkl')
info=input_group('请输入要预测的数据', [
    input('Sepal.Length：', name='x1', type=FLOAT),
    input('Sepal.Width：', name='x2', type=FLOAT),
    input('Petal.Length：', name='x3', type=FLOAT),
    input('Petal.Width：', name='x4', type=FLOAT)
])
print(info)
x = list(info.values())
put_text('你输入的数据是：%s' % (x))
model.inference([x])
r=model.print_result()
put_text('模型预测的结果是：' + flower[r[0]['预测值']])
print('模型预测的结果是：' +flower[r[0]['预测值']])
```
2）运行效果
4.连接开源硬件开发智能作品
1）

