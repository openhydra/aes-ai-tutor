如何快速获得XEdu
网页：浦育平台为使用者提供了XEdu容器，我们可以把它想象成一台部署好XEdu和配套AI开发工具的远程电脑，这种形式很好的解决了环境搭建和算力问题。
本地：为了满足广大中小学师生本本地环境的需求，XEdu安装方式分为一键安装包安装、pip安装、docker安装和openhydra安装。快速获得自然是推荐选择XEdu一键安装包（CPU版本），它满足大部分机房需求。
在浦育平台使用XEdu
1. 打开浦育平台，注册登录账号
浦育平台openinnolab
：https://www.openinnolab.org.cn/pjedu/home
2. 克隆一个XEdu容器的项目
推荐初学者从克隆别人的项目开始学习XEdu，克隆项目的步骤如下：
“项目”->"搜索XEdu"->"挑选喜欢的项目"->"克隆"
这样你就得到了一个XEdu项目，跟随项目就可以快速入门XEdu了
3. 新建一个XEdu容器的项目
推荐初学者使用Notebook开始XEdu的学习，新建项目的步骤如下：
“工具”->"人工智能工坊"->"Notebook编程"->"XEdu"
在本地使用XEdu
准备工作
下载工具：XEdu一键安装包
下载方式
飞书网盘：XEdu v1.6.7d.exe
硬件要求：准备win10及以上的电脑，将一键安装包安装到纯英文路径下。飞书网盘里的一件安装包会不定期更新，可时常到网盘中查看与下载最新版。
安装步骤
第一步：双击运行“XEdu v1.6.7d.exe”文件，将自解压为XEdu文件夹。
第二步：打开XEdu简介.pdf，快速了解一键安装包的使用。
第三步：快速测试XEdu示例代码。
打开根目录下的"jupyter编辑器.bat"，即自动启动浏览器并显示界面，如下图所示。
此时可打开"demo"文件夹中的ipynb文件，如"MMEdu_cls_notebook.ipynb"。选中代码单元格，点击常用工具栏"运行"按钮，就可以运行单元格中的代码，单元格左侧[*]内的星号变为数字，表示该单元格运行完成。按步骤即可测试体验XEdu代码。
更多一键安装包的使用请前往XEdu一键安装包说明。

案例六：综合项目石头剪刀布的实时识别（XEduHub+BaseNN）
项目说明：
组合XEdu工具集的工具完成一个综合项目非常方便，本项目使用XEduHub提取手势图像的关键点信息，再将这些关键点信息作为特征输入到一个自己搭建的全连接神经网络模型中进行训练，此步骤由BaseNN实现，最后到本地完成模型应用，实现石头剪刀布手势实时识别的综合项目。
项目地址：https://openinnolab.org.cn/pjlab/project?id=66062a39a888634b8a1bf2ca&backpath=/pjedu/userprofile?slideKey=project#public
项目步骤：
任务一：关键点检测和简单应用
XEduHub提供了能够快速识别人手关键点的模型：pose_hand21，该模型能够识别人手上的21个关键点，如下图所示。手部关键点检测的代码可参考学习人手关键点。
首先分析检测到的手部关键点的结构：
掌心：0；
大拇指：1-4；
食指：5-8；
中指：9-12；
无名指：12-16；
小拇指：17-20。
经过人手关键点的模型推理，可以得到包含21对关键点坐标信息的组数，保存在keypoints变量中，通过访问数组元素，便能够轻松获取每个点的坐标信息。例如大拇指指尖4号关键点的x，y关键点坐标是keypoints[4]。
1）基于规则实现手势分类
假设已经检测出了一组手部关键点，可以通过坐标点信息制定规则来区分不同的手势。例如我们写了一个计算伸展手指数量的函数，并设计判断规则通过伸展手指数量区分不同的手势。然而通过下方的代码不难看出这种方式比较麻烦，需对手指进行细致分析，同时结果可能不准确。
```python
import numpy as np
计算两点之间的欧氏距离
def distance(p1, p2):
    return np.sqrt((p1[0] - p2[0])2 + (p1[1] - p2[1])2)
计算伸展手指的数量
def count_extended_fingers(keypoints, wrist_point=0, finger_tips=[4, 8, 12, 16, 20], threshold=50):
    #finger_tips: 每个手指尖端关键点的索引列表，默认为拇指到小指的尖端。
    # 初始化伸展手指的数量为0
    extended_fingers = 0
    distances = []  # 存储掌心到手指尖的距离
# 获取手腕关键点的坐标
wrist = keypoints[wrist_point]
# 遍历手指尖端关键点
for fingertip in finger_tips:
    # 计算手指尖端和掌心之间的距离
    dist = distance(wrist, keypoints[fingertip])
    # 计算第一节指骨到掌心之间的距离
    dist1 = distance(wrist, keypoints[fingertip-1])
    # 计算第二节指骨到掌心之间的距离
    dist2 = distance(wrist, keypoints[fingertip-2])
    distances.append(dist)
    # 如果是大拇指距离大于阈值则认为该手指是伸展的
    if dist > threshold and fingertip==4:
        extended_fingers += 1  # 增加伸展手指的数量
    # 如果距离大于阈值且指尖距离大于第一节指骨距离，第一节指骨距离大于第二节指骨距离则认为该手指是伸展的    
    elif  dist > threshold and dist>dist1>dist2:
        extended_fingers += 1  # 增加伸展手指的数量
# 返回伸展手指的总数和每个手指的距离
return extended_fingers, distances
假设 keypoints 是从模型获取的关键点列表
extended_fingers, finger_distances = count_extended_fingers(keypoints)
threshold=5
判断手势
if extended_fingers == 0 or (finger_distances[0]>threshold and extended_fingers == 1):
    hand_gesture = "石头"
elif extended_fingers == 2 and finger_distances[1] > threshold and finger_distances[2] > threshold:
    hand_gesture = "剪刀"
elif extended_fingers == 5 and all(dist > threshold for dist in finger_distances):
    hand_gesture = "布"
else:
    hand_gesture = "未知手势"
print(hand_gesture)
```
2）实时人手关键点检测
还有一种简单的应用，可以实现实时人手关键点检测，只需连接摄像头，调用OpenCV库，对每一帧图像进行关键点检测，并将关键点检测的结果可视化就可以实现实时人手关键点检测。
```python
from XEdu.hub import Workflow as wf # 导入库
import cv2
cap = cv2.VideoCapture(0)
det  = wf(task='det_hand') # 实例化模型
model = wf(task='pose_hand21') # 实例化模型
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    bboxs,img = det.inference(data=frame,img_type='cv2') # 进行推理
    for i in bboxs:
        keypoints,img =model.inference(data=img,img_type='cv2',bbox=i) # 进行推理
        cv2.imshow('video', img)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break  
cap.release()
cv2.destroyAllWindows()
```
任务二：搭建全连接神经网络训练石头剪刀布手势模型
除了任务一基于规则实现手势分类以外，还可以通过机器学习的方式，训练一个手势分类模型区分不同的手势。
准备工作：准备数据
首先我们可以批量提取出所有手势图像的手势关键点数据，做一个CSV格式的关键点数据集，每行代表一张图片提出的手部关键点坐标和这张图的类别。用如下代码可以做到随拍随提取关键点，并生成关键点数据集（注意需提前安装库），可修改total值设置采集的数据条数。
```python
import csv
from XEdu.hub import Workflow as wf
import cv2
import os
cap = cv2.VideoCapture(0)
class_name=input('请输入本次想要采集的手势名称：')
c_file='classes.txt'
if not os.path.exists(c_file):
    open(c_file,'w')
with open(c_file,'r') as f:
    lines=f.readlines()
    print(lines)
    class_index=len(lines)
    print('类别{',class_name,'}对应的序号是{',class_index,'}。')
with open(c_file,'a') as f:
    s=str(class_name)+','+str(class_index)+'\n'
    f.write(s)
pose = wf(task='hand')
feature_data=[]
cnt=0
from time import sleep
print('3秒后开始采集数据，请做好准备....')
sleep(3)
total=100   # 一共采集多少条数据【可自行修改】
from tqdm import tqdm
pbar=tqdm(total=total)
while cnt<total:
    sleep(0.1)
    ret, frame = cap.read()
if not ret:
    continue
keypoints,img =pose.inference(data=frame,img_type='cv2')
if cv2.waitKey(1) & 0xFF == ord('q'):
    break
format_result = pose.format_output(lang='en', isprint=False)
avg=0
for i in format_result['scores']:
    avg+=i
avg/=21
if avg<0.5:
    cv2.imshow('video', frame)
    print('手部置信度未超过0.5:', avg)
    continue
else:
    cnt+=1
    pbar.update(1)
    cv2.imshow('video', img)
my_data=format_result['keypoints']
feature=[]
for d in format_result['keypoints']:
    feature.append(d[0])
    feature.append(d[1])
feature_data.append(feature)
cap.release()
cv2.destroyAllWindows()
header = [f"Feature {i+1}" for i in range(21*2)] + ["Label"]
csv_data = []
for i in range(len(feature_data)):
    csv_data.append(feature_data[i].copy())
    csv_data[-1].append(class_index)
print(csv_data)
with open('hand_'+str(class_name)+'.csv','w',encoding='UTF8',newline='') as f:
    writer = csv.writer(f)
    writer.writerow(header)
    writer.writerows(csv_data)
print('saved to '+'hand_'+str(class_name)+'.csv')
```
再次运行上面的代码，可以生成第二类手势的数据集。
经过多次收集，可以形成多类数据，用下面这段代码进行数据合并。
```python
import pandas as pd
classes=pd.read_csv('classes.txt',header=None)
dataset=pd.DataFrame()
for c in classes[0]:
    df=pd.read_csv('hand_'+c+'.csv')
    dataset=pd.concat([dataset,df],ignore_index=True)
print(dataset)
dataset.to_csv('hand_total.csv',index=False)
```
当制作完成了这样一个数据集（见项目文件），我们便可以使用XEdu的一系列训练模型的工具去学习这些数据的特征，从而去训练一个石头剪刀布手势识别模型。
第1步 划分数据集
在准备训练前，我们建议先完成数据集划分，即将数据集拆分为训练集和验证集，训练集用于训练模型，验证集用于评估模型的性能。此步骤可以手动完成，也可以用代码完成，可借助XEdu的数据处理库BaseDT，指定csv文件路径以及划分比例，将特征数据集划分为训练集和验证集，并将训练集和验证集的特征和标签均提取出来。
```python
from BaseDT.dataset import split_tab_dataset
指定待拆分的csv数据集
path = "data/workflow_pose.csv"
指定特征数据列、标签列、训练集比重
tx,ty,val_x,val_y = split_tab_dataset(path,data_column=range(1,43),label_column=43,train_val_ratio=0.8)
```
第2步 用BaseNN搭建全连接神经网络训练模型
```python
导入BaseNN库
from BaseNN import nn
声明模型
model = nn('cls')
载入数据
train_path = 'data/workflow_pose_train.csv'
model.load_tab_data(train_path,batch_size=2100)
自己搭建网络
model.add('linear', size=(42, 140), activation='relu') 
model.add('linear', size=(140, 52), activation='relu') 
model.add('linear', size=(52, 3), activation='softmax') 
设置随机数种子
model.set_seed(888)
model.optimizer = 'Adam' #'SGD' , 'Adam' , 'Adagrad' , 'ASGD' 内置不同优化器
learn_rate = 0.001 #学习率
max_epoch = 200 # 最大迭代次数
metrics = 'acc'# 评估指标
model.save_fold = 'checkpoints/BaseNN' # 模型保存路径
模型训练
model.train(lr=learn_rate, epochs=max_epoch,metrics=metrics)
```
第3步 模型评估
使用第1步拆分出的验证集数据评估模型的性能。
```python
import numpy as np
计算验证集准确率
def cal_accuracy(y, pred_y):
    res = pred_y.argmax(axis=1)
    tp = np.array(y)==np.array(res)
    acc = np.sum(tp)/ y.shape[0]
    return acc
model = nn('cls') # 声明模型
checkpoint = 'checkpoints/BaseNN/basenn.pth' # 现有模型路径
读取验证数据
val_path = 'data/workflow_pose_val.csv'
x_val = np.loadtxt(val_path, dtype=float, delimiter=',',skiprows=1,usecols=range(0,42)) 
y_val = np.loadtxt(val_path, dtype=float, delimiter=',',skiprows=1,usecols=42) # 读取最后一列，标签
result = model.inference(x_val, checkpoint=checkpoint) # 直接推理
acc = cal_accuracy(y_val, result)
print('验证集准确率: {:.2f}%'.format(100.0 * acc))
```
第4步 模型测试
用某组数据进行推理预测。
python
model = nn('cls') # 声明模型
checkpoint = 'checkpoints/BaseNN/basenn.pth' # 现有模型路径
data = [[ 89.14984302, 114.5882458 ,  62.63654601, 104.86670357,
        47.90693656,  91.90464725,  33.17732712,  84.24525034,
        18.15312548,  81.88851283,  70.59053511,  68.63186433,
        59.98521631,  45.3590814 ,  52.03122721,  29.4511032 ,
        47.02316   ,  15.89986251,  85.02555237,  64.80216587,
        85.61473675,  38.87805324,  83.55259143,  22.08629847,
        82.66881486,  10.00801873,  93.27413366,  66.27512681,
        95.63087117,  44.18071264,  92.09576491,  37.4050923 ,
        86.49851332,  40.64560638,  99.46056963,  69.51564089,
       100.63893838,  51.54551737,  98.28220087,  48.89418767,
        95.63087117,  48.0104111 ]] # 指定一组坐标数据
result = model.inference(data, checkpoint=checkpoint) # 直接推理
model.print_result(result) # 输出推理结果
第5步 模型转换与应用
为了方便模型应用先可借助BaseNN完成模型转换，转换为ONNX格式后更方便模型使用。
python
from BaseNN import nn
model = nn('cls')
checkpoint = 'checkpoints/BaseNN3/basenn.pth' # 指定待转换的模型权重文件
model.convert(checkpoint=checkpoint, out_file='checkpoints/basenn.onnx')
任务三：实时手势分类
任务一 单张图片完成手势分类
模型推理时，需要保持推理的数据与训练的数据格式一致，所以新的图片也需完成人手关键点检测，并且做维度处理。如下代码实现了上述功能
```python
from XEdu.hub import Workflow as wf
import numpy as np
img_path = 'paper01-001.png' # 指定进行推理的图片路径
det  = wf(task='det_hand') # 实例化模型
model = wf(task='pose_hand21') # 实例化模型
bboxs = det.inference(data=img_path) # 进行推理
keypoints_list = []
for i in bboxs:
    keypoints =model.inference(data=img_path,bbox=i) # 进行推理
    keypoints_list.append(keypoints)
展平数组
pose_features = np.concatenate(keypoints_list).reshape(len(keypoints_list), -1)
声明分类模型
bn = wf(task='basenn',checkpoint='checkpoints/basenn.onnx')
result = bn.inference(data=pose_features)
res = bn.format_output(lang='zh')
```
任务二 模型应用：实时手势分类
了解了单张图片推理的实现逻辑，我们可以应用一下这个模型，比如我们把onnx模型下载到本地，连接一个摄像头，再借助OpenCV库完成一个实时手势分类的应用，参考代码如下。
```python
from XEdu.hub import Workflow as wf
import cv2
import numpy as np
cap = cv2.VideoCapture(0)
det = wf(task='det_hand')  # 实例化模型
model = wf(task='pose_hand21')  # 实例化模型
bn = wf(task='basenn', checkpoint='basenn.onnx')  # 声明分类模型
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    bboxs, img = det.inference(data=frame, img_type='cv2')  # 进行推理
    keypoints_list = []
    for i in bboxs:
        keypoints, img = model.inference(data=img, img_type='cv2', bbox=i)  # 进行推理
        keypoints_list.append(keypoints)
    if len(keypoints_list) > 0:  # 判断是否检测到手部关键点
        pose_features = np.concatenate(keypoints_list).reshape(len(keypoints_list), -1)
        result = bn.inference(data=pose_features)
        res = bn.format_output(lang='zh')
        # 指定分类标签
        label = ['paper', 'rock', 'scissors']
        # 输出类别结果
        prediction=[]
        for i in range(0,len(res)):
            index = (res[i]['预测值'])
            prediction.append(label[index])
        print(prediction) 
        # 在显示图像的窗口中添加预测结果的文本
        cv2.putText(img, f'prediction:{prediction}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)  # 使用BGR颜色代码
        cv2.imshow('video', img)
if cv2.waitKey(1) & 0xFF == ord('q'):
    break
cap.release()
cv2.destroyAllWindows()
```
实现效果：

案例二：用BaseML训练机器学习模型（抛物线）
项目说明：
BaseML库提供了众多机器学习训练方法，如线性回归、KNN、SVM等等，可以快速训练和应用模型，更多算法说明详见机器学习典型算法一览表。本项目使用BaseML中的回归算法，以及其他算法训练投石车落地距离预测模型。投石车落地距离预测是一个典型的抛物线问题，根据投石角度与距离对照表，用机器学习方法预测抛物线函数。
项目地址：用BaseML训练机器学习模型（抛物线）
https://www.openinnolab.org.cn/pjlab/project?id=66123df8e0ea4a2a08b20464&backpath=/pjlab/projects/list#public
机器学习的基本流程
机器学习实际上分为两个阶段，首先是模型训练过程，即“学习”；然后是模型推理过程，即“应用”。典型的机器学习流程可以分为数据准备、模型搭建、模型训练与评估、模型应用等环节（如下图）。
1.数据准备
数据是描述客观事物或现象的符号记录，可以是数字、文字、图像、声音等形式。机器学习需要很多数据，我们称之为“数据集”。要训练怎样的模型，就要准备怎样的数据。例如要训练温度转换的模型，就要准备很多条类似“摄氏温度和华氏温度对应表”的数据。
2.模型搭建
搭建机器学习的模型，核心工作是实现一个具有特定功能的算法。实现机器学习的算法需要编写程序，难度较大。但好消息是Python有多个机器学习的库，这些库中内置了各种优秀的算法，只要根据需要选择合适的算法，就可以直接完成模型的搭建。
3.模型训练与评估
对于训练好的模型，需要评估一下其推理能力，类似人学习了某个课程后，还要做点单元小测试，看看掌握了多少。对回归任务来说，简单好用的评估指标之一是R平方值，对分类任务来说，一般选择准确率。通过比对推断结果与实际标注结果的差异，可以计算出评估指标。如果推理效果不好，要重新检查数据和模型，再次训练。
4.模型应用
当训练出来的模型的评估表现不错，那就可以保存模型。保存出来的模型文件，可以导入使用或供其他程序使用。其实模型应用环节和传统编程就差别不大了，只要输入一组新数据，就能输出预测结果。
项目步骤：
任务一：使用BaseML训练一个线性回归模型
第0步 引入包（建议将库更新为最新版本再导入）
```python
导入库文件，选择回归模块
from BaseML import Regression as reg
```
第1步 实例化模型
```python
构建线性回归模型
model = reg(algorithm = 'LinearRegression')
```
第2步 载入数据
```python
指定数据集
model.load_tab_data('data/投石角度与距离.csv')
```
第3步 模型训练
```python
模型训练
model.train()
```
第4步 模型评估
```python
模型评估
model.valid('data/投石角度与距离评估.csv', metrics='r2') 
评价指标可视化
model.metricplot()
```
第5步 模型保存
```python
模型保存
model.save('checkpoints/baseml_model/lr_catapult.pkl')
```
任务二：使用多种算法实现并测算R2值
除了上文介绍的线性回归模型，BaseML还可以训练其他回归和聚类算法，如下图所示。BaseML模型训练的代码详解详见BaseML功能详解。
①线性回归
```python
导入库文件，选择回归模块
from BaseML import Regression as reg
构建线性回归模型
model = reg(algorithm = 'LinearRegression')
载入数据集
model.load_tab_data('data/投石角度与距离.csv')
训练模型
model.train()
读取验证集进行验证并计算R平方值
r2_Linear, result = model.valid('data/投石角度与距离评估.csv', metrics='r2') # 载入验证数据
model.metricplot() # 可视化验证效果
```
②多项式回归
```python
导入库文件，选择回归模块
from BaseML import Regression as reg
构建线性回归模型
model = reg(algorithm = 'Polynomial')
载入数据集
model.load_tab_data('data/投石角度与距离.csv')
训练模型
model.train()
读取验证集进行验证并计算R平方值
r2_Poly, result = model.valid('data/投石角度与距离评估.csv', metrics='r2') # 载入验证数据
model.metricplot() # 可视化验证效果
```
③支持向量机
```python
导入库文件，选择回归模块
from BaseML import Regression as reg
构建线性回归模型
model = reg(algorithm = 'SVM')
载入数据集
model.load_tab_data('data/投石角度与距离.csv')
训练模型
model.train()
读取验证集进行验证并计算R平方值
r2_SVM, result = model.valid('data/投石角度与距离评估.csv', metrics='r2') # 载入验证数据
model.metricplot() # 可视化验证效果
```
④自适应增强算法
```python
导入库文件，选择回归模块
from BaseML import Regression as reg
构建线性回归模型
model = reg(algorithm = 'AdaBoost')
载入数据集
model.load_tab_data('data/投石角度与距离.csv')
训练模型
model.train()
读取验证集进行验证并计算R平方值
r2_Ada, result = model.valid('data/投石角度与距离评估.csv', metrics='r2') # 载入验证数据
model.metricplot() # 可视化验证效果
```
更多算法说明详见机器学习典型算法一览表。
对比不同算法的R平方值
总结分析：在该数据集上，选择何种算法最合适？
拓展：借助XEduHub完成推理
借助通用推理库XEduHub也可以完成BaseML模型的推理，示例代码如下。
python
from XEdu.hub import Workflow as wf
baseml = wf(task='baseml',checkpoint='checkpoints/baseml_model/lr_catapult.pkl')# 指定使用的pkl模型
data = [[34]] # 根据训练模型时使用的数据来定
result= baseml.inference(data=data)# 进行模型推理
print(result)

案例三：用BaseNN训练搭建全连接神经网络（鸢尾花）
项目说明：
BaseNN可以方便地逐层搭建神经网络，支持搭建CNN和RNN，或二者的结合，训练深度学习模型。可前往解锁BaseNN基本使用方法的教程。本项目核心功能是完成使用经典的鸢尾花数据集完成鸢尾花分类，最后完成了一个简单的鸢尾花分类小应用，输入花萼长度、宽度、花瓣长度、宽度，可以输出预测结果。
项目地址：https://www.openinnolab.org.cn/pjlab/project?id=641bc2359c0eb14f22fdbbb1&sc=635638d69ed68060c638f979#public
数据集：UCI Machine Learning Repository: Iris Data Set（https://archive.ics.uci.edu/ml/datasets/Iris）
项目来源：《人工智能初步》人教地图p69
项目步骤：
任务一：鸢尾花模型训练
第0步 引入包（建议将库更新为最新版本再导入）
```python
导入BaseNN库
from BaseNN import nn
```
第1步 声明模型
python
model = nn('cls')
第2步 载入数据
python
train_path = 'data/iris_training.csv'
model.load_tab_data(train_path, batch_size=120)
第3步 搭建模型
逐层添加，搭建起模型结构。注释标明了数据经过各层的维度变化。
python
model.add(layer='linear',size=(4, 10),activation='relu') # [120, 10]
model.add(layer='linear',size=(10, 5), activation='relu') # [120, 5]
model.add(layer='linear', size=(5, 3), activation='softmax') # [120, 3]
以上使用add()方法添加层，参数layer='linear'表示添加的层是线性层，size=(4,10)表示该层输入维度为4，输出维度为10，activation='relu'表示使用ReLU激活函数。
第4步 模型训练
模型训练可以采用以下代码：
```python
设置模型保存的路径
model.save_fold = 'checkpoints/iris_ckpt'
模型训练
model.train(lr=0.01, epochs=1000)
```
也可以使用继续训练：
python
checkpoint = 'checkpoints/basenn.pth'
model.train(lr=0.01, epochs=1000, checkpoint=checkpoint)
参数lr为学习率， epochs为训练轮数，checkpoint为现有模型路径，当使用checkpoint参数时，模型基于一个已有的模型继续训练，不使用checkpoint参数时，模型从零开始训练。
第5步 模型测试
用测试数据查看模型效果。
```python
import numpy as np
用测试数据查看模型效果
model2 = nn('cls')
test_path = 'data/iris_test.csv'
test_x = np.loadtxt(test_path, dtype=float, delimiter=',',skiprows=1,usecols=range(0,4)) 
res = model2.inference(test_x, checkpoint="checkpoints/iris_ckpt/basenn.pth")
model2.print_result(res)
获取最后一列的真实值
test_y = np.loadtxt(test_path, dtype=float, delimiter=',',skiprows=1,usecols=4) 
定义一个计算分类正确率的函数
def cal_accuracy(y, pred_y):
    res = pred_y.argmax(axis=1)
    tp = np.array(y)==np.array(res)
    acc = np.sum(tp)/ y.shape[0]
    return acc
计算分类正确率
print("分类正确率为：",cal_accuracy(test_y, res))
```
用某组测试数据查看模型效果。
```python
用某组测试数据查看模型效果
data = [test_x[0]]
checkpoint = 'checkpoints/iris_ckpt/basenn.pth'
res = model.inference(data=data, checkpoint=checkpoint)
model.print_result(res) # 输出字典格式结果
```
参数data为待推理的测试数据数据，该参数必须传入值；
checkpoint为已有模型路径，即使用现有的模型进行推理。
上文介绍了借助BaseNN从模型训练到模型测试的简单方法，此外BaseNN支持搭建CNN和RNN，或二者的结合。可前往解锁BaseNN基本使用方法的教程。
任务二：模型转换和后续应用
如果想要快速部署模型，可进行模型转换。BaseNN模型转换的代码如下：
python
from BaseNN import nn
model = nn('cls')
model.convert(checkpoint="checkpoints/iris_ckpt/basenn.pth",out_file="basenn_cd.onnx")
借助生成的示例代码，简单修改（如下所示），即可在本地或者硬件上运行（提前安装XEduHub库），甚至可以借助一些开源工具库做一个网页应用。
```python
from XEdu.hub import Workflow as wf
模型声明
basenn = wf(task='basenn',checkpoint='basenn_cd.onnx')
待推理数据，此处仅以随机二维数组为例，以下为1个维度为4的特征
table = [[5.9, 3. , 4.2, 1.5]]
模型推理
res = basenn.inference(data=table)
标准化推理结果
result = basenn.format_output(lang="zh")
```
还可以借助一些开源工具库（如PyWebIO）编写一个人工智能应用，如下代码可实现手动输入观察到的鸢尾花特征，输出花种判断。
```python
from pywebio.input import *
from pywebio.output import *
from XEdu.hub import Workflow as wf
模型声明
basenn = wf(task='basenn',checkpoint='basenn_cd.onnx')
def pre():
    a=input('请输入花萼长度：', type=FLOAT)
    b=input('请输入请花萼宽度：', type=FLOAT)
    c=input('请输入花瓣长度：', type=FLOAT)
    d=input('请输入花瓣宽度：', type=FLOAT)
    data = [a,b,c,d]
    result = basenn.inference(data=data)
    res = basenn.format_output(lang="zh")
    label=['山鸢尾','变色鸢尾','维吉尼亚鸢尾']
    put_text('预测结果是：', str(label[res[0]['预测值']]))
if name == 'main':
    pre()
```
运行效果如下：
更多模型转换和应用的教程详见模型转换和应用https://xedu.readthedocs.io/zh/master/how_to_use/support_resources/model_convert.html#。
拓展阅读：无代码完成本项目的模型训练和转换
本地版XEdu一键安装包中内置了无代码训练工具，支持BaseNN模型训练，我们也可以使用无代码方式完成鸢尾花模型训练和转换。无代码训练工具使用的步骤与前文介绍的代码实现方式完全一致。
第0步 模块选择
EasyTrain界面打开之后，选择BaseNN页面。
第1步 数据集选择
在下拉框中指定训练的数据集，网页会读取XEdu/dataset/basenn之下数据集。一键安装包中自带数据集：iris/iris_test.csv，iris/iris_training.csv。
数据集的格式要求为：csv文件(BaseNN任务)。纵轴为样本，横轴为特征，第一行为表头，最后一列为标签。
第2步 模型搭建
点击“添加网络层”可以增加网络层，点击右侧“×”可以减少网络层。
注意：
第一层的输入维度要和数据集的特征维度（特征的数量）相等。
因为数据是从上一层流向下一层，因此下一层的输入维度要和上一层的输出维度保持相等。
最后一层的输出维度要和类别数相同。
第3步 参数设置
点击“设置基本参数↓”，可以设置学习率，批次大小，训练轮数，随机种子。完成设置后点击“确认”按钮，成功设置会弹出窗口提醒设置成功。
点击“设置其他训练参数”，可以设置分类数量（仅用于分类任务），优化器，权重衰减、设备、预训练模型。完成设置后点击“提交”按钮。
*预训练模型放置位置标准：XEdu/checkpoints/训练任务/数据集名（严格一致）/预训练模型.pth
更多参数学习请跳转深度学习训练参数详解
在完成参数设置后，点击右侧的"生成代码"，可以生成训练代码，生成代码之后才可以训练。
生成的代码会显示在上方的代码框，点击右上角复制按钮还可以一键复制代码。
第4步 开始训练
代码生成之后点击"进入训练"按钮即可跳转至训练页面，工具会先检查你的电脑有没有安装MMEdu的库，如果没有请先去”小黑窗“进行pip安装BaseNN。
点击“开始训练”按钮即可一键开始训练模型。
出现“loading”表示模型正在训练中，但是也有可能是因为没有安装相应的库，数据集路径错误，数据集为空等等原因导致训练失败，所以先检查一下后台”小黑窗“有没有报错。
如果有报错，修改相应的错误，再通过刷新页面或者关闭”小黑窗“重启工具的方式重新训练。如果没有报错，请耐心等待，由于本地cpu训练模型的速度较慢，可以先去喝杯水休息一下~
若想中断训练，可以点击“停止训练”按钮。
在训练过程中，loss曲线会实时显示在左侧的Loss Chart框中，accuracy曲线会实时显示在左侧的Accuracy  Chart框中。坐标轴的横坐标为训练轮数，纵坐标为对应数值。
自己训练的模型文件将保存在XEdu/my_checkpoints中。每次训练都会生成一个文件夹，可以通过文件夹名称上的日期时间找到对应的模型。
完成模型训练之后，窗口会弹出模型转换，可以点击“是”可实现pth格式模型准换成onnx格式模型。若要自行转换可使用EasyConvert。
如需尝试用代码完成BaseNN模型训练或了解BaseNN具体功能，请参照BaseNN的教程。

案例一：用XEduhub执行推理任务（检测任务）
项目说明：
XEduHub针对一些常见任务，提供了现成的优质模型，可以完成目标检测、关键点检测等等，还可以实现自训练模型推理，用示例详见XEduHub功能详解。本项目完成了直接调用XEduHub一个内置模型det_hand实现检测手的功能，只用7行代码就可实现。
项目地址：用XEduhub执行推理任务（检测任务）
https://www.openinnolab.org.cn/pjlab/project?id=6612327ea888634b8a6de8f6&backpath=/pjlab/projects/list#public
项目步骤：
任务一：检测单任务（以人手目标检测为例）
下面是人手目标检测模型（det_hand）的完整代码：
python
from XEdu.hub import Workflow as wf # 导入库
hand_det = wf(task='det_hand') # 实例化模型
img_path = 'demo/hand1.jpg'  # 指定进行推理的图片路径
boxes,img_with_box = hand_det.inference(data=img_path,img_type='cv2') # 进行推理
format_result =hand_det.format_output(lang='zh') # 结果格式化输出
hand_det.show(img_with_box) # 可视化结果
hand_det.save(img_with_box,'demo/img_with_box.jpg') # 保存可视化结果
第0步 导入库（建议将库更新为最新版本再导入）
python
from XEdu.hub import Workflow as wf # 导入库
第1步 模型声明
python
hand_det = wf(task='det_hand') # 实例化模型
在第一次声明模型时代码运行用时较长，是因为要将预训练模型从云端下载到本地中，从而便于用户进行使用。
当代码在本地运行时，会先在本地的同级目录checkpoints的文件夹中寻找是否有已下载的预训练模型，如果没有，到本地缓存中寻找，如果本地缓存没有，查看是不是指定了模型的路径，如果都没有，到网络下载。
第2步 指定一张待检测的图片
python
img_path = 'demo/hand.jpg'  # 指定进行推理的图片路径
第3步 模型推理
python
boxes = hand_det.inference(data=img_path) # 进行推理
[[354.98464312 171.77575248 993.5257285  867.79527937]]
变量boxes以二维数组的形式保存了检测框左上角顶点的坐标(x1,y1)和右下角顶点的坐标(x2,y2)（之所以是二维数组，是因为该模型能够检测多个人手，因此当检测到多个人手时，就会有多个[x1,y1,x2,y2]的一维数组，所以需要以二维数组形式保存）
第4步 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看检测框坐标以及置信度，代码如下：
python
format_result = hand_det.format_output(lang='zh') # 结果格式化输出
第5步 可视化结果
在推理函数变量中加入img_type='cv2'，这时候会有两个返回值，除了第一个是检测框之外，还返回了每个像素点的颜色信息img_with_keypoints。再通过show()函数就可以看到像素点组成图片可以看到，此时的图片带有目标检测框。
python
boxes,img_with_box = hand_det.inference(data=img_path,img_type='cv2') # 进行推理
hand_det.show(img_with_box) # 可视化结果
XEduHub内置了多个模型，像目标检测任务，除了人手目标检测，还支持人脸目标检测、人体目标检测……使用方法基本一致，更多使用示例详见XEduHub功能详解。
也可以通过学习“XEduHub实例代码-入门完整版入门”项目进行学习，https://www.openinnolab.org.cn/pjlab/project?id=65518e1ae79a38197e449843&backpath=/pjedu/userprofile?slideKey=project#public
任务二：检测多任务（目标检测+关键点检测）
如果再结合关键点检测模型，还能实现多任务的检测。下面几行代码，实现了先检测手，再对每个检测到的手进行关键点检测。
python
from XEdu.hub import Workflow as wf # 导入库
det  = wf(task='det_hand') # 实例化模型
model = wf(task='pose_hand21') # 实例化模型
img_path = 'demo/hand3.jpg' # 指定进行推理的图片路径
bboxs,img = det.inference(data=img_path,img_type='cv2') # 进行推理
for i in bboxs:
    keypoints,img = model.inference(data=img,img_type='cv2',bbox=i) # 进行推理
model.show(img) # 可视化结果
此外还有多种模型组合运用的案例，可以通过学习“XEduHub实例代码-入门完整版入门”项目进行学习，https://www.openinnolab.org.cn/pjlab/project?id=65518e1ae79a38197e449843&backpath=/pjedu/userprofile?slideKey=project#public

案例四：用MMEdu训练LeNet图像分类模型（手写体）
项目说明：
MMEdu是人工智能视觉算法集成的深度学习开发工具，目前图像分类模块MMClassifiation支持的SOTA模型有LeNet、MobileNet、ResNet18、ResNet50等，支持训练的数据集格式为ImageNet。更多关于MMClassifiation功能详见请前往解锁MMEdu的图像分类模块。
本项目使用MMEdu的图像分类模块MMClassification，根据经典的手写体ImageNet格式数据集，训练LeNet模型实现手写体识别。
项目地址：https://openinnolab.org.cn/pjlab/project?id=64a3c64ed6c5dc7310302853&sc=62f34141bf4f550f3e926e0e#public
数据集来源：mnist数据集，来源于National Institute of Standards and Technology，改编自MNIST。另外MMEdu图像分类模块要求的数据集格式为ImageNet格式，包含三个文件夹和三个文本文件，文件夹内，不同类别图片按照文件夹分门别类排好，通过trainning_set、val_set、test_set区分训练集、验证集和测试集。文本文件classes.txt说明类别名称与序号的对应关系，val.txt说明验证集图片路径与类别序号的对应关系，test.txt说明测试集图片路径与类别序号的对应关系。如何从零开始制作符合要求的数据集详见后文。
项目步骤：
任务一：训练LeNet手写体识别模型
第0步 导入基础库（建议将库更新为最新版本再导入）
python
from MMEdu import MMClassification as cls
第1步 实例化模型（选择LeNet）
python
model = cls(backbone='LeNet') # 实例化模型为model
第2步 配置基本信息
AI模型训练时需要配置的基本信息有三类，分别是：图片分类的类别数量（model.num_classes），模型保存的路径（model.save_fold）和数据集的路径（model.load_dataset）。
python
model.num_classes = 10 # 手写体的类别是0-9，共十类数字
model.load_dataset(path='/data/MELLBZ/mnist') # 从指定数据集路径中加载数据
model.save_fold = 'checkpoints/cls_model/230226' # 模型保存路径，可自定义最后一个文件名
第3步 开始训练模型
python
model.train(epochs=10, lr=0.01, validate=True)
注：如有GPU可启动GPU训练，在训练函数中加个参数device='cuda'，则训练代码变成如下这句。
python
model.train(epochs=10, lr=0.01, validate=True, device='cuda')
训练过程中观察输出的每一轮acc的变化，判断模型在验证集上的准确率。
任务二：模型测试（用新的图片完成推理）
第0步 导入基础库（建议将库更新为最新版本再导入）
python
from MMEdu import MMClassification as cls
第1步 实例化模型
python
model = cls(backbone='LeNet')
第2步 指定模型权重文件的所在路径
python
checkpoint = 'checkpoints/cls_model/best_accuracy_top-5_epoch_4.pth' # 指定权重文件路径
第1步和第2步的模型需对应，首先模型权重需存在，同时还需该模型训练时实例化模型时选择的网络与推理时一致。
第3步 指定图片
python
img_path = 'picture/2.png' # 指定图片路径
第4步 开始推理
python
result = model.inference(image=img_path, show=True, checkpoint = checkpoint) # 模型推理
model.print_result(result) # 结果转换为中文输出
上文简单介绍了如何用MMEdu训练一个图像分类模型，更多关于MMEdu模型训练和推理的方法详见请前往解锁MMEdu的图像分类模块https://xedu.readthedocs.io/zh/master/mmedu/mmclassification.html#mmclassification。 
拓展：模型转换和应用
当一个深度学习模型训练完成后，最终的任务是要结合其他编程工具，编写一个人工智能应用。一般来说，这些规模较小的模型都是会运行在一些边缘设备（指性能较弱的移动端和嵌入式设备）上。此时你可以使用MMEdu的模型转换工具将模型转换为ONNX格式，便于部署。
python
from MMEdu import MMClassification as cls
model = cls(backbone='LeNet')
checkpoint = 'checkpoints/cls_model/best_accuracy_top-5_epoch_4.pth'
out_file="cls.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
接下来无需借助MMEdu库（安装涉及较多依赖库），只需借助XEuHub库便可完成推理。
python
from XEdu.hub import Workflow as wf
mmcls = wf(task='mmedu',checkpoint='cls.onnx')# 指定使用的onnx模型
result, result_img =  mmcls.inference(data='test.jpg',img_type='cv2')# 进行模型推理
format_result = mmcls.format_output(lang="zh")# 推理结果格式化输出
mmcls.show(result_img)# 展示推理结果图片
mmcls.save(result_img,'new.jpg')# 保存推理结果图片
编写一个人工智能应用并没有那么困难，比如可以借助Gradio这个开源的用于快速原型设计和部署机器学习模型的交互式界面的工具库就能快速搭建一个简易的模型展示应用，如下代码可实现在一个网页上传一张图片，返回推理结果。
```python
import gradio as gr
from XEdu.hub import Workflow as wf
mm = wf(task='mmedu',checkpoint='cls.onnx') 
def predict(img):
    res,img = mm.inference(data=img,img_type='cv2') # 模型推理
    result = mm.format_output(lang="zh") # 标准化推理结果
    text1 = '预测结果：'+result['预测结果']
    text2 = '标签：'+str(result['标签'])
    return text1,text2
image = gr.Image(type="filepath")
demo = gr.Interface(fn=predict, inputs=image, outputs=["text","text"])
demo.launch(share=True)
```
更多模型转换和应用的内容请看模型转换和应用(https://xedu.readthedocs.io/zh/master/how_to_use/support_resources/model_convert.html)。

案例五：用MMEdu训练SSD_Lite目标检测模型（猫狗）
项目说明：
MMEdu是人工智能视觉算法集成的深度学习开发工具，目前目标检测模块MMDetection支持的SOTA模型有SSD_Lite、FaterRCNN、Yolov3等，支持训练的数据集格式为COCO，更多关于MMDetection功能详见请前往揭秘MMEdu的目标检测模块。本项目使用MMEdu的目标检测模块MMDetection，根据猫狗多目标COCO数据集，训练SSD_Lite模型实现猫狗目标检测。
项目地址：https://openinnolab.org.cn/pjlab/project?id=64055f119c0eb14f22db647c&sc=62f34141bf4f550f3e926e0e#public
数据集：本项目使用的是浦育平台公开的猫狗目标检测数据集。目标检测模块MMDetection支持的数据集是COCO格式。如何从零开始制作COCO数据集详见从零开始制作一个COCO格式数据集。
项目步骤：
任务一：训练SSD_Lite猫狗目标检测模型
第0步 导入基础库（建议将库更新为最新版本再导入）
python
from MMEdu import MMDetection as det
第1步 实例化模型（选择SSD_Lite）
python
model = det(backbone='SSD_Lite')
第2步 配置基本信息
AI模型训练需要配置的基本信息有三类，分别是：图片分类的类别数量（model.num_classes），模型保存的路径（model.save_fold）和数据集的路径（model.load_dataset）。
python
model.num_classes = 2 # 猫和狗共2类
model.load_dataset(path='/data/H47U12/cat_dog_det') 
model.save_fold = 'checkpoints/det_model/catdogs'
第3步 开始训练模型
python
model.train(epochs=10 ,lr=0.001,batch_size=4, validate=True)
训练过程中观察输出的每一轮bbox_mAP的变化，判断模型在验证集上的准确率。
任务二：基于预训练模型继续训练
全新开始训练一个模型，一般要花较长时间。因此我们强烈建议在预训练模型的基础上继续训练，哪怕你要分类的数据集和预训练的数据集并不一样。
```python
model.num_classes = 2 # 猫和狗共2类
model.load_dataset(path='/data/H47U12/cat_dog_det') 
预训练模型权重路线
checkpoint = 'checkpoints/pretrain_ssdlite_mobilenetv2.pth'
model.save_fold = 'checkpoints/det_model/catdogs_pretrain' 
启动cpu容器将device='cpu'，启动GPU容器将device='cuda'
model.train(epochs=10, lr=0.001, validate=True, batch_size = 4, device='cuda', checkpoint=checkpoint)
```
预训练模型下载地址：https://p6bm2if73b.feishu.cn/drive/folder/fldcnxios44vrIOV9Je3wPLmExf
注：一般训练目标检测模型耗时较久，浦育平台可启动GPU服务器，建议去浦育平台完成模型训练，启动GPU服务器后便可以在训练参数中添加device='cuda'启动GPU训练。
任务三：模型测试（用新的图片完成推理）
第0步 导入基础库（建议将库更新为最新版本再导入）
from MMEdu import MMClassification as cls
第1步 实例化模型
python
model = cls(backbone='SSD_Lite')
第2步 指定模型权重文件的所在路径
python
checkpoint = 'checkpoints/det_model/best_bbox_mAP_epoch_7.pth' # 指定权重文件路径
第1步和第2步的模型需对应，首先模型权重需存在，同时还需该模型训练时实例化模型时选择的网络与推理时一致。
第3步 指定图片
python
img_path = 'picture/2.png' # 指定图片路径
第4步 开始推理
python
result = model.inference(image=img, show=True, checkpoint = checkpoint,device='cuda') # 模型推理
model.print_result(result) # 结果转换为中文输出
上文简单介绍了如何用MMEdu训练一个目标检测模型，更多关于MMEdu模型训练和推理的方法详见请前往揭秘MMEdu的目标检测模块。
任务四：模型转换和应用
同样的，可以在模型应用前先完成模型转换，目标检测模型转换的代码风格和图像分类类似。
python
from MMEdu import MMDetection as det
model = det(backbone='SSD_Lite')
checkpoint = 'checkpoints/best_bbox_mAP_epoch_7.pth'
out_file='cats_dogs_det.onnx' # 指定输出的文件即转换后的文件
model.convert(checkpoint=checkpoint, backend="ONNX", out_file=out_file)
模型应用的基础代码：
python
from XEdu.hub import Workflow as wf
mmdet = wf(task='mmedu',checkpoint='cats_dogs_det.onnx')# 指定使用的onnx模型
result, result_img =  mmdet.inference(data='/data/H47U12/cat_dog_det/images/valid/001.jpg',img_type='cv2')# 进行模型推理
format_result = mmdet.format_output(lang="zh")# 推理结果格式化输出
mmdet.show(result_img)# 展示推理结果图片
mmdet.save(result_img,'new.jpg')# 保存推理结果图片
此时您可以挑选自己熟悉的硬件，去做自己训练并完成转换的模型部署啦，只需要下载转换的ONNX模型，在硬件上安装库即可。最简单的方式是借助摄像头，再使用OpenCV这个轻松完成图像和视频处理的工具库，实现猫狗实时检测。
python
from XEdu.hub import Workflow as wf
import cv2
cap = cv2.VideoCapture(0)
mmdet = wf(task='mmedu',checkpoint='cats_dogs_det.onnx')
while cap.isOpened():
    ret, img = cap.read()
    if not ret:
        break
    result, result_img=  mmdet.inference(data=img,img_type='cv2')
    format_result = mmdet.format_output(lang="zh")
    cv2.imshow('video', result_img)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break    
cap.release()
cv2.destroyAllWindows()
更多模型应用与部署的介绍详见模型应用与部署。

使用说明
欢迎各位小伙伴们来到XEdu的AI天地，要想“畅游”XEdu，入门手册少不了，接下来将会一一介绍XEdu的奇妙AI工具们，带领大家快速了解XEdu各个模块的功能和特点，此外，我们还提供了链接“传送门”，方便小伙伴们轻松玩转AI。
XEdu的使用有两种：网页或者本地
网页推荐使用浦育平台，里面有XEdu容器，能够省去环境的配置，并且每周会有一定额度的免费远程算力，上手更加轻松。
本地环境的安装推荐选择XEdu一键安装包（CPU版本），它满足大部分机房需求。
案例一：用XEduHub执行推理任务（检测任务）
简介：
XEduHub针对一些常见任务，提供了现成的优质模型，可以完成目标检测、关键点检测等等，还可以实现自训练模型推理，让初学者能轻松进行AI应用实践。本项目完成了直接调用XEduHub一个内置模型det_hand实现检测手的功能，只用7行代码就可实现。
链接：
案例详解：用XEduhub执行推理任务（检测任务）
https://xedu.readthedocs.io/zh/master/how_to_quick_start/how_to_start_hub.html#
项目链接：用XEduhub执行推理任务（检测任务）
https://www.openinnolab.org.cn/pjlab/project?id=6612327ea888634b8a6de8f6&backpath=/pjlab/projects/list#public
案例二：用BaseML训练机器学习模型（抛物线）
简介：
BaseML库提供了众多机器学习训练方法，如线性回归、KNN、SVM等等，可以快速训练和应用模型。本项目使用BaseML中的回归算法，以及其他算法训练投石车落地距离预测模型。投石车落地距离预测是一个典型的抛物线问题，根据投石角度与距离对照表，用机器学习方法预测抛物线函数。
链接：
案例详解：用BaseML训练机器学习模型（抛物线）
https://xedu.readthedocs.io/zh/master/how_to_quick_start/how_to_start_baseml.html#
项目链接：用BaseML训练机器学习模型（抛物线）
https://www.openinnolab.org.cn/pjlab/project?id=66123df8e0ea4a2a08b20464&backpath=/pjlab/projects/list#public
案例三：用BaseNN训练搭建全连接神经网络（鸢尾花）
简介：
BaseNN可以方便地逐层搭建神经网络，支持搭建CNN和RNN，或二者的结合，训练深度学习模型。本项目核心功能是完成使用经典的鸢尾花数据集完成鸢尾花分类，最后完成了一个简单的鸢尾花分类小应用，输入花萼长度、宽度、花瓣长度、宽度，可以输出预测结果。
链接：
案例详解：用BaseNN训练搭建全连接神经网络（鸢尾花）
https://xedu.readthedocs.io/zh/master/how_to_quick_start/how_to_start_basenn.html#
项目链接：
用BaseNN库搭建全连接神经网络训练IRIS鸢尾花分类模型
https://openinnolab.org.cn/pjlab/project?id=641bc2359c0eb14f22fdbbb1&backpath=/pjlab/projects/list#public
案例四：用MMEdu训练LeNet图像分类模型（手写体）
简介：
MMEdu是人工智能视觉算法集成的深度学习开发工具。本项目使用MMEdu的图像分类模块MMClassification，根据经典的手写体ImageNet格式数据集，训练LeNet模型实现手写体识别。此外目前MMClassifiation支持的SOTA模型有LeNet、MobileNet、ResNet18、ResNet50等，支持训练的数据集格式为ImageNet。
链接：
案例详解：用MMEdu训练LeNet图像分类模型（手写体）
https://xedu.readthedocs.io/zh/master/how_to_quick_start/how_to_start_mmcls.html#
项目链接：用MMEdu实现MNIST手写体数字识别（XEdu官方版）
https://openinnolab.org.cn/pjlab/project?id=64a3c64ed6c5dc7310302853&sc=62f34141bf4f550f3e926e0e#public
案例五：用MMEdu训练SSD_Lite目标检测模型（猫狗）
简介：
MMEdu是人工智能视觉算法集成的深度学习开发工具。本项目使用MMEdu的目标检测模块MMDetection，根据猫狗多目标COCO数据集，训练SSD_Lite模型实现猫狗目标检测。此外此外目前MMDetection支持的SOTA模型有SSD_Lite、FaterRCNN、Yolov3等，支持训练的数据集格式为COCO。
链接：
案例详解：用MMEdu训练SSD_Lite目标检测模型（猫狗）
https://xedu.readthedocs.io/zh/master/how_to_quick_start/how_to_start_mmdet.html
项目链接：用MMEdu解决目标检测问题（以猫狗检测为例）
https://openinnolab.org.cn/pjlab/project?id=64055f119c0eb14f22db647c&sc=62f34141bf4f550f3e926e0e#public
案例六：综合项目石头剪刀布的实时识别（XEduhub+BaseNN）
简介：
组合XEdu的工具完成一个综合项目非常方便，本项目使用XEduHub提取手势图像的关键点信息，再将这些关键点信息作为特征输入到一个自己搭建的全连接神经网络模型中进行训练，此步骤由BaseNN实现，最后到本地完成模型应用。
链接：
案例详解：综合项目石头剪刀布的实时识别（XEduhub+BaseNN）
https://xedu.readthedocs.io/zh/master/how_to_quick_start/how_to_make_a_small_project.html#
项目链接：用XEduHub和BaseNN完成石头剪刀布手势识别
https://openinnolab.org.cn/pjlab/project?id=66062a39a888634b8a1bf2ca&backpath=/pjedu/userprofile?slideKey=project#public

