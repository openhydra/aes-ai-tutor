XEduHub安装
你可以通过pip命令来安装XEduHub。
pip install xedu-python 或 pip install XEdu-python
更新库文件：
python
pip install --upgrade XEdu-python
XEdu-python已经内置在XEdu的一键安装包中，解压后即可使用。

XEduHub功能详解
XEduHub作为一个深度学习工具库，集成了许多深度学习领域优质的SOTA模型，能够帮助用户在不进模型训练的前提下，用少量的代码，快速实现计算机视觉、自然语言处理等多个深度学习领域的任务。
XEduHub支持两类任务，分为内置任务和通用任务两种。顾名思义，内置任务指预置的各种优质开源AI模型，包含了图像分类、目标检测、关键点检测、OCR等。通用任务指用XEdu系列工具训练的模型，如MMEdu（计算机视觉）、BaseNN（自定义神经网络）和BaseML（传统机器学习），还支持用其他工具训练的模型。考虑到兼容性，大部分模型以ONNX为主。不管使用哪类任务，XEduHub都使用Workflow工具进行推理，核心代码仅4行，语法非常简洁。
通用模型推理工具Workflow
我们在使用XEduHub时都需要执行这段代码from XEdu.hub import Workflow as wf。Workflow的基本逻辑是使用训练好的模型对数据进行推理。
那什么是Workflow呢？在使用XEduHub里的单个模型时，Workflow就是模型推理的工作流，从数据，到输入模型，再到输出推理结果。在使用XEduHub里多个模型进行联动时，Workflow可以看做不同模型之间的数据流动，例如首先进行多人的目标检测，将检测到的数据传入关键点识别模型从而对每个人体进行关键点识别。
下面开始介绍Workflow中丰富的深度学习工具。
强烈安利项目XEduHub实例代码-入门完整版
https://www.openinnolab.org.cn/pjlab/project?id=65518e1ae79a38197e449843&backpath=/pjlab/projects/list#public
通过学习“XEduHub实例代码-入门完整版”，可以在项目实践中探索XEduHub的魅力，项目中通俗易懂的讲解和实例代码也能帮助初学者快速入门XEduHub。
1. 目标检测
目标检测是一种计算机视觉任务，其目标是在图像或视频中检测并定位物体的位置，并为每个物体分配类别标签。
实现目标检测通常包括特征提取、物体位置定位、物体类别分类等步骤。这一技术广泛应用于自动驾驶、安全监控、医学影像分析、图像搜索等各种领域，为实现自动化和智能化应用提供了关键支持。
XEduHub目标支持目标检测任务有：coco目标检测det_coco、人体目标检测det_body、人脸目标检测det_face和人手目标检测det_hand。
coco目标检测
COCO（Common Objects in Context）是一个用于目标检测和图像分割任务的广泛使用的数据集和评估基准。它是计算机视觉领域中最重要的数据集之一，在XEduHub中的该模型能够检测出80类coco数据集中的物体：det_coco，以及加强版det_coco_l。
代码样例
python
from XEdu.hub import Workflow as wf
det_coco = wf(task='det_coco')
result,img_with_box = det_coco.inference(data='data/det_coco.jpg',img_type='pil') # 进行模型推理
format_result = det_coco.format_output(lang='zh') # 将推理结果进行格式化输出
det_coco.show(img_with_box) # 展示推理图片
det_coco.save(img_with_box,'img_with_box.jpg')# 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
det_coco = wf(task='det_coco')
wf()中共有三个参数可以设置：
task选择任务。coco目标检测的模型为det_coco, det_coco_l。det_coco_l相比det_coco模型规模较大，性能较强，但是推理的速度较慢。
checkpoint指定模型文件所在的路径，如det_coco = wf(task='det_coco',checkpoint='det_coco.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即det_coco.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
result = det_coco.inference(data='data/det_coco.jpg')
推理方式2：
python
result,img_with_box = det_coco.inference(data='data/det_coco.jpg',img_type='pil')
模型推理inference()可传入参数：
data(string|numpy.ndarray): 指定待目标检测的图片。
show(flag): 可取值[True,False] ,如果取值为True，在推理完成后会直接输出目标检测完成后的图片，默认为False。
img_type(string): 目标检测完成后会返回含有目标检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，即如果不传入值，则不会返回图。
thr(float): 设置检测框阈值，取值范围为[0,1]超过该阈值的检测框被视为有效检测框，进行显示。
target_class(string)：该参数在使用det_coco的时候可以指定要检测的对象，如：person，cake等等。
若要查看coco目标检测中的所有类别可运行以下代码：
python
wf.coco_class()
```
输出结果
{1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorbike', 5: 'aeroplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 12: 'stop sign', 13: 'parking meter', 14: 'bench', 15: 'bird', 16: 'cat', 17: 'dog', 18: 'horse', 19: 'sheep', 20: 'cow', 21: 'elephant', 22: 'bear', 23: 'zebra', 24: 'giraffe', 25: 'backpack', 26: 'umbrella', 27: 'handbag', 28: 'tie', 29: 'suitcase', 30: 'frisbee', 31: 'skis', 32: 'snowboard', 33: 'sports ball', 34: 'kite', 35: 'baseball bat', 36: 'baseball glove', 37: 'skateboard', 38: 'surfboard', 39: 'tennis racket', 40: 'bottle', 41: 'wine glass', 42: 'cup', 43: 'fork', 44: 'knife', 45: 'spoon', 46: 'bowl', 47: 'banana', 48: 'apple', 49: 'sandwich', 50: 'orange', 51: 'broccoli', 52: 'carrot', 53: 'hot dog', 54: 'pizza', 55: 'donut', 56: 'cake', 57: 'chair', 58: 'sofa', 59: 'pottedplant', 60: 'bed', 61: 'diningtable', 62: 'toilet', 63: 'tvmonitor', 64: 'laptop', 65: 'mouse', 66: 'remote', 67: 'keyboard', 68: 'cell phone', 69: 'microwave', 70: 'oven', 71: 'toaster', 72: 'sink', 73: 'refrigerator', 74: 'book', 75: 'clock', 76: 'vase', 77: 'scissors', 78: 'teddy bear', 79: 'hair drier', 80: 'toothbrush'}
```
模型推理返回结果：
result：以二维数组的形式保存了检测框左上角顶点的坐标(x1,y1)和右下角顶点的坐标(x2,y2)（之所以是二维数组，是因为该模型能够检测多个对象，因此当检测到多个对象时，就会有多个[x1,y1,x2,y2]的一维数组，所以需要以二维数组形式保存），我们可以利用这四个数据计算出其他两个顶点的坐标，以及检测框的宽度和高度。
# reslut
  array([[  4.14969308, 164.72434085, 157.05228533, 260.79559871],
         [548.76094273, 322.64147513, 585.47729492, 400.10044466],
         [291.30113874, 216.19574056, 378.10302734, 325.63501167],
         [362.52070836, 215.27021027, 442.51865932, 318.6803018 ],
         [547.75268555, 297.44887938, 587.52868652, 399.59864902],
         [214.87919399, 230.39230279, 302.72096906, 321.90567071],
         [548.62234933, 330.30918053, 585.72174072, 399.95723615],
         [198.86025565, 178.02907828, 261.54931205, 215.89466899]])
img_with_box：：是个三维数组，以cv2格式保存了包含了检测框的图片。
3. 结果输出
python
format_result = det_coco.format_output(lang='zh') # 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
{'检测框': [[4.149693080357143,
          164.72434084756034,
          157.05228533063615,
          260.79559871128623],
         [548.7609427315848,
          322.64147513253346,
          585.477294921875,
          400.100444657462],
         [291.3011387416295,
          216.19574056352886,
          378.10302734375,
          325.63501167297363],
         [362.5207083565848,
          215.27021026611328,
          442.51865931919644,
          318.68030180249895],
         [547.752685546875,
          297.44887937818254,
          587.5286865234375,
          399.5986490249634],
         [214.8791939871652,
          230.39230278560092,
          302.7209690638951,
          321.90567071097234],
         [548.6223493303571,
          330.30918053218295,
          585.7217407226562,
          399.95723615373885],
         [198.8602556501116,
          178.02907827922274,
          261.549312046596,
          215.8946689878191]],
 '分数': [0.74398744,
        0.49514472,
        0.4024352,
        0.3905959,
        0.3684283,
        0.34017423,
        0.33570302,
        0.3242398],
 '类别': ['tvmonitor',
        'vase',
        'chair',
        'chair',
        'bottle',
        'chair',
        'bottle',
        'pottedplant']}
```
format_result以字典形式存储了推理结果，共有三个键：检测框、分数和类别。检测框以二维数组形式保存了每个检测框的坐标信息[x1,y1,x2,y2]，而分数则是对应下标的检测框的置信度，以一维数组形式保存，类别则是检测框中对象所属的类别，以一维数组形式保存。
python
det_coco.show(img_with_box) # 展示推理图片
show()能够输出带有检测框以及对应类别的结果图像。
4. 结果保存
python
det_coco.save(img_with_box,'img_with_box.jpg') # 保存推理图片
save()方法能够保存带有检测框以及对应类别的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
人体目标检测
人体目标检测的任务是在图像或视频中检测和定位人体的位置，并为每个检测到的人体分配一个相应的类别标签。借助此任务，一般可以实现人体画面的识别与提取，为后续的图像处理和分析提供基础。
XEduHub提供了进行人体目标检测的模型：det_body，det_body_l，这两个模型能够进行人体目标检测，后者为增强版模型。
代码样例
python
from XEdu.hub import Workflow as wf
det_body = wf(task='det_body')
result,img_with_box = det_body.inference(data='data/det_body.jpg',img_type='pil') # 进行模型推理
format_result = det_body.format_output(lang='zh') # 将推理结果进行格式化输出
det_body.show(img_with_box)# 展示推理图片
det_body.save(img_with_box,'img_with_box.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
det_body = wf(task='det_body')
wf()中共有三个参数可以设置：
task选择任务。人体目标检测模型为det_body, det_body_l。det_body_l相比det_body模型规模较大，性能较强，但是推理的速度较慢。
checkpoint指定模型文件所在的路径，如det_body = wf(task='det_body',checkpoint='det_body.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即det_body.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
result = det_body.inference(data='data/det_body.jpg') # 进行模型推理
推理方式2：
python
result,img_with_box = det_body.inference(data='data/det_body.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data(string|numpy.ndarray): 指定待目标检测的图片。
show(flag): 可取值[True,False] ,如果取值为True，在推理完成后会直接输出目标检测完成后的图片，默认为False。
img_type(string): 目标检测完成后会返回含有目标检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，即如果不传入值，则不会返回图。
thr(float): 设置检测框阈值，取值范围为[0,1]超过该阈值的检测框被视为有效检测框，进行显示。
模型推理返回结果：
result：变量boxes以二维数组的形式保存了检测框左上角顶点的坐标(x1,y1)和右下角顶点的坐标(x2,y2)（之所以是二维数组，是因为该模型能够检测多个人体，因此当检测到多个人体时，就会有多个[x1,y1,x2,y2]的一维数组，所以需要以二维数组形式保存），我们可以利用这四个数据计算出其他两个顶点的坐标，以及检测框的宽度和高度。
img_with_box：：是个三维数组，以cv2格式保存了包含了检测框的图片。
3. 结果输出
python
format_result = det_body.format_output(lang='zh')# 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，共有两个键：检测框、分数。检测框以二维数组形式保存了每个检测框的坐标信息[x1,y1,x2,y2]，而分数则是对应下标的检测框的置信度，以一维数组形式保存。
```
输出结果
{'检测框': [[323.2777170453753,
          72.95395088195801,
          917.5945354189192,
          1130.7357228142876]],
 '分数': [0.8851305]}
```
python
det_body.show(img_with_box)# 展示推理图片
show()能够输出带有检测框的结果图像。
4. 结果保存
python
det_body.save(img_with_box,'img_with_box.jpg')# 保存推理图片
save()方法能够保存带有检测框的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
人脸目标检测
人脸目标检测指的是检测和定位一张图片中的人脸。XEduHub使用的是opencv的人脸检测模型，能够快速准确地检测出一张图片中所有的人脸。XEduHub提供了进行人脸目标检测的模型：det_face，能够快速准确地检测出图片中的所有人脸。
代码样例
python
from XEdu.hub import Workflow as wf
det_face = wf(task='det_face')
result,img_with_box = det_face.inference(data='data/det_face.jpg',img_type='pil') # 进行模型推理
format_result = det_face.format_output(lang='zh') # 将推理结果进行格式化输出
det_face.show(img_with_box) # 展示推理图片
det_face.save(img_with_box,'img_with_box.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
det_face = wf(task='det_face')
wf()中共有三个参数可以设置：
task选择任务。人脸目标检测模型为det_face。
checkpoint指定模型文件所在的路径，如det_face = wf(task='det_face',checkpoint='det_face.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即det_face.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
result = det_face.inference(data='data/det_face.jpg') # 进行模型推理
推理方式2：
python
result,img_with_box = det_face.inference(data='data/det_face.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data(string|numpy.ndarray): 指定待目标检测的图片。
show(flag): 可取值[True,False] ,如果取值为True，在推理完成后会直接输出目标检测完成后的图片，默认为False。
img_type(string): 目标检测完成后会返回含有目标检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，即如果不传入值，则不会返回图。
thr(float): 设置检测框阈值，取值范围为[0,1]超过该阈值的检测框被视为有效检测框，进行显示。
minSize(tuple(int,int))：检测框的最小尺寸，小于该尺寸的目标会被过滤掉，默认为(50,50)。
maxSize(tuple(int,int))：检测框的最大尺寸,大于该尺寸的目标会被过滤掉，默认为输入图像的大小。
scaleFactor(float)：该参数用于缩放图像，以便在检测过程中使用不同大小的窗口来识别人脸。较小的值会导致检测速度加快，但可能会错过一些小的人脸；较大的值可以提高检测的准确性，但会减慢检测速度。通常，这个值会在1.1到1.5之间进行调整，默认为1.1。
minNeighbors(int)：该参数定义了构成检测目标的最小邻域矩形个数。如果这个值设置得太高，可能会导致检测器过于严格，错过一些实际的人脸；如果设置得太低，检测器可能会变得过于宽松，错误地检测到非人脸区域。通常，这个值会在2到10之间进行调整，默认为5。
模型推理返回结果：
result：以二维数组的形式保存了检测框左上角顶点的坐标(x1,y1)和右下角顶点的坐标(x2,y2)（之所以是二维数组，是因为该模型能够检测多个人脸，因此当检测到多个人脸时，就会有多个[x1,y1,x2,y2]的一维数组，所以需要以二维数组形式保存），我们可以利用这四个数据计算出其他两个顶点的坐标，以及检测框的宽度和高度。
img_with_box：：是个三维数组，以cv2格式保存了包含了检测框的图片。
3. 结果输出
python
format_result = det_face.format_output(lang='zh')# 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，只有一个键：检测框。检测框以二维数组形式保存了每个检测框的坐标信息[x1,y1,x2,y2]。需要注意的是由于使用的为opencv的人脸检测模型，因此在format_output时缺少了分数这一指标。
```
输出结果
{'检测框': [[202, 237, 940, 975]]}
```
结果可视化：
python
det_face.show(img_with_box) # 展示推理图片
show()能够输出带有检测框的结果图像。
多人脸检测结果如下：（如果看到很多不该识别为人脸的地方出现了检测框，我们可以在推理是增加minSize参数来设置检测框的最小尺寸，将小于该尺寸的目标过滤掉，默认为(50,50)，同时也可以增加maxSize参数来设置检测框的最大尺寸，大于该尺寸的目标会被过滤掉，默认为输入图像的大小）
4. 结果保存
python
det_face.save(img_with_box,'img_with_box.jpg')# 保存推理图片
save()方法能够保存带有检测框的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
手部目标检测
手部目标检测指的是检测和定位一张图片中的人手。XEduHub提供了进行手部目标检测的模型：det_hand，能够快速准确地检测出图片中的所有人手。
代码样例
python
from XEdu.hub import Workflow as wf
det_hand = wf(task='det_hand')
result,img_with_box = det_hand.inference(data='data/det_hand.jpg',img_type='pil') # 进行模型推理
format_result = det_hand.format_output(lang='zh') # 将推理结果进行格式化输出
det_hand.show(img_with_box) # 展示推理图片
det_hand.save(img_with_box,'img_with_box.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
det_hand = wf(task='det_hand')
wf()中共有三个参数可以设置：
task选择任务。手部关键点提取模型为det_hand。
checkpoint指定模型文件所在的路径，如det_hand = wf(task='det_hand',checkpoint='det_hand.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即det_hand.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
result = det_hand.inference(data='data/det_hand.jpg') # 进行模型推理
推理方式2：
python
result,img_with_box = det_hand.inference(data='data/det_hand.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data：指定待检测的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出目标检测完成后的图片。
img_type：目标检测完成后会返回含有检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
thr: 设置检测框阈值，取值范围为[0,1]超过该阈值的检测框被视为有效检测框，进行显示。
模型推理返回结果：
result：以二维数组的形式保存了检测框左上角顶点的坐标(x1,y1)和右下角顶点的坐标(x2,y2)（之所以是二维数组，是因为该模型能够检测多个人手，因此当检测到多个人手时，就会有多个[x1,y1,x2,y2]的一维数组，所以需要以二维数组形式保存），我们可以利用这四个数据计算出其他两个顶点的坐标，以及检测框的宽度和高度。
img_with_box：：是个三维数组，以cv2格式保存了包含了检测框的图片。
3. 结果输出
python
format_result = det_hand.format_output(lang='zh') # 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
{'检测框': [[354.9846431187221,
          171.7757524762835,
          993.5257284981864,
          867.7952793666294]],
 '分数': [0.8558253]}
```
format_result以字典形式存储了推理结果，共有两个键：检测框、分数。检测框以二维数组形式保存了每个检测框的坐标信息[x1,y1,x2,y2]，而分数则是对应下标的检测框的置信度，以一维数组形式保存。
结果可视化：
python
det_hand.show(img_with_box) # 展示推理图片
show()能够输出带有检测框的结果图像。
4. 结果保存
python
det_hand.save(img_with_box,'img_with_box.jpg') # 保存推理图片
save()方法能够保存带有检测框的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
2. 关键点识别
关键点识别是深度学习中的一项关键任务，旨在检测图像或视频中的关键位置，通常代表物体或人体的重要部位。XEduHub支持的关键点识别任务有：人体关键点pose_body、人脸关键点pose_face、人手关键点pose_hand和所有人体关键点识别pose_wholebody。
注意事项：这里我们强烈建议提取关键点之前应先进行目标检测。
例如进行人体关键点检测pose_body之前，先使用det_body在图片中检测中人体目标，提取人体画面，再对每个人体目标进行更加精准的关键点检测。可参考项目XEduHub实例代码-入门完整版中的 “3-1 综合项目：目标检测+关键点检测”。
当然关键点识别也可以单独用，但是效果并不保证。
人体关键点识别
人体关键点识别是一项计算机视觉任务，旨在检测和定位图像或视频中人体的关键位置，通常是关节、身体部位或特定的解剖结构。
这些关键点的检测可以用于人体姿态估计和分类、动作分析、手势识别等多种应用。
XEduHub提供了三个识别人体关键点的优质模型:pose_body17,pose_body17_l和pose_body26，能够在使用cpu推理的情况下，快速识别出身体的关键点。
模型的数字表示识别出人体关键点的数量，l代表了large，表示规模较大的，性能较强的模型，但是缺点在于推理速度较慢。
pose_body17与pose_body17_l模型能识别出17个人体骨骼关键点，pose_body26模型能识别出26个人体骨骼关键点。
代码样例
python
from XEdu.hub import Workflow as wf
body = wf(task='pose_body') # 数字可省略，当省略时，默认为pose_body17
keypoints,img_with_keypoints = body.inference(data='data/body.jpg',img_type='pil') # 进行模型推理
format_result = body.format_output(lang='zh') # 将推理结果进行格式化输出
body.show(img_with_keypoints) # 展示推理图片
body.save(img_with_keypoints,'img_with_keypoints.jpg')  # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
body = wf(task='pose_body') # 数字可省略，当省略时，默认为pose_body17
wf()中共有三个参数可以设置：
task选择任务。在人体关键点识别模型中，task可选取值为：[pose_body17,pose_body17_l,pose_body26]，默认为pose_body17。
checkpoint指定模型文件所在的路径，如pose_body = wf(task='pose_body',checkpoint='pose_body.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即pose_body17.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
keypoints = body.inference(data='data/body.jpg') # 进行模型推理
推理方式2：
python
keypoints,img_with_keypoints = body.inference(data='data/body.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data: 指定待识别关键点的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出关键点识别完成后的图片。
img_type: 关键点识别完成后会返回含有关键点的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
bbox：该参数可配合目标检测结果使用。例如在多人手关键点检测任务中，我们先识别人手位置，再检测手部关键点，该参数指定了要识别哪个目标检测框中的关键点。
模型推理返回结果：
keypoints以二维数组的形式保存了所有关键点的坐标，每个关键点(x,y)被表示为[x,y]根据前面的图示，要获取到某个特定序号i的关键点，只需要访问keypoints[i]即可。
img_with_keypoints是个三维数组，以对应img_type格式保存了关键点识别完成后图片的像素点信息。
3. 结果输出
python
format_result = body.format_output(lang='zh') # 参数lang设置了输出结果的语言，默认为中文
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，共有两个键：关键点坐标和分数。关键点坐标以二维数组形式保存了每个关键点的[x,y]坐标，而分数则是对应下标的关键点的分数，以一维数组形式保存。
```
输出结果
{'关键点坐标': [[596.3159722222222, 163.53819444444457],
           [624.6579861111111, 140.86458333333326],
           [576.4765625, 149.3671875],
           [658.6684027777778, 166.37239583333326],
           [553.8029513888889, 177.7092013888889],
           [735.1918402777778, 288.24305555555543],
           [511.28993055555566, 322.2534722222222],
           [871.2335069444446, 387.44010416666674],
           [406.4244791666665, 407.2795138888889],
           [789.0416666666667, 347.7612847222222],
           [369.5798611111111, 381.7717013888889],
           [735.1918402777778, 642.5182291666667],
           [590.6475694444443, 656.6892361111111],
           [831.5546875, 832.4097222222224],
           [516.9583333333335, 855.0833333333333],
           [825.8862847222222, 1013.7986111111111],
           [488.61631944444434, 1039.306423611111]],
 '分数': [0.9026135802268982,
        0.9218268990516663,
        0.9566612243652344,
        0.9752001762390137,
        0.9569406509399414,
        0.9008727669715881,
        0.8631673455238342,
        0.9160149693489075,
        0.7716920375823975,
        0.7274723649024963,
        0.7813198566436768,
        0.8071638941764832,
        0.8179947733879089,
        0.8660239577293396,
        0.8697924613952637,
        0.9618276953697205,
        0.9472517371177673]}
```
结果可视化：
python
body.show(img_with_keypoints)
show()能够输出带有关键点和关键点连线的结果图像。
若此时发现关键点识别效果不佳，关键点乱飞，我们可以果断采用在提取关键点之前先进行目标检测的方式。如当前任务'pose_body'，就可以在之前先进行'det_body'。详情可参考项目XEduHub实例代码-入门完整版中的 “3-1 综合项目：目标检测+关键点检测”。
4. 结果保存
python
body.save(img_with_keypoints,'img_with_keypoints.jpg')
save()方法能够保存保存带有关键点和关键点连线结果图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
人脸关键点
人脸关键点识别是计算机视觉领域中的一项任务，它的目标是检测和定位人脸图像中代表面部特征的重要点，例如眼睛、鼻子、嘴巴、眉毛等。这些关键点的准确定位对于许多应用非常重要，包括人脸识别、表情分析、虚拟化妆、人机交互等。
XEduHub提供了识别人脸关键点的模型：pose_face106，这意味着该模型能够识别人脸上的106个关键点。如下图所示是106个关键点在脸部的分布情况，我们可以利用这些关键点的分布特征进行人脸识别，或者对人的表情进行分析和分类等。
代码样例
python
from XEdu.hub import Workflow as wf
face = wf(task='pose_face') # 数字可省略，当省略时，默认为pose_face106
keypoints,img_with_keypoints = face.inference(data='data/face.jpg',img_type='pil') # 进行模型推理
format_result = face.format_output(lang='zh') # 将推理结果进行格式化输出
face.show(img_with_keypoints) # 展示推理图片
face.save(img_with_keypoints,'img_with_keypoints.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
face = wf(task='pose_face') # 数字可省略，默认为face106
wf()中共有三个参数可以设置：
task选择任务。人脸关键点识别模型为pose_face106（数字可省略，默认为pose_face）。
checkpoint指定模型文件所在的路径，如pose_face = wf(task='pose_face',checkpoint='pose_face.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即pose_face106.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
keypoints = face.inference(data='data/face.jpg') # 进行模型推理
推理方式2：
python
keypoints,img_with_keypoints = face.inference(data='data/face.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data: 指定待识别关键点的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出关键点识别完成后的图片。
img_type: 关键点识别完成后会返回含有关键点的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
bbox：该参数可配合目标检测使用。在多人手关键点检测中，该参数指定了要识别哪个检测框中的关键点。
模型推理返回结果：
keypoints以二维数组的形式保存了所有关键点的坐标，每个关键点(x,y)被表示为[x,y]根据前面的图示，要获取到某个特定序号i的关键点，只需要访问keypoints[i]即可。
img_with_keypoints是个三维数组，以对应img_type格式保存了关键点识别完成后图片的像素点信息。
3. 结果输出
python
format_result = face.format_output(lang='zh') # 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，共有两个键：关键点坐标和分数。关键点坐标以二维数组形式保存了每个关键点的[x,y]坐标（face106模型共检测出106个关键点），而分数则是对应下标的关键点的分数，以一维数组形式保存。
结果可视化
python
face.show(img_with_keypoints) # 展示推理图片
show()能够输出带有关键点的结果图像。
若此时发现关键点识别效果不佳，关键点乱飞，我们可以果断采用在提取关键点之前先进行目标检测的方式。如当前任务'pose_face'，就可以在之前先进行'det_face'。详情可参考项目XEduHub实例代码-入门完整版中的 “3-1 综合项目：目标检测+关键点检测”。
4. 结果保存
python
face.save(img_with_keypoints,'img_with_keypoints.jpg') # 保存推理图片
save()方法能够保存带有关键点的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
人手关键点
人手关键点识别是一项计算机视觉任务，其目标是检测和定位图像或视频中人手的关键位置，通常包括手指、手掌、手腕等关键部位的位置。这些关键点的识别对于手势识别、手部姿态估计、手部追踪、手势控制设备等应用具有重要意义。
XEduHub提供了能够快速识别人手关键点的模型：pose_hand21，该模型能够识别人手上的21个关键点，如下图所示。你可以根据自身需要对关键点进行进一步处理。例如：手势的不同会体现在关键点位置的分布上，这样就可以利用这些关键点进行手势的分类和识别。
代码样例
python
from XEdu.hub import Workflow as wf
hand = wf(task='pose_hand') # 数字可省略，当省略时，默认为pose_hand21
keypoints,img_with_keypoints = hand.inference(data='data/hand.jpg',img_type='pil') # 进行模型推理
format_result = hand.format_output(lang='zh') # 将推理结果进行格式化输出
hand.show(img_with_keypoints) # 展示推理图片
hand.save(img_with_keypoints,'img_with_keypoints.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
hand = wf(task='pose_hand') # 数字可省略，当省略时，默认为pose_hand21
wf()中共有三个参数可以设置：
task选择任务。人手关键点识别模型为pose_hand。
checkpoint指定模型文件所在的路径，如pose_hand = wf(task='pose_hand',checkpoint='pose_hand.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即pose_hand.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
推理方式1：
python
keypoints = hand.inference(data='data/hand.jpg') # 进行模型推理
推理方式2：
python
keypoints,img_with_keypoints = hand.inference(data='data/hand.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data: 指定待识别关键点的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出关键点识别完成后的图片。
img_type: 关键点识别完成后会返回含有关键点的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
bbox：该参数可配合目标检测使用。在多人手关键点检测中，该参数指定了要识别哪个检测框中的关键点。
模型推理返回结果：
keypoints以二维数组的形式保存了所有关键点的坐标，每个关键点(x,y)被表示为[x,y]根据前面的图示，要获取到某个特定序号i的关键点，只需要访问keypoints[i]即可。
img_with_keypoints是个三维数组，以pil格式保存了关键点识别完成后的图片。
3. 结果输出
python
format_result = hand.format_output(lang='zh')# 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，共有两个键：关键点坐标和分数。关键点坐标以二维数组形式保存了每个关键点的[x,y]坐标，而分数则是对应下标的关键点的分数，以一维数组形式保存。
python
hand.show(img_with_keypoints) # 展示推理图片
show()能够输出带有关键点的结果图像。
若此时发现关键点识别效果不佳，关键点乱飞，我们可以果断采用在提取关键点之前先进行目标检测的方式。如当前任务'pose_hand'，就可以在之前先进行'det_hand'。详情可参考项目XEduHub实例代码-入门完整版中的 “3-1 综合项目：目标检测+关键点检测”。
4. 结果保存
python
hand.save(img_with_keypoints,'img_with_keypoints.jpg') # 保存推理图片
save()方法能够保存带有关键点的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
人体所有关键点
XEduHub提供了识别人体所有关键点，包括人手、人脸和人体躯干部分关键点的模型：pose_wholebody133。具体关键点的序号及其分布如下图所示：
代码样例
python
from XEdu.hub import Workflow as wf
wholebody = wf(task='pose_wholebody') # 数字可省略，当省略时，默认为pose_wholebody133
keypoints,img_with_keypoints = wholebody.inference(data='data/wholebody.jpg',img_type='pil') # 进行模型推理
format_result = wholebody.format_output(lang='zh') # 将推理结果进行格式化输出
wholebody.show(img_with_keypoints) # 展示推理图片
wholebody.save(img_with_keypoints,'img_with_keypoints.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
wholebody = wf(task='pose_wholebody') # 数字可省略，当省略时，默认为pose_wholebody133
wf()中共有三个参数可以设置：
task选择任务。全身关键点提取模型为pose_wholebody。
checkpoint指定模型文件所在的路径，如wholebody = wf(task='pose_wholebody',checkpoint='pose_wholebody.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即pose_wholebody.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
```python
keypoints = wholebody.inference(data='data/wholebody.jpg') # 模型推理方式1
keypoints,img_with_keypoints = wholebody.inference(data='data/wholebody.jpg',img_type='pil') # 模型推理方式2
```
模型推理inference()可传入参数：
data: 指定待识别关键点的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出关键点识别完成后的图片。
img_type: 关键点识别完成后会返回含有关键点的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
bbox：该参数可配合目标检测使用。在多人手关键点检测中，该参数指定了要识别哪个检测框中的关键点。
模型推理返回结果：
keypoints以二维数组的形式保存了所有关键点的坐标，每个关键点(x,y)被表示为[x,y]根据前面的图示，要获取到某个特定序号i的关键点，只需要访问keypoints[i]即可。
img_with_keypoints是个三维数组，以pil格式保存了关键点识别完成后的图片。
3. 结果输出
python
format_result = wholebody.format_output(lang='zh') # 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，共有两个键：关键点坐标和分数。关键点坐标以二维数组形式保存了每个关键点的[x,y]坐标，而分数则是对应下标的关键点的分数，以一维数组形式保存。
python
wholebody.show(img_with_keypoints) # 展示推理图片
show()能够输出带有关键点的结果图像。
若此时发现关键点识别效果不佳，关键点乱飞，请在提取关键点之前先进行目标检测。如当前任务'pose_wholebody'，就可以在之前先进行'det_body'。详情可参考项目XEduHub实例代码-入门完整版中的 “3-1 综合项目：目标检测+关键点检测”。
4. 结果保存
python
wholebody.save(img_with_keypoints,'img_with_keypoints.jpg') # 保存推理图片
save()方法能够保存带有关键点的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
3. 光学字符识别（OCR）
光学字符识别（Optical Character Recognition, OCR）是一项用于将图像或扫描的文档转换为可编辑的文本格式的技术。OCR技术能够自动识别和提取图像或扫描文档中的文本，并将其转化为计算机可处理的文本格式。OCR技术在车牌识别、证件识别、文档扫描、拍照搜题等多个场景有着广泛应用。
XEduHub使用的OCR模型是来自百度的开源免费的OCR模型：rapidocr，这个模型运行速度快，性能优越，小巧灵活，并且能支持超过6000种字符的识别，如简体中文、繁体中文、英文、数字和其他艺术字等等。
注意：你可以在当前项目中找到名为font的文件夹，里面的FZVTK.TTF文件是一种字体文件，为了显示识别出的文字而使用。
代码样例
python
from XEdu.hub import Workflow as wf
ocr = wf(task='ocr')
result,ocr_img = ocr.inference(data='data/ocr_img.png',img_type='cv2') # 进行模型推理
ocr_format_result = ocr.format_output(lang="zh") # 推理结果格式化输出
ocr.show(ocr_img) # 展示推理结果图片
ocr.save(ocr_img,'ocr_result.jpg') # 保存推理结果图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
ocr = wf(task='ocr')
wf()中只有一个参数可以设置：
task选择任务类型，光学字符识别（OCR）的模型为ocr。
注意：ocr任务并没有使用ONNX模型，也不会以onnx方式下载模型文件，而是自动下载和安装一个Python库，因此不同于之前任务的下载方式，无需指定下载路径。如果在断网情况第一次使用，可以先通过pip install rapidocr_onnxruntime==1.3.7预先下载库。
2. 模型推理
```python
result = ocr.inference(data='data/ocr_img.png') # 模型推理方式1
result,ocr_img = ocr.inference(data='data/ocr_img.png',img_type='cv2') # 模型推理方式2
```
模型推理inference()可传入参数：
data: 指定待进行OCR识别的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出OCR识别完成后的图片。
img_type：OCR识别完成后会返回含有识别结果的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']。
result以一维数组的形式保存了识别出的文本及其检测框的四个顶点(x,y)坐标。
```
result
[('8-01', [[37.0, 82.0], [226.0, 94.0], [221.0, 177.0], [32.0, 165.0]]),
 ('消火栓', [[177.0, 282.0], [1109.0, 288.0], [1106.0, 610.0], [174.0, 603.0]]),
 ('FIREHYDRANT',
  [[278.0, 580.0], [1010.0, 580.0], [1010.0, 642.0], [278.0, 642.0]]),
 ('种花元学',
  [[1111.0, 604.0], [1160.0, 604.0], [1160.0, 618.0], [1111.0, 618.0]]),
 ('消连道一酒防安全连查点',
  [[1080.0, 635.0], [1155.0, 635.0], [1155.0, 652.0], [1080.0, 652.0]]),
 ('地址：文料大楼6楼8-',
  [[918.0, 647.0], [1000.0, 647.0], [1000.0, 664.0], [918.0, 664.0]]),
 ('码：3T-00001018',
  [[1081.0, 652.0], [1147.0, 652.0], [1147.0, 666.0], [1081.0, 666.0]]),
 ('区中生校区',
  [[1081.0, 670.0], [1126.0, 670.0], [1126.0, 683.0], [1081.0, 683.0]]),
 ('查4-1',
  [[1097.0, 663.0], [1130.0, 663.0], [1130.0, 674.0], [1097.0, 674.0]]),
 ('工大楼（中）上公#',
  [[1076.0, 676.0], [1253.0, 678.0], [1253.0, 696.0], [1076.0, 693.0]]),
 ('消火栓使用说明', [[469.0, 761.0], [901.0, 764.0], [901.0, 821.0], [469.0, 818.0]]),
 ('How to Use a Fire Hydrant',
  [[463.0, 822.0], [902.0, 826.0], [902.0, 857.0], [463.0, 852.0]]),
 ('1.按下消火栓箱门右侧的按钮，并打开箱门。',
  [[166.0, 1039.0], [525.0, 1037.0], [525.0, 1060.0], [166.0, 1062.0]]),
 ('Press thebuttonontheright sideof thefirecabinet,thenopen thedoor.',
  [[178.0, 1061.0], [763.0, 1059.0], [763.0, 1082.0], [178.0, 1084.0]]),
 ('2.拉出水带，并将水带两端接口分别对准消火栓固定接口和水枪接口，顺时针方向旋转、拧紧。',
  [[165.0, 1102.0], [924.0, 1102.0], [924.0, 1128.0], [165.0, 1128.0]]),
 ('Takeoutthehose,andconnect the interfacesof thehosewiththehydrantand thenozzlebyrotatingclockwise.',
  [[184.0, 1123.0], [1094.0, 1125.0], [1094.0, 1151.0], [184.0, 1149.0]]),
 ('3.按下消火栓箱内的启泵按钮，指示灯闪亮说明消防泵已启动。',
  [[165.0, 1169.0], [669.0, 1168.0], [669.0, 1191.0], [165.0, 1192.0]]),
 ('4.由一人紧握水枪，另一人逆时针旋开消火栓阀门至最大开度。',
  [[167.0, 1234.0], [668.0, 1234.0], [668.0, 1257.0], [167.0, 1257.0]]),
 ('Onepersonholdsthenozzle tightly,whiletheother turnsonthevalveanticlockwise tomaximumopening',
  [[187.0, 1253.0], [1039.0, 1258.0], [1039.0, 1284.0], [187.0, 1279.0]]),
 ('火警电话',
  [[295.0, 1327.0], [458.0, 1332.0], [457.0, 1375.0], [294.0, 1370.0]]),
 ('119', [[466.0, 1330.0], [569.0, 1330.0], [569.0, 1396.0], [466.0, 1396.0]]),
 ('内部应急电话',
  [[629.0, 1331.0], [869.0, 1334.0], [869.0, 1375.0], [629.0, 1373.0]]),
 ('Fire Telephone',
  [[292.0, 1369.0], [460.0, 1373.0], [460.0, 1400.0], [292.0, 1396.0]]),
 ('Emergency24/7Hotline(021)62238110',
  [[624.0, 1372.0], [1072.0, 1374.0], [1072.0, 1401.0], [624.0, 1399.0]]),
 ('消防设施 请保持完好',
  [[409.0, 1456.0], [851.0, 1459.0], [851.0, 1503.0], [409.0, 1500.0]])]
```
如呈现的输出结果所示，数组中每个元素的形式为元组：（识别文本，检测框顶点坐标）。四个顶点坐标顺序分别为[左上，右上，左下，右下]。
ocr_img的格式为cv2，保存了ocr识别后的结果图片。
3. 结果输出
python
ocr_format_result = ocr.format_output(lang="zh")
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
{'检测框': [[[37.0, 82.0], [226.0, 94.0], [221.0, 177.0], [32.0, 165.0]],
         [[177.0, 282.0], [1109.0, 288.0], [1106.0, 610.0], [174.0, 603.0]],
         [[278.0, 580.0], [1010.0, 580.0], [1010.0, 642.0], [278.0, 642.0]],
         [[1111.0, 604.0], [1160.0, 604.0], [1160.0, 618.0], [1111.0, 618.0]],
         [[1080.0, 635.0], [1155.0, 635.0], [1155.0, 652.0], [1080.0, 652.0]],
         [[918.0, 647.0], [1000.0, 647.0], [1000.0, 664.0], [918.0, 664.0]],
         [[1081.0, 652.0], [1147.0, 652.0], [1147.0, 666.0], [1081.0, 666.0]],
         [[1081.0, 670.0], [1126.0, 670.0], [1126.0, 683.0], [1081.0, 683.0]],
         [[1097.0, 663.0], [1130.0, 663.0], [1130.0, 674.0], [1097.0, 674.0]],
         [[1076.0, 676.0], [1253.0, 678.0], [1253.0, 696.0], [1076.0, 693.0]],
         [[469.0, 761.0], [901.0, 764.0], [901.0, 821.0], [469.0, 818.0]],
         [[463.0, 822.0], [902.0, 826.0], [902.0, 857.0], [463.0, 852.0]],
         [[166.0, 1039.0], [525.0, 1037.0], [525.0, 1060.0], [166.0, 1062.0]],
         [[178.0, 1061.0], [763.0, 1059.0], [763.0, 1082.0], [178.0, 1084.0]],
         [[165.0, 1102.0], [924.0, 1102.0], [924.0, 1128.0], [165.0, 1128.0]],
         [[184.0, 1123.0], [1094.0, 1125.0], [1094.0, 1151.0], [184.0, 1149.0]],
         [[165.0, 1169.0], [669.0, 1168.0], [669.0, 1191.0], [165.0, 1192.0]],
         [[167.0, 1234.0], [668.0, 1234.0], [668.0, 1257.0], [167.0, 1257.0]],
         [[187.0, 1253.0], [1039.0, 1258.0], [1039.0, 1284.0], [187.0, 1279.0]],
         [[295.0, 1327.0], [458.0, 1332.0], [457.0, 1375.0], [294.0, 1370.0]],
         [[466.0, 1330.0], [569.0, 1330.0], [569.0, 1396.0], [466.0, 1396.0]],
         [[629.0, 1331.0], [869.0, 1334.0], [869.0, 1375.0], [629.0, 1373.0]],
         [[292.0, 1369.0], [460.0, 1373.0], [460.0, 1400.0], [292.0, 1396.0]],
         [[624.0, 1372.0], [1072.0, 1374.0], [1072.0, 1401.0], [624.0, 1399.0]],
         [[409.0, 1456.0], [851.0, 1459.0], [851.0, 1503.0], [409.0, 1500.0]]],
 '分数': [0.7910715460777282,
        0.7438937872648239,
        0.8779238214095434,
        0.37094803899526596,
        0.6871711413065592,
        0.8419906993707021,
        0.7611332259007862,
        0.6775176425774893,
        0.43559680581092836,
        0.5932324767112732,
        0.8621773198246956,
        0.896224904518861,
        0.9342550689523871,
        0.9395184453689691,
        0.9641873836517334,
        0.9524000250563329,
        0.9582841634750366,
        0.9612718860308329,
        0.9428217871033627,
        0.798493766784668,
        0.7463672459125519,
        0.8557159645216805,
        0.8900542497634888,
        0.9359333357390236,
        0.8789843645962802],
 '文本': ['8-01',
        '消火栓',
        'FIREHYDRANT',
        '种花元学',
        '消连道一酒防安全连查点',
        '地址：文料大楼6楼8-',
        '码：3T-00001018',
        '区中生校区',
        '查4-1',
        '工大楼（中）上公#',
        '消火栓使用说明',
        'How to Use a Fire Hydrant',
        '1.按下消火栓箱门右侧的按钮，并打开箱门。',
        'Press thebuttonontheright sideof thefirecabinet,thenopen thedoor.',
        '2.拉出水带，并将水带两端接口分别对准消火栓固定接口和水枪接口，顺时针方向旋转、拧紧。',
        'Takeoutthehose,andconnect the interfacesof thehosewiththehydrantand '
        'thenozzlebyrotatingclockwise.',
        '3.按下消火栓箱内的启泵按钮，指示灯闪亮说明消防泵已启动。',
        '4.由一人紧握水枪，另一人逆时针旋开消火栓阀门至最大开度。',
        'Onepersonholdsthenozzle tightly,whiletheother '
        'turnsonthevalveanticlockwise tomaximumopening',
        '火警电话',
        '119',
        '内部应急电话',
        'Fire Telephone',
        'Emergency24/7Hotline(021)62238110',
        '消防设施 请保持完好']}
```
format_output的结果以字典形式存储了推理结果，共有三个键：检测框、分数和文本。检测框以三维数组形式保存了每个检测框的四个顶点的[x,y]坐标，而分数则是对应下标的检测框分数，以一维数组形式保存。文本则是每个检测框中识别出的文本，以一维数组形式保存。
结果可视化：
python
ocr.show(ocr_img) # 展示推理结果图片
显示结果图片：由两部分组成，左侧为原图片，右侧为经过ocr识别出的文本，并且该文本的位置与原图片中文本的位置保持对应。
4. 结果保存
python
ocr.save(ocr_img,'ocr_result.jpg') # 保存推理结果图片
save()方法能够保存ocr识别后的结果图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
4. 图像分类
图像分类是一个分类任务，它能够将不同的图像划分到指定的类别中，实现最小的分类误差和最高精度。XEduHub提供了进行图像分类的模型：cls_imagenet，该模型的分类类别取自ImageNet的一千个分类，这意味着该模型能够将输入的图像划分到这一千个分类中的类别上。
代码样例
python
from XEdu.hub import Workflow as wf
cls = wf(task='cls_imagenet') # 模型声明
img_path = 'demo/cat.png' # 指定进行推理的图片路径
result,cls_img = cls.inference(data=img_path,img_type='cv2') # 进行推理
cls_format_result = cls.format_output(lang='zh')  # 结果格式化输出
cls.show(cls_img) # 展示原图
cls.save(cls_img,'cls_result.jpg')# 保存图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
cls = wf(task="cls_imagenet") # 模型声明
wf()中共有三个参数可以设置：
task选择任务。图像分类的模型为cls_imagenet。
checkpoint指定模型文件所在的路径，如cls = wf(task='cls_imagenet',checkpoint='cls_imagenet.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即cls_imagenet.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
```python
result = cls.inference(data='data/cat101.jpg') # 模型推理方式1
result,cls_img = cls.inference(data='data/cat101.jpg', img_type='pil') # 进行模型推理方式2
```
模型推理inference()可传入参数：
data: 指定待分类的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出原图。
img_type：返回原图，该参数指定了返回图片的格式，可选有:['cv2','pil']。
推理结果result是一个二维数组，表示这个图片在ImageNet的一千个分类中，属于每个分类的概率。
cls_img的格式为cv2，呈现的就是一张原图，并非带模型推理结果的图。
3. 结果输出
python
format_result = cls.format_output(lang='zh') #推理结果格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result是一个字典，以格式化的方式展示了这张图片最有可能的分类结果。预测值表示图片分类标签在所有一千个分类中的索引，分数是属于这个分类的概率，预测类别是分类标签的内容。
```
输出结果
{'预测值': 281, '分数': 0.5337719, '预测类别': 'tabby, tabby cat'}
```
配合展示原图：
cls.show(cls_img) # 展示原图
5. 内容生成
内容生成模型是一种人工智能模型，它能够根据输入的提示或指令生成新的内容，如文本、图像、音频或视频。
XEduHub提供了两个图像内容生成任务：图像风格迁移gen_style和图像着色gen_color。
风格迁移
1） 图像风格迁移模型的使用
图像风格迁移就是根据一幅风格图像(style image)，将任意一张其他图像转化成这个风格，并尽量保留原图的内容(content image)。
XEduHub中的风格迁移使用有两类：
预设风格迁移：预设好五种风格，用户只传入一张内容图像，迁移至该风格。
自定义风格迁移：用户传入一张内容图像和一张风格图像，迁移至风格图像的风格。
实例讲解1：马赛克（mosaic）风格迁移模型的使用
每个风格使用的代码风格是类似的，接下来通过学习一个完整示范，可以达到举一反三的效果
下面是实例马赛克（mosaic）风格迁移模型的完整代码：
python
from XEdu.hub import Workflow as wf
style = wf(task='gen_style',style='mosaic')
result, img = style.inference(data='data/cat101.jpg',img_type='cv2') # 进行模型推理
style.show(img) # 展示推理图片
style.save(img,'style_cat.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
style = wf(task='gen_style',style='mosaic')
wf()中共有四个参数可以设置：
task选择任务。风格迁移的模型为gen_style。
style选择风格迁移所使用的风格。可选的风格有udnie、mosaic、rain-princess、candy和pointilism，也可以用一张图片作为风格源。
checkpoint指定模型文件所在的路径，如style = wf(task='gen_style',style='mosaic',checkpoint='gen_style_mosaic.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即gen_style_mosaic.onnx（任务名加下划线加风格名）。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
运行代码wf.support_style()可查看当前预设的风格。当前预设风格共有五种，如下图所示。
udnie: 该幅作品是法国艺术家弗朗西斯·毕卡比亚 （Francis Picabia） 于 1913 年创作的一幅布面油画。这幅抽象画抽象的形式和金属色的反射让人想起机器的世界。
mosaic: 马赛克是由有色石头、玻璃或陶瓷制成的规则或不规则的小块图案或图像，由石膏/砂浆固定到位并覆盖表面。
rain-princess：该幅作品的作者是李奥尼德·阿夫列莫夫，他继梵高之后，当代最著名的现代印象派艺术家。风景、城市和人物在他的画笔下（更确切的可以说是刮刀），具有一种独特的风格，用色大胆、明亮，传达他的乐观。
candy: 该风格通过糖果般绚丽的色块以及象征棒棒糖的圆圈图案，传递出甜蜜童真。
pointilism： 点彩画是一种绘画技术，其中将小而独特的色点应用于图案以形成图像。
style可选参数为：['mosaic','candy','rain-princess','udnie','pointilism']，也可以输入一张其他图片的路径来自定义风格，如style='fangao.jpg'。为了方便用户使用预设风格，还可以通过输入预设风格对应的标签值来进行设定，如style=0。
预设风格
对应标签值
mosaic
0
candy
1
rain-princess
2
udnie
3
pointilism
4
2. 模型推理
python
result, img = style.inference(data='data/cat101.jpg',img_type='cv2') # 进行模型推理
模型推理inference()可传入参数：
data: 待进行风格迁移的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[true,false] 默认为false。如果取值为true，在推理完成后会直接输出风格迁移完成后的图片。
img_type: 推理完成后会直接输出风格迁移完成后的图片。该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
模型推理返回结果：
result和img都是三维数组，以cv2格式保存了风格迁移完成后的图片。
3. 结果输出
python
style.show(img) # 展示推理后的图片
show()能够输出风格迁移后的结果图像。
4. 结果保存
python
style.save(img,'style_cat.jpg') # 保存推理图片
save()方法能够保存风格迁移后的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
实例讲解2：自定义风格迁移模型的使用
当我们看到喜欢的风格的图像，并想要迁移到其他图像上时，我们就可以使用XEduHub中的自定义风格迁移模型。
例如我喜欢“my_style”这张图片，我想要将其风格迁移到我的风景照上，生成新的图像
将图片的路径来自定义风格，style='demo/my_style.jpg'
下面是实例自定义风格迁移模型的完整代码：
python
from XEdu.hub import Workflow as wf # 导入库
style = wf(task='gen_style',style='demo/my_style.jpg') # 实例化模型
img_path = 'demo/ShangHai.jpg'  # 指定进行推理的图片路径
result, new_img = style.inference(data=img_path,img_type='cv2') # 进行模型推理
style.show(new_img) # 可视化结果
style.save(new_img, 'demo/style_my_style_ShangHai.jpg') # 保存可视化结果
2. 图像着色模型的使用
图像着色模型是将灰度图像转换为彩色图像的模型，它根据图像的内容、场景和上下文等信息来推断合理的颜色分布，实现从灰度到彩色的映射。
当我们有一张黑白图片想要为它上色时，可以使用XEduHub提供的gen_color图像着色任务。通过调用基于卷积神经网络 (CNN)训练的模型进行推理，自动地为黑白图像添加颜色，实现了快速生成逼真的着色效果。
代码样例
python
from XEdu.hub import Workflow as wf # 导入库
color = wf(task='gen_color') # 实例化模型
result, img = color.inference(data='demo/gray_img1.jpg',img_type='cv2') # 进行模型推
color.show(img) # 可视化结果
color.save(img,'demo/color_img.jpg') # 保存可视化结果
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf # 导入库
color = wf(task='gen_color') # 实例化模型
wf()中共有三个参数可以设置：
task选择任务。图像分类的模型为gen_color。
checkpoint指定模型文件所在的路径，如color = wf(task='gen_color',checkpoint='gen_color.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即gen_color.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
python
result, img = color.inference(data='demo/gray_img1.jpg',img_type='cv2') # 进行模型推理
模型推理inference()可传入参数：
data: 待进行风格迁移的图片，可以是以图片路径形式传入，也可直接传入cv2或pil格式的图片。
show: 可取值：[true,false] 默认为false。如果取值为true，在推理完成后会直接输出风格迁移完成后的图片。
img_type: 推理完成后会直接输出图像着色完成后的图片。该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
模型推理返回结果：
result和img都是三维数组，以cv2格式保存了风格迁移完成后的图片。
3. 结果输出
python
color.show(img) # 展示推理后的图片
show()能够输出着色后的结果图像。
4. 结果保存
python
color.save(img,'color_img.jpg') # 保存推理图片
save()方法能够保存着色后的图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
6. 全景驾驶感知系统
全景驾驶感知系统是一种高效的多任务学习网络，“多任务”表示该模型可同时执行交通对象检测、可行驶道路区域分割和车道检测任务，能很好地帮助自动驾驶汽车通过摄像头全面了解周围环境。我们可以在实时自动驾驶的项目中组合运用不同的检测任务，来控制车辆的动作，以达到更好的效果。XEduHub提供了进行全景驾驶感知的任务：drive_perception。
代码样例
python
from XEdu.hub import Workflow as wf
drive = wf(task='drive_perception') # 实例化模型
result,img = drive.inference(data='demo/drive.png',img_type='cv2') # 模型推理
drive.format_output(lang='zh') # 将推理结果进行格式化输出
drive.show(img) # 展示推理图片
drive.save(img,'img_perception.jpg') # 保存推理图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
drive = wf(task='drive_perception') # 实例化模型
wf()中共有三个参数可以设置：
task选择任务。全景驾驶感知系统的模型为drive_perception。
checkpoint指定模型文件所在的路径，如drive = wf(task='drive_perception',checkpoint='drive_perception.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即drive_perception.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
python
result,img = drive.inference(data='demo/drive.png',img_type='cv2') # 模型推理
模型推理inference()可传入参数：
data：指定待检测的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出目标检测完成后的图片。
img_type：目标检测完成后会返回含有检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
thr: 设置检测框阈值，取值范围为[0,1]超过该阈值的检测框被视为有效检测框，进行显示。
模型推理返回结果：
[array([[671, 398,  98,  78],
       [627, 393,  26,  25]]), array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 1, 1, 1],
       [0, 0, 0, ..., 1, 1, 1],
       [0, 0, 0, ..., 1, 1, 1]], dtype=uint8)]
result：以三维数组的形式保存了车辆检测（红色框），车道线分割（蓝色色块），可行驶区域（绿色色块）。
车辆检测result[0]：以二维数组保存了车辆目标检测框左上角顶点的坐标(x1,y1)和右下角顶点的坐标(x2,y2)（之所以是二维数组，是因为该模型能够检测多辆车，因此当检测到多辆车时，就会有多个[x1,y1,x2,y2]的一维数组，所以需要以二维数组形式保存），我们可以利用这四个数据计算出其他两个顶点的坐标，以及检测框的宽度和高度。
车道线分割result[1]：以由0，1组成二维数组（w*h），保存图像中每个像素的mask，mask为1表示该像素为车道线分割目标，mask为0表示该像素是背景。
可行驶区域result[2]：以由0，1组成二维数组（w*h），保存图像中每个像素的mask，mask为1表示该像素为可驾驶区域分割目标，mask为0表示该像素是背景。
img_with_box：：是个三维数组，以cv2格式保存了包含了检测框与分割目标的图片。
3. 结果输出
python
format_result = drive.format_output(lang='zh') # 将推理结果进行格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
format_result以字典形式存储了推理结果，有四个键：检测框、分数、车道线掩码、可行驶区域掩码。检测框以二维数组形式保存了每个检测框的坐标信息[x1,y1,x2,y2]。
```
输出结果
{'检测框': [[671, 398, 98, 78], [627, 393, 26, 25]],
 '分数': [0.9999916553497314, 0.999988317489624],
 '车道线掩码': array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),
 '可行驶区域掩码': array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 1, 1, 1],
       [0, 0, 0, ..., 1, 1, 1],
       [0, 0, 0, ..., 1, 1, 1]], dtype=uint8)}
```
python
drive.show(img) # 展示推理图片
show()能够输出带有检测框与分割目标的图片。
4. 结果保存
python
drive.save(img,'img_perception.jpg') # 保存推理图片
save()方法能够保存带有检测框与分割目标的图片。
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
7. 多模态图文特征提取
多模态图文特征提取技术是一种将计算机无法直接理解图像或文本转换成计算机擅长理解的数字数字向量。通过“特征提取”方式得到的数字向量，能完成零样本分类、文本翻译，图像聚类等任务。XEduHub提供了图像特征提取和文本特征提取任务：'embedding_image'，'embedding_text'。
图像特征提取
当我们使用图像特征提取，本质上是将图像“编码”或“嵌入”到向量形式的一系列数字中，让图像->向量。
这些向量可以捕捉图像中的局部特征，如颜色、纹理和形状等。图像特征提取有助于计算机识别图像中的对象、场景和动作等。
代码样例
python
from XEdu.hub import Workflow as wf # 导入库
img_emb = wf(task='embedding_image') # 实例化模型
image_embeddings = img_emb.inference(data='demo/cat.png') # 模型推理
print(image_embeddings) # 输出向量
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf # 导入库
img_emb = wf(task='embedding_image') # 实例化模型
wf()中有三个参数可以设置：
task选择任务。任务名称为embedding_image。
checkpoint指定模型文件所在的路径，如img_emb = wf(task='embedding_image',checkpoint='embedding_image.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即embedding_image.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
任务模型文件获取与存放请查看下文。
2. 模型推理
python
image_embeddings = img_emb.inference(data='demo/cat.png') # 模型推理
模型推理inference()可传入参数：
data：指定待特征提取的图片。可以直接传入图像路径data='cat.jpg' 或者多张图像路径列表data= ['cat.jpg','dog.jpg']。
模型推理返回结果：
[[-3.85740042e-01 -1.81814015e-01  2.94580430e-01 -3.75688493e-01
  -9.97719467e-02 -3.99263024e-01 -4.88978140e-02  4.52158630e-01
   3.44679683e-01  8.63113552e-02  3.00731242e-01 -4.26063895e-01
   3.67905170e-01 -1.52026504e-01  2.76561409e-01 -1.71330012e-02
   1.70775101e-01  5.59596419e-02  1.35128006e-01  1.62233084e-01
  -3.01426768e-01  2.13003829e-01  4.14773196e-01  6.58608004e-02
  -3.26728284e-01 -8.21363479e-02  3.83218110e-01  1.98388845e-03
   7.52402395e-02 -9.47817862e-02 -3.44407916e-01  3.05405080e-01
   2.16008723e-02  2.35447213e-02  1.86178669e-01  9.39529836e-02
   8.10235366e-02 -1.63445249e-01  2.17456058e-01  1.91864216e+00
  -8.11877787e-01 -5.21380484e-01 -1.63235351e-01 -5.83956987e-02
   1.12670302e-01 -1.23800576e+00  3.60498071e-01  1.70865953e-01
   2.36062884e-01 -1.30168095e-01  2.28240922e-01  4.12806034e-01
  -1.44194707e-01  5.99492826e-02  5.90620399e-01  3.17890644e-02
   5.66926479e-01  3.08387011e-01 -1.93981409e-01 -3.26512724e-01
   1.30550885e+00 -1.63456917e-01 -1.71479523e-01  1.87635124e-01
  -1.62023515e-01 -5.04231989e-01  1.93340555e-01  7.79000640e-01
  -2.60949045e-01  4.51564491e-01  1.93995193e-01 -5.56772709e-01
   2.24175125e-01 -2.24783584e-01 -4.67097044e-01 -1.89082280e-01
  -5.35702705e-03 -5.06964564e-01 -6.83084354e-02 -1.69336617e-01
  -3.91773462e-01 -3.84069264e-01 -5.86372465e-02 -4.74828005e-01
   1.57550305e-01 -4.58408058e-01  1.21461821e+00 -5.39815187e-01
  -1.67127803e-01  1.52734384e-01  1.82488933e-02 -3.33601654e-01
  -5.91347456e+00  1.90935433e-01 -8.74737203e-01  5.37124157e-01
   3.54932584e-02 -4.14988220e-01 -2.38636166e-01  1.60797548e+00
   2.25136548e-01 -4.80371296e-01 -1.21367343e-01  2.91841656e-01
  -1.68474913e-01  4.46746111e-01  2.07979515e-01  2.23423451e-01
   1.83344692e-01 -1.40982628e-01 -6.63975060e-01  6.43657669e-02
   4.81830776e-01  1.65944129e-01 -2.49111857e-02 -1.20062128e-01
   1.40799791e-01  2.88082600e-01  2.76114374e-01 -1.58252344e-01
   2.33967036e-01  3.16998273e-01  3.99250299e-01 -9.65632871e-03
   1.15269527e-01 -3.71721506e-01  1.94188878e-01  3.90523285e-01
   4.12000626e-01 -1.80458277e-03 -3.60838771e-01 -1.12181015e-01
  -5.05329251e-01  7.17117429e-01 -5.48307180e-01  2.06300825e-01
  -4.58916605e-01 -1.18503585e-01 -1.23531006e-01 -2.95604229e-01
   1.80596381e-01  1.27064258e-01 -1.58087850e-01  2.26205975e-01
  -6.21176720e-01 -7.06477538e-02  5.28910637e-01  5.26533902e-01
   9.11014974e-02  3.42928290e-01 -3.36795449e-01  2.60566026e-01
  -5.16621172e-01  2.01655403e-01 -4.43294048e-01 -1.68800324e-01
  -2.02344880e-02 -7.48819470e-01  6.20617419e-02 -1.59256980e-01
   3.74310374e-01  1.32567585e-01 -7.37757385e-02  8.41598064e-02
   2.06333309e-01 -2.64279962e-01  9.55645740e-02  1.89634040e-01
  -2.16711044e-01 -5.38360178e-02 -2.44866431e-01  6.27175808e-01
   4.68757153e-02  3.16201866e-01 -2.51018405e-01  2.42545903e-02
   9.87151146e-01 -3.33542049e-01  3.88702422e-01  2.55128801e-01
   3.66802216e-01 -7.47172385e-02 -5.85177168e-02 -3.47979367e-05
  -1.37775317e-01  6.06879890e-02  1.93679333e-01 -5.56745172e-01
  -3.32078993e-01 -5.45896962e-02  2.34644830e-01 -1.59180284e-01
   3.32888722e-01  1.31697670e-01  6.03572786e-01  1.21926449e-01
   8.54009390e-02 -6.74351096e-01 -1.53455126e+00 -1.49148270e-01
  -1.66233048e-01 -6.23880923e-02  1.20600522e-01  8.16958994e-02
   1.58847123e-02 -7.76146650e-02 -3.43320146e-02 -1.23784691e-01
  -1.11312561e-01  2.94828147e-01 -4.02154982e-01  9.57918584e-01
  -5.63874245e-01  3.49417627e-01  7.77897388e-02 -5.08594215e-01
   3.05591300e-02 -2.32043847e-01  9.69643235e-01 -5.39023459e-01
   4.33879554e-01  8.27765942e-01 -1.34395599e-01 -2.25581735e-01
  -2.71910757e-01  2.28383809e-01 -6.05013222e-04  2.75357395e-01
  -2.09673107e-01 -1.57775283e-01  1.27838835e-01  1.00237012e-01
  -4.72770751e-01  3.51460636e-01  3.12582344e-01  2.68262267e-01
   2.98875272e-01 -2.82303870e-01 -1.84585154e-01 -3.18509102e-01
   1.99579477e-01  1.22903138e-01  3.13289464e-04 -1.19046569e-02
   2.33945757e-01  4.52335238e-01  5.41643262e-01 -1.11395741e+00
  -1.81421086e-01 -1.79955930e-01 -7.46780261e-02  1.34541512e-01
   1.62621692e-01 -2.31587499e-01  1.91942528e-01  7.60079473e-02
  -8.75764787e-02 -1.25062481e-01  3.01574469e-01 -3.55449557e-01
  -2.55525708e-01  5.87767124e-01  1.70087546e-01 -1.90540528e+00
  -2.10818663e-01 -4.08018738e-01 -1.20051704e-01 -2.18928725e-01
  -3.14087391e-01 -4.32843268e-02 -2.06375062e-01 -1.36374623e-01
   6.00037217e-01  3.48301530e-02 -3.98940384e-01  1.20394975e-01
   1.24070890e-01  1.04261652e-01 -2.98556775e-01 -1.18747264e-01
   2.42990702e-01  1.51160806e-02 -4.76059765e-01  2.83466309e-01
  -2.65547872e-01  2.53341258e-01 -1.21838975e+00 -4.79129553e-02
  -3.88028845e-02  1.18025638e-01 -1.82919413e-01 -3.47036183e-01
  -3.42601538e-02  1.42673150e-01  2.14231908e-01 -2.30143934e-01
   1.30996168e-01 -1.37965798e-01  3.79844606e-01  4.15164113e-01
   1.05876885e-01 -1.57549411e-01 -2.77942568e-01 -4.60220486e-01
   1.38039052e-01 -1.69159055e-01 -3.82189423e-01  3.77442986e-02
   2.67798603e-01 -5.85539937e-02 -4.75669950e-01  3.39839756e-02
   7.16263771e-01 -2.56723523e-01  2.77137637e-01  4.75717485e-01
   2.24060252e-01  1.42611891e-01 -1.47834927e-01 -2.51081705e-01
   5.57422936e-01 -1.40718400e-01 -2.34217167e-01  4.03005749e-01
   1.54427320e-01  3.60494554e-01  9.46574286e-02 -2.43841931e-02
   1.44166082e-01 -4.30197239e-01  7.96608254e-02 -4.21275467e-01
   2.90648878e-01  8.94440413e-02 -5.84194437e-02  3.80072743e-01
   1.68659866e-01 -1.26878828e-01  3.88667434e-02 -5.88277280e-02
  -2.54280746e-01 -5.78271866e-01  1.36692956e-01  5.84963083e-01
   1.42402038e-01 -3.91594678e-01 -2.75257006e-02 -3.98596495e-01
   2.12786466e-01 -1.99996769e-01 -6.46989644e-01 -5.18083394e-01
   7.53146887e-01 -2.66945362e-01 -2.31973708e-01  5.26251793e-01
  -2.86485106e-01 -7.33324647e-01  3.97864729e-02  1.16903648e-01
   6.12574518e-02  1.77814111e-01  2.99390286e-01  1.37499496e-01
  -4.10501480e-01  1.91558301e-02 -1.72412276e-01 -2.86826253e-01
  -1.28502920e-01 -3.30712676e-01  3.59060019e-01  3.30571085e-01
  -3.34168911e-01 -4.03294861e-01 -6.88896000e-01 -8.84154737e-02
   8.99289250e-01  4.45285738e-01 -9.32471633e-01  3.20112407e-02
   4.82425421e-01 -7.42304802e-01 -5.27542889e-01  7.85614848e-02
   4.69861269e-01  1.79590791e-01  1.59789890e-01 -2.87791759e-01
   4.69775200e-02 -1.83868849e+00  1.53750971e-01  4.77876440e-02
   3.37841123e-01  1.50803804e-01  1.39148414e-01  3.65805954e-01
   1.10074878e-02 -3.03741008e-01 -1.82401910e-01 -4.56681699e-02
   7.47369975e-02  5.53856045e-03  1.22067243e-01  2.81827092e-01
  -8.37573111e-02  8.54545832e-02 -5.03253818e-01 -4.87406909e-01
   9.99620199e-01 -4.46691096e-01  4.88039315e-01  4.28979099e-03
  -7.01835155e-01  1.41174972e-01 -4.24902737e-02 -1.45253658e-01
   2.06106380e-02 -8.70095640e-02  2.44391918e-01 -6.75197363e-01
  -7.78099895e-02  6.13597393e-01 -2.50040293e-02 -2.12778434e-01
   1.43364608e-01  5.92731893e-01 -5.92901148e-02  2.67523468e-01
  -9.79188681e-01  6.87061399e-02 -2.04341471e-01  3.16503465e-01
  -5.24448574e-01  4.33634557e-02  1.32866547e-01 -5.65323114e-01
   1.08812883e-01  2.03480750e-01  2.07546815e-01 -1.16574995e-01
  -1.09661967e-02  2.19430268e-01 -7.74818435e-02  2.30491683e-01
   5.95359147e-01 -4.35534179e-01 -3.18401903e-01 -2.49920055e-01
   9.30231512e-02 -2.82962203e-01  1.35547385e-01 -6.19788170e-01
  -5.45631349e-02  8.27184916e-01 -2.99773753e-01  5.22855222e-01
   6.19698524e-01  8.09411883e-01  1.37993291e-01  3.32339823e-01
  -3.24640989e-01 -1.05401665e-01  2.94174254e-01  5.19716069e-02
   6.32487774e-01  3.20299491e-02 -4.62978512e-01 -1.87023833e-01
   4.52803016e-01 -3.47914696e-02  3.27060193e-01 -4.21827257e-01
  -4.97323275e-01 -4.16128635e-01 -3.15771133e-01  5.17482832e-02
  -3.62640381e-01 -1.50059134e-01 -4.28877860e-01 -7.57826626e-01
   2.19028667e-01 -4.49721038e-01  2.82200187e-01  1.38432071e-01
  -6.74649239e-01  5.60074598e-02 -1.86310157e-01  3.48043889e-02
  -4.19803619e-01 -1.81031242e-01  2.40428239e-01 -6.49814829e-02
   4.65104222e-01 -1.13697350e-02  1.97285146e-01  1.71784297e-01
  -1.05541974e-01  3.76087278e-01 -4.27732319e-01 -1.43245876e-01
  -2.69167125e-01  4.53836232e-01 -1.38121948e-01  3.12318951e-02
   2.15785950e-02 -2.58834958e-01 -2.91318238e-01 -1.84020817e-01
  -2.87409008e-01  4.47631925e-01  1.90001428e-02 -3.18864509e-02]]
result：以二维数组的形式保存了每张图片特征提取后的512维向量。
文本特征提取
当我们使用文本特征提取，本质上是将文本的上下文和场景“编码”或“嵌入”到向量形式的一系列数字中，让文本->向量。
这些向量将词语映射到数值空间中，使得词语成为有意义的数值向量。
因为该模型的训练集来源是互联网网页提取的4亿对图像文本对的编码，所以这里的文本可以为网络上出现的任意名词，或是一段文字。你可以加入很多描述性文本，让之后的“零样本分类”变得十分有趣！
代码样例
python
from XEdu.hub import Workflow as wf # 导入库
txt_emb = wf(task='embedding_text') # 实例化模型
txt_embeddings = txt_emb.inference(data=['a black cat','a yellow cat']) # 模型推理
print(txt_embeddings) # 输出向量
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf # 导入库
txt_emb = wf(task='embedding_text') # 实例化模型
wf()中只有一个参数可以设置：
task选择任务。任务名称为embedding_text。
checkpoint指定模型文件所在的路径，如txt_emb = wf(task='embedding_text',checkpoint='embedding_text.onnx')。如果没有指定模型路径，Workflow会默认在本地同级的“checkpoints”文件夹中寻找与任务名对应的模型文件，即embedding_text.onnx。否则将通过网络到浦源平台的专用地址下载。
download_path指定模型的下载路径。缺省情况下，模型文件会下载到“checkpoints”文件夹中，“checkpoints”文件夹不存在则自动建立。如果希望代码在没有网络的设备上也能运行，请同步复制checkpoints文件夹。如希望模型保存放在其他路径，则设置download_path参数，如download_path='my_checkpoint'。注意，download_path参数为文件夹名称。建议该参数留空，使用默认地址。
2. 模型推理
python
txt_embeddings = txt_emb.inference(data=['a black cat','a yellow cat']) # 模型推理
模型推理inference()可传入参数：
data：指定待特征提取的文本。可以直接传入文本data= 'cat'或者多条文本列表data= ['a black cat','a yellow cat']。
模型推理返回结果：
[[ 0.41094494 -0.29223138 -0.06906292 ...  0.00064418 -0.23246601
   0.12385264]
 [ 0.24434651 -0.19498482 -0.30111405 ... -0.06937407 -0.2500025
   0.08038913]]
result：以二维数组的形式保存了每条文本特征提取后的512维向量。
提完了特征能干啥？
零样本分类！
什么是零样本分类呢？举个例子，现在我们想要分类图片中的猫是黑色的还是黄色的，按照图像分类的方式，我们需要收集数据集，并且标注数据集，再进行模型训练，最后才能使用训练出来的模型对图像进行分类。而现在，我们使用的“图像特征提取”和“文本特征提取”只需通过特征向量就可以进行分类，避免了大量的标注工作。
上文中我们已经通过图像特征提取和文本特征提取把cat.jpg,'a black cat','a yellow cat'分别变成了3堆数字（3个512维向量），但是很显然，我们看不懂这些数字，但是计算机可以！
通过让计算机将数字进行运算，即将图像和文本的特征向量作比较，就能看出很多信息，这也叫计算向量之间相似度。
为了方便大家计算向量之间的相似度，我们也提供了一系列数据处理函数，函数具体内容请见XEdu的常见函数。
下面就示范使用cosine_similarity比较两个embedding序列的相似度。可以直接使用get_similarity函数，选择method='cosine'来实现。
python
from XEdu.utils import get_similarity # 导入库
get_similarity(image_embeddings, txt_embeddings,method='cosine') # 计算相似度
该函数可以比较两个embedding序列的相似度，这里的相似度是以余弦相似度为计算指标的，其公式为：$$Cosine(x,y) = \frac{x \cdot y}{|x||y|}$$。
假设输入的待比较embedding序列尺度分别为(N, D)和(M, D)，则输出的结果尺度为(N, M)。
现在我们可以看到cat.jpg与'a black cat'向量的相似度为0.007789988070726395，而与'a yellow cat'向量的相似度为0.9922100305557251。显而易见，这张可爱的黄色猫咪图像与'a yellow cat'文本描述更为贴近。
8. MMEdu模型推理
XEduHub现在可以支持使用MMEdu导出的onnx模型进行推理啦！如果你想了解如何使用MMEdu训练模型，可以看这里：解锁图像分类模块：MMEduCls、揭秘目标检测模块：MMEduDet。
如果你想了解如何将使用MMEdu训练好的模型转换成ONNX格式，可以前往最后一步：模型转换。OK，准备好了ONNX模型，那么就开始使用XEduHub吧！
MMEdu训练的图像分类模型
代码样例
python
from XEdu.hub import Workflow as wf
mmcls = wf(task='mmedu',checkpoint='cats_dogs.onnx')# 指定使用的onnx模型
result, result_img =  mmcls.inference(data='data/cat.png',img_type='pil')# 进行模型推理
format_result = mmcls.format_output(lang='zh') # 推理结果格式化输出
mmcls.show(result_img) # 展示推理结果图片
mmcls.save(result_img,'new_cat.jpg')# 保存推理结果图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
mmcls = wf(task='mmedu',checkpoint='cats_dogs.onnx') # 指定使用的onnx模型
wf()中共有两个参数可以设置：
task：只需要设置task为mmedu ，而不需要指定是哪种任务。
checkpoint：指定你的模型的路径，如checkpoint='cats_dogs.onnx'。
这里我们以猫狗分类模型为例，项目指路：猫狗分类。
2. 模型推理
python
result, result_img =  mmcls.inference(data='data/cat101.jpg',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data：指定待检测的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出目标检测完成后的图片。
img_type：分类完成后会返回含有分类标签的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
thr(float): 设置推理阈值，取值范围为[0,1]，预测结果的置信度高于这个阈值时，这些结果才会被认为是有效的。
result是一个字典，包含三个键：标签、置信度和预测结果。
result_img以pil格式保存了模型推理完成后的图片（原图+推理结果）。
3. 结果输出
python
format_result = mmcls.format_output(lang='zh') # 推理结果格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
{'标签': 0, '置信度': 0.79525626, '预测结果': 'cat'}
```
format_result以字典形式保存了模型的推理结果，包括所属标签、置信度、以及预测结果。
python
mmcls.show(result_img) # 展示推理结果图片
show()能够推理后的结果图像。与原图相比，结果图片在左上角多了pred_label, pred_socre和pred_class三个数据，对应着标签、置信度和预测结果。
4. 结果保存
python
mmcls.save(img,'new_cat.jpg') # 保存推理结果图片
save()方法能够保存推理后的结果图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
MMEdu的目标检测模型
代码样例
python
from XEdu.hub import Workflow as wf
mmdet = wf(task='mmedu',checkpoint='plate.onnx') # 指定使用的onnx模型
result, result_img =  mmdet.inference(data='data/plate0.png',img_type='pil') # 进行模型推理
format_result = mmdet.format_output(lang='zh') # 推理结果格式化输出
mmdet.show(result_img) # 展示推理结果图片
mmdet.save(result_img,'new_plate.jpg') # 保存推理结果图片
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
mmdet = wf(task='mmedu',checkpoint='plate.onnx') # 指定使用的onnx模型
wf()中共有两个参数可以设置：
task：只需要设置task为mmedu ，而不需要指定是哪种任务。
checkpoint：指定你的模型的路径，如checkpoint='plate.onnx'。
这里以车牌识别为例进行说明。项目指路：使用MMEdu实现车牌检测
2. 模型推理
python
result, result_img =  mmdet.inference(data='data/plate0.png',img_type='pil') # 进行模型推理
模型推理inference()可传入参数：
data：指定待检测的图片。
show: 可取值：[True,False] 默认为False。如果取值为True，在推理完成后会直接输出目标检测完成后的图片。
img_type：目标检测完成后会返回含有检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
thr(float): 设置检测框阈值，取值范围为[0,1]，预测结果的置信度高于这个阈值时，这些结果才会被认为是有效的。
result的结果是一个数组，里面保存了结果字典。该字典有四个键：标签、置信度、坐标以及预测结果。其中坐标表示了检测框的两个顶点：左上(x1,y1)和右下(x2,y2)。
result_img以pil格式保存了模型推理完成后的图片（原图+推理结果）。
3. 结果输出
python
format_result = mmdet.format_output(lang='zh') # 推理结果格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
[{'标签': 0,
  '置信度': 0.63597506,
  '坐标': {'x1': 358, 'y1': 476, 'x2': 680, 'y2': 580},
  '预测结果': 'plate'}]
```
format_output的结果是一个数组，里面保存了结果字典。该字典有四个键：标签、置信度、坐标以及预测结果。其中坐标表示了检测框的两个顶点：左上(x1,y1)和右下(x2,y2)。
python
mmdet.show(result_img) # 展示推理结果图片
show()能够推理后的结果图像。与原图相比，结果图片还包含车牌周围的检测框以及结果信息。
4. 结果保存
python
mmdet.save(img,'new_plate.jpg') # 保存推理结果图片
save()方法能够保存推理后的结果图像
该方法接收两个参数，一个是图像数据，另一个是图像的保存路径。
9. BaseNN模型推理
XEduHub现在可以支持使用BaseNN导出的onnx模型进行推理啦！如果你想了解如何将使用BaseNN训练好的模型转换成ONNX格式，可以看这里：BaseNN模型文件格式转换。OK，准备好了ONNX模型，那么就开始使用XEduHub吧！
代码样例
```python
使用BaseNN训练的手写数字识别模型进行推理
from XEdu.hub import Workflow as wf
basenn = wf(task='basenn',checkpoint='basenn.onnx') # 指定使用的onnx模型
result = base.inference(data='data/6.jpg') # 进行模型推理
format_result = basenn.format_output()
```
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
basenn = wf(task='basenn',checkpoint='basenn.onnx') # 指定使用的onnx模型
wf()中共有两个参数可以设置：
task：只需要设置task为basenn ，而不需要指定是哪种任务。
checkpoint：指定你的模型的路径，如checkpoint='basenn.onnx'。
2. 模型推理
python
result = base.inference(data='data/6.jpg') # 进行模型推理
模型推理inference()可传入参数：
data：指定待检测的图片。
result的结果是一个二维数组，第一个元素表示这张图属于0-9的数字类别的概率，可以看到当为0时，概率接近，因此该手写数字是0。
```
输出结果
[array([[1.0000000e+00, 1.2138132e-24, 1.7666091e-10, 1.8000461e-19,
         1.5096989e-24, 1.4399931e-17, 1.0446696e-16, 3.8026964e-19,
         2.8802003e-18, 1.3136030e-15]], dtype=float32)]
```
注意！基于BaseNN模型推理结果不包含图片！不需要指定img_type参数并返回图片，因为大部分使用BaseNN解决的任务只需要输出分类标签、文本或者数组数据等。
3. 结果输出
python
format_result = basenn.format_output()
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
{0: {'预测值': 0, '分数': 1.0}}
```
format_output的结果是一个结果字典，这个字典的第一个元素有两个键，预测值、分数，代表着该手写数字的分类标签以及属于该分类标签的概率。
10. BaseML模型推理
XEduHub现在可以支持使用BaseML导出的pkl模型文件进行推理啦！如果你想了解如何将使用BaseML训练模型并保存成.pkl模型文件，可以看这里：BaseML模型保存。OK，准备好了pkl模型，那么就开始使用XEduHub吧！
代码样例
```python
使用BaseML训练的鸢尾花聚类模型推理
from XEdu.hub import Workflow as wf
baseml = wf(task='baseml',checkpoint='baseml.pkl') # 指定使用的pkl模型
data = [[5.1,1.5],[7,4.7]] # 该项目中训练数据只有两维，因此推理时给出两维数据
result= baseml.inference(data=data) # 进行模型推理
format_output = baseml.format_output(lang='zh') # 推理结果格式化输出
```
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
baseml = wf(task='baseml',checkpoint='baseml.pkl') # 指定使用的pkl模型
wf()中共有两个参数可以设置：
task：只需要设置task为baseml ，而不需要指定是哪种任务。
checkpoint：指定你的模型的路径，如checkpoint='baseml.pkl'。
2. 模型推理
python
data = [[5.1,1.5],[7,4.7]] # 该项目中训练数据只有两维，因此推理时给出两维数据
result= baseml.inference(data=data) # 进行模型推理
mmdet.inference可传入参数：
data：指定待推理数据（数据类型和格式跟模型训练有关）。
注意！基于BaseML模型推理结果不包含图片，不需要指定img_type参数并返回图片，因为大部分使用BaseML解决的任务只需要输出分类标签、文本或者数组数据等。
result的结果是一个一维数组，里面保存了模型推理结果。
```
result
array([1，0])
```
3. 结果输出
python
format_output = baseml.format_output(lang='zh')# 推理结果格式化输出
format_output()能够将模型推理结果以标准美观的方式进行输出。输出结果与format_result保存的内容一致。
format_output()中共有两个参数可以设置：
lang(string) - 可选参数，设置了输出结果的语言，可选取值为：['zh','en','ru','de','fr']，分别为中文、英文、俄语、德语、法语，默认为中文。
isprint(bool) - 可选参数，设置了是否格式化输出，可选取值为：[True,False]，默认为True。
```
输出结果
{'预测值': array([1，0])}
```
format_result以字典形式保存了模型的推理结果，由于使用的是聚类模型，输出结果为这两个特征数据所对应的聚类标签。
如果此时你有冲动去使用BaseML完成模型训练到推理，再到转换与应用，快去下文学习BaseML的相关使用吧！
11. 其他onnx模型推理
XEduHub现在可以支持使用用户自定义的ONNX模型文件进行推理啦！这意味着你可以不仅仅使用MMEdu或者BaseNN训练模型并转换而成的ONNX模型文件进行推理，还可以使用其他各个地方的ONNX模型文件，但是有个重要的前提：你需要会使用这个模型，了解模型输入的训练数据以及模型的输出结果。OK，如果你已经做好了充足的准备，那么就开始使用XEduHub吧！
代码样例
```python
from XEdu.hub import Workflow as wf
import cv2
import numpy as np
custom = wf(task="custom",checkpoint="custom.onnx")
def pre(path): # 输入数据（此处为文件路径）前处理的输入参数就是模型推理时的输入参数，这里是图片的路径
    """
    这个前处理方法实现了将待推理的图片读入并进行数字化，调整数据类型、增加维度、调整各维度的顺序。
    """
    img = cv2.imread(path) # 读取图像
    img = img.astype(np.float32) # 调整数据类型
    img = np.expand_dims(img,0) # 增加batch维
    img = np.transpose(img, (0,3,1,2)) # [batch,channel,width,height]
    return img # 输出前处理过的数据（此处为四维numpy数组）
def post(res,data): # 输入推理结果和前处理后的数据
    """
    这个后处理方法实现了获取并返回推理结果中置信度最大的类别标签。
    """
    res = np.argmax(res[0]) # 返回类别索引
    return res # 输出结果
result = custom.inference(data='det.jpg',preprocess=pre,postprocess=post)
print(result)
```
代码解释
1. 模型声明
python
from XEdu.hub import Workflow as wf
custom = wf(task="custom",checkpoint="custom.onnx")
wf()中共有两个参数可以设置：
task：只需要设置task为custom ，而不需要指定是哪种任务。
checkpoint：指定你的模型的路径，如checkpoint='custom.onnx'。
2. 模型推理
```python
import cv2
import numpy as np
custom = wf(task="custom",checkpoint="custom.onnx")
def pre(path): # 输入数据（此处为文件路径）前处理的输入参数就是模型推理时的输入参数，这里是图片的路径
    """
    这个前处理方法实现了将待推理的图片读入并进行数字化，调整数据类型、增加维度、调整各维度的顺序。
    """
    img = cv2.imread(path) # 读取图像
    img = img.astype(np.float32) # 调整数据类型
    img = np.expand_dims(img,0) # 增加batch维
    img = np.transpose(img, (0,3,1,2)) # [batch,channel,width,height]
    return img # 输出前处理过的数据（此处为四维numpy数组）
def post(res,data): # 输入推理结果和前处理后的数据
    """
    这个后处理方法实现了获取并返回推理结果中置信度最大的类别标签。
    """
    res = np.argmax(res[0]) # 返回类别索引
    return res # 输出结果
```
在这里，使用自定义的ONNX模型进行推理的时候，你需要自己的需求实现模型输入数据的前处理以及输出数据的后处理方法，确保在进行模型推理的正常运行。
举一个例子，如果你手中有一个onnx模型文件，这个模型是一个目标检测模型，在训练时的训练数据是将图片读入后进行数字化处理得到的numpy数组，那么你在使用XEduHub时，基于该模型进行推理之前，需要设计对应的前处理方法将图片进行数字化。
同样地，如果你的模型的输出结果是一个一维数组，里面包含了所有类别标签对应的置信度，那么如果你想要输出检测出来的最有可能的类别标签，你就需要设计后处理方法，使得输出的推理结果满足你的需要。
以上是前处理和后处理方法的代码示例，以前文提到的目标检测为例。
在定义好了前处理和后处理函数之后，就可以进行模型推理了！记得要传入前后处理函数的名称到模型参数中。
```python
result = custom.inference(data='det.jpg',preprocess=pre,postprocess=post)
print(result
812
```
模型推理inference()可传入参数：
data：指定待检测的图片
preprocess: 指定前处理函数
postprocess：指定后处理函数
XEduHub任务模型文件获取与存放
XEduHub提供了大量优秀的任务模型，我们不仅可以通过wf()代码的运行实现模型的自动下载，还可以自主通过浦源平台（上海AI实验室的另一个平台）下载。
只要进入模型仓库，在Model File里就可以看到各种任务模型。网址：https://openxlab.org.cn/models/detail/xedu/hub-model
没有网络，如何让代码wf()运行时找到找到模型文件呢？
在没有指定模型路径checkpoints参数的情况下，wf()运行时会先检查是否已下载了对应任务的模型，检查的顺序如下：
本地的同级目录的checkpoints文件夹中，与任务同名的模型文件。
本地缓存中。
如果都没有，就会到网络上下载。
因此，无论是网络下载还是自己训练的模型使用，有两种解决思路：
在本地同级目录中新建checkpoints文件夹，将模型存放在该文件夹中。
使用参数checkpoint，指定模型路径，如model=wf(task='pose_body17',checkpoint='my_path/pose_body17.onnx')`
最后提醒一下，自己到网络下载或自己训练的模型需要是ONNX格式。ONNX是由微软、亚马逊 、Facebook 和 IBM 等公司共同开发的开放神经网络交换格式，即Open Neural Network Exchange，兼容性比较好。

XEduHub功能详解
XEduHub是什么？
XEduHub是一个专为快速、便捷地利用最先进的深度学习模型完成任务而设计的工具库。其设计灵感源自PyTorchHub，旨在以工作流的方式，高效地完成深度学习任务。XEduHub的独特之处在于它内置了大量优质的深度学习SOTA模型，无需用户自行进行繁琐的模型训练。用户只需将这些现成的模型应用于特定任务，便能轻松进行AI应用实践。
XEduHub有多棒？
简单易用：就像玩玩具一样，不需要专业知识，只要按照指导，你就可以使用这些AI模型。
无需训练：你不需要自己制作玩具，里面的AI模型都已经为你准备好了。
节省时间：不需要等待，使用XEduHub，选取你需要的模型，然后就可以开始你的AI之旅。
解锁XEduHub的使用方法
XEduHub作为一个深度学习工具库，集成了许多深度学习领域优质的SOTA模型，能够帮助用户在不进模型训练的前提下，用少量的代码，快速实现计算机视觉、自然语言处理等多个深度学习领域的任务。
一般使用步骤是：
步骤1：安装并导入XEduHub库
步骤2：选择你的AI玩具
步骤3：使用AI玩具
有了模型，你就可以使用它来完成你的任务啦！
```python
步骤一：导入库
from XEdu.hub import Workflow as wf
步骤二：选择你的AI玩具
face = wf(task="face") # 实例化模型
步骤三：使用你的AI玩具
img = 'face.jpg'
进行推理，同时返回结果和带标注的图片
result,new_img = face.inference(data=img,img_type='cv2')
print(result) # 输出推理结果
face.show(new_img) # 显示带标注图片
```
一旦你安装好XEduHub并导入到代码中后，你就可以查看里面所有的AI模型。看看哪一个是你想要的，然后选择它！下文会为你分任务解读。示例代码如下：
```python
from XEdu.hub import Workflow as wf
目前支持的任务
wf.support_task()
```
内置任务
XEduHub内置多个深度学习领域优质的SOTA模型，支持多种类型的内置任务。
写在前面：为了更好地兼容每个版本的任务名称，我们建立了一个任务写法的映射表。
下表每一行列出的写法都是等价的，例如你要声明一个检测人体17个关键点的模型，你可以给task参数传入body, body17或者pose_body17，当然推荐规范写法是pose_body17。
可用写法
推荐规范写法
body
pose_body17
body17
pose_body17
body26
pose_body26
pose_hand
pose_hand21
pose_body
pose_body17
pose_wholebody
pose_wholebody133
pose_face
pose_face106
方向一：关键点识别
关键点识别是深度学习中的一项关键任务，旨在检测图像或视频中的关键位置，通常代表物体或人体的重要部位。
1. 模型声明
在第一次声明模型时代码运行用时较长，是因为要将预训练模型从云端下载到本地中，从而便于用户进行使用。
你可以在当前项目中找到名为checkpoints的文件夹，里面保存的就是下载下来的预训练模型。当代码运行时，会先在本地的同级目录中寻找是否有已下载的预训练模型，如果没有，到本地缓存中寻找，如果本地缓存没有，查看是不是指定了模型的路径，如果都没有，到网络下载。
人体关键点
人体关键点识别是一项计算机视觉任务，旨在检测和定位图像或视频中人体的关键位置，通常是关节、身体部位或特定的解剖结构。
这些关键点的检测可以用于人体姿态估计和分类、动作分析、手势识别等多种应用。
XEduHub提供了两个识别人体关键点的优质模型，能够在使用cpu推理的情况下，快速识别出身体的关键点。
body17和body26 数字表示了识别出人体关键点的数量。
声明代码如下：
python
from XEdu.hub import Workflow as wf
body = wf(task='body') # 数字可省略，当省略时，默认为body17
body17模型能识别出17个人体骨骼关键点，示意图如下，你可以根据自己的需要选择其中的特定的关键点进行后续的处理。
body26模型能识别出26个人体骨骼关键点，与body17相比，示意图如下，你可以根据自己的需要选择其中的特定的关键点进行后续的处理。
人脸关键点
人脸关键点识别是计算机视觉领域中的一项任务，它的目标是检测和定位人脸图像中代表面部特征的重要点，例如眼睛、鼻子、嘴巴、眉毛等。这些关键点的准确定位对于许多应用非常重要，包括人脸识别、表情分析、虚拟化妆、人机交互等。
XEduHub提供了识别人脸关键点的模型：face106，这意味着该模型能够识别人脸上的106个关键点。如下图所示是106个关键点在脸部的分布情况，我们可以利用这些关键点的分布特征进行人脸识别，或者对人的表情进行分析和分类等。
声明代码如下：
python
from XEdu.hub import Workflow as wf
face = wf(task='face') # 数字可省略，默认为face106
人手关键点
人手关键点识别是一项计算机视觉任务，其目标是检测和定位图像或视频中人手的关键位置，通常包括手指、手掌、手腕等关键部位的位置。这些关键点的识别对于手势识别、手部姿态估计、手部追踪、手势控制设备等应用具有重要意义。
XEduHub提供了能够快速识别人手关键点的模型：hand21，该模型能够识别人手上的21个关键点，如下图所示。你可以根据自身需要对关键点进行进一步处理。例如：手势的不同会体现在关键点位置的分布上，这样就可以利用这些关键点进行手势的分类和识别。
声明代码如下：
python
from XEdu.hub import Workflow as wf
hand = wf(task='hand') # 数字可省略，默认为hand21
人体所有关键点
XEduHub提供了识别人体所有关键点，包括人手、人脸和人体躯干部分关键点的模型：wholebody133。具体关键点的序号及其分布如下图所示：
声明代码如下：
python
from XEdu.hub import Workflow as wf
wholebody = wf(task='wholebody') # 数字可省略，默认为wholebody133
2. 模型推理
由于已经从云端下载好了预训练的SOTA模型，因此只需要传入相应图片即可进行模型推理任务，识别相应的关键点，以人体关键点识别为例，模型推理代码如下：
python
img = "data/body.jpg" # 指定待识别关键点的图片的路径
keypoints,img_with_keypoints = body.inference(data=img,img_type='pil') # 进行模型推理
keypoints以三维数组的形式保存了所有关键点的坐标，每个关键点(x,y)被表示为[x,y]根据前面的图示，要获取到某个特定序号i的关键点，只需要访问keypoints[0][i]即可。
img_with_keypoints是个三维数组，以pil格式保存了关键点识别完成后的图片。
inference()可传入参数：
data: 指定待识别关键点的图片。
show: 可取值：[true,false] 默认为false。如果取值为true，在推理完成后会直接输出关键点识别完成后的图片。
img_type: 关键点识别完成后会返回含有关键点的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
bbox：该参数可配合目标检测使用。在多人关键点检测中，该参数指定了要识别哪个检测框中的关键点。
3. 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看关键点坐标以及分数（可以理解为置信度），代码如下：
python
format_result = body.format_output(lang='zh')# 参数language设置了输出结果的语言
format_result以字典形式存储了推理结果，共有两个键：关键点坐标和分数。关键点坐标以二维数组形式保存了每个关键点的[x,y]坐标，而分数则是对应下标的关键点的置信度，以一维数组形式保存。
置信度是机器学习中常用的概念，描述的是模型对于单个预测的确定程度，反映了模型对该预测的信心强度。
显示带有关键点和关键点连线的结果图像
python
body.show(img_with_keypoints)
4. 结果保存
XEduHub提供了保存带有关键点和关键点连线结果图像的方法，代码如下：
python
body.save(img_with_keypoints,'img_with_keypoints.jpg')
5.完整代码
python
from XEdu.hub import Workflow as wf
body = wf(task='body') # 数字可省略，当省略时，默认为body17
img = "data/body.jpg" # 指定待识别关键点的图片的路径
keypoints,img_with_keypoints = body.inference(data=img,img_type='pil') # 进行模型推理
format_result = body.format_output(lang='zh')# 参数language设置了输出结果的语言
body.show(img_with_keypoints)
body.save(img_with_keypoints,'img_with_keypoints.jpg')
方向二：目标检测
目标检测是一种计算机视觉任务，其目标是在图像或视频中检测并定位物体的位置，并为每个物体分配类别标签。
实现目标检测通常包括特征提取、物体位置定位、物体类别分类等步骤。这一技术广泛应用于自动驾驶、安全监控、医学影像分析、图像搜索等各种领域，为实现自动化和智能化应用提供了关键支持。
1. 模型声明
在第一次声明模型时代码运行用时较长，是因为要将预训练模型从云端下载到本地中，从而便于用户进行使用。
你可以在当前项目中找到名为checkpoints的文件夹，里面保存的就是下载下来的预训练模型。
人体目标检测
人体目标检测的任务是在图像或视频中检测和定位人体的位置，并为每个检测到的人体分配一个相应的类别标签。
XEduHub提供了进行人体目标检测的模型：bodydetect，该模型能够进行单人的人体目标检测。
声明代码如下：
python
from XEdu.hub import Workflow as wf
body_det = wf(task='bodydetect')
coco目标检测
COCO（Common Objects in Context）是一个用于目标检测和图像分割任务的广泛使用的数据集和评估基准。它是计算机视觉领域中最重要的数据集之一，在XEduHub中的该模型能够检测出80类coco数据集中的物体：cocodetect，声明代码如下:
python
from XEdu.hub import Workflow as wf
coco_det = wf(task='cocodetect')
若要查看coco目标检测中的所有类别可运行以下代码：
python
wf.coco_class()
人脸检测
人脸检测指的是检测和定位一张图片中的人脸。XEduHub使用的是opencv的人脸检测模型，能够快速准确地检测出一张图片中所有的人脸。
需要注意的是由于使用的为opencv的人脸检测模型，因此在format_output时缺少了分数这一指标。
声明代码如下：
python
from XEdu.hub import Workflow as wf
face_det = wf(task='facedetect')
手部检测
手部检测指的是检测和定位一张图片中的人手。XEduHub采用的是MMPose框架中rtmpose中的手部检测模型，能够快速准确地检测出图片中的所有人手
声明代码如下：
python
from XEdu.hub import Workflow as wf
hand_det = wf(task="handdetect")
2. 模型推理
由于已经从云端下载好了预训练的SOTA模型，因此只需要传入相应图片即可进行模型推理任务，实现目标检测。以人体目标检测为例，模型推理代码如下：
python
img = 'data/body.jpg'
result,img_with_box = body_det.inference(data=img,img_type='cv2')
result以二维数组的形式保存了检测框左上角顶点的(x,y)坐标以及检测框的宽度w和高度h（之所以是二维数组，是因为该模型能够检测多个人体，因此当检测到多个人体时，就会有多个[x,y,w,h]的一维数组，所以需要以二维数组形式保存），我们可以利用这四个数据计算出其他三个顶点的坐标。
img_with_box是个三维数组，以cv2格式保存了包含了检测框的图片。
body_det.inference()可传入参数：
data：指定待检测的图片。
show: 可取值：[true,false] 默认为false。如果取值为true，在推理完成后会直接输出目标检测完成后的图片。
img_type：目标检测完成后会返回含有检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
target_class：该参数在使用cocodetect的时候可以指定要检测的对象，如：person，cake等等。
thr: 设置检测框阈值，超过该阈值的检测框被视为有效检测框，进行显示。
3. 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看检测框位置信息、检测分数以及目标的分类类别。
format_result以字典形式存储了推理结果，共有三个键：检测框、分数和类别。检测框以二维数组形式保存了每个检测框的坐标信息[x,y,w,h]，而分数则是对应下标的检测框的置信度，以一维数组形式保存，类别则是检测框中对象所属的类别，以一维数组形式保存。
代码如下：
python
format_result =body_det.format_output(lang='zh')# 参数language设置了输出结果的语言
显示带有检测框的图片
python
body_det.show(img_with_box)
4. 结果保存
XEduHub提供了保存带有检测框图片的方法，代码如下：
python
body_det.save(img_with_box,'img_with_box.jpg')
5. 完整代码
python
from XEdu.Hub import Workflow as wf
body_det = wf(task='bodydetect')
img = 'data/body.jpg'
result,img_with_box = body_det.inference(data=img,img_type='cv2')
format_result =body_det.format_output(lang='zh')# 参数language设置了输出结果的语言
body_det.show(img_with_box)
body_det.save(img_with_box,'img_with_box.jpg')
方向三：光学字符识别（OCR）
光学字符识别（Optical Character Recognition, OCR）是一项用于将图像或扫描的文档转换为可编辑的文本格式的技术。
OCR技术能够自动识别和提取图像或扫描文档中的文本，并将其转化为计算机可处理的文本格式。
OCR技术在车牌识别、证件识别、文档扫描、拍照搜题等多个场景有着广泛应用。
1. 模型声明
XEduHub使用的OCR模型是来自百度的开源免费的OCR模型：rapidocr，这个模型运行速度快，性能优越，小巧灵活，并且能支持超过6000种字符的识别，如简体中文、繁体中文、英文、数字和其他艺术字等等。
注意：你可以在当前项目中找到名为font的文件夹，里面的FZVTK.TTF文件是一种字体文件，为了显示识别出的文字而使用。
声明代码如下：
python
from XEdu.hub import Workflow as wf
ocr = wf(task="ocr")
2. 模型推理
只需要传入相应图片即可进行字符识别。模型推理代码如下：
python
img = 'data/ocr.jpg'
result,ocr_img = ocr.inference(data=img,img_type='cv2')
result以一维数组的形式保存了识别出的文本及其检测框的四个顶点(x,y)坐标.
如图所示，数组中每个元素的形式为元组：（识别文本，检测框顶点坐标）。四个顶点坐标顺序分别为[左上，右上，左下，右下]。
ocr_img的格式为cv2，如下图所示
ocr.inference()可传入参数：
data：指定待识别的图片。
show: 可取值：[true,false] 默认为false。如果取值为true，在推理完成后会直接输出OCR完成后的图片。
img_type：目标检测完成后会返回含有检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']
3. 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看检测框位置信息、分数以及识别出的文本。
format_output的结果以字典形式存储了推理结果，共有三个键：检测框坐标、分数和文本。检测框坐标以三维数组形式保存了每个检测框的四个顶点的[x,y]坐标，而分数则是对应下标的检测框的置信度，以一维数组形式保存。文本则是每个检测框中识别出的文本，以一维数组形式保存。
代码如下：
python
ocr_format_result = ocr.format_output(lang="zh")
显示结果图片：由两部分组成，左侧为原图片，右侧为经过ocr识别出的文本，并且该文本的位置与原图片中文本的位置保持对应。
python
ocr.show(ocr_img)
4. 结果保存
XEduHub提供了保存OCR识别后的图片的方法，代码如下：
python
ocr.save(ocr_img,"ocr_img.jpg")
5. 完整代码
python
from XEdu.Hub import Workflow as wf
ocr = wf(task="ocr")
img = 'data/ocr.jpg'
result,ocr_img = ocr.inference(data=img,img_type='cv2')
ocr_format_result = ocr.format_output(lang="zh")
ocr.show(ocr_img)
ocr.save(ocr_img)
外置任务
XEduHub除了内置多个深度学习领域优质的SOTA模型，支持多种类型的内置任务，同时也支持指定外置任务，如MMEdu、BaseNN。
基于MMEdu导出模型推理
XEduHub现在可以支持使用MMEdu导出的onnx模型进行推理啦！如果你想了解如何使用MMEdu训练模型，可以看这里：解锁图像分类模块：MMClassification、揭秘目标检测模块：MMDetection。
如果你想了解如何将使用MMEdu训练好的模型转换成ONNX格式，可以看这里最后一步：AI模型转换。OK，准备好了ONNX模型，那么就开始使用XEduHub吧！
1. 模型声明
与外置任务的模型声明不同之处在于：task和checkpoint的设置。首先，你只需要设置task为"mmedu"，而不需要指定是哪种任务；其次，你需要指定你的模型的路径，并传入到checkpoint参数。
这里我们以猫狗分类模型为例，项目指路：猫狗分类。
python
from XEdu.hub import Workflow as wf
mmedu = wf(task="mmedu",checkpoint="cat_dogs.onnx")
如果要使用MMDetection，这里以车牌识别为例进行说明。项目指路：使用MMEdu实现车牌检测
python
from XEdu.hub import Workflow as wf
mmdet = wf(task='mmedu',checkpoint='plate.onnx')
2. 模型推理
在完成模型声明后，传入待推理的数据即可完成推理。
以下是使用MMEdu分类模型的推理代码和结果。
python
img = 'cat.jpg'
result, new_img =  mmedu.inference(data=img,img_type="pil",show=True)
以下是使用MMEdu检测模型的推理代码和结果。
python
result = mmdet.inference(data='plate0.png',show=True,img_type='cv2')
mmedu.inference可传入参数：
data：指定待检测的图片。
show: 可取值：[true,false] 默认为false。如果取值为true，在推理完成后会直接输出目标检测完成后的图片。
img_type：目标检测完成后会返回含有检测框的图片，该参数指定了返回图片的格式，可选有:['cv2','pil']，默认值为None，如果不传入值，则不会返回图。
result和img是模型推理后返回的推理结果。
result结果如下图所示:
对于分类模型而言，result的结果是一个一维数组，这代表着每个分类标签的置信度，第一个元素是这张图片为猫的置信度，第二个元素是这张图片为狗的置信度，显然，这张图片为猫的置信度接近100%，自然这张图片被分类为猫。
img结果是打上分类标签和分数的原图片，在这里以数字化的方式（三维数组）呈现。
对于检测模型而言，result的结果是一个三维数组，如下图所示，其中第一个元素[103.20052, 148.52896, 227.04301, 189.76846, 0.58629453]代表着第一个有效检测框的信息。前四个数字，分别对应着左上角(x1,y1)和右下角(x2,y2)检测框的坐标值，最后一个数字代表着这个检测框的置信度。
img结果是打上检测框的原图片，在这里以数字化的方式（三维数组）呈现。
3. 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看输出结果。
以下是MMEdu分类模型的输出结果。
python
format_result = mmedu.format_output(lang="zh")
format_result以字典形式保存了模型的推理结果，包括所属标签、置信度、以及预测结果。
显示结果图片：与原图相比，结果图片在左上角多了pred_label, pred_socre和pred_class三个数据，对应着标签、置信度和预测结果。
python
mmedu.show(new_img)
以下是MMEdu检测模型的输出结果。
python
format_result = mmdet.format_output(lang='zh')
format_result以字典形式保存了模型的推理结果，包括所属标签、置信度、检测框的坐标信息以及预测结果。
显示结果图片：与原图相比，结果图片还包含车牌周围的检测框以及结果信息。
4. 结果保存
XEduHub提供了保存MMEdu模型推理后的图片的方法，代码如下：
python
mmedu.save(img,'new_cat.jpg')
mmdet.save(img,'new_plate.jpg')
5. 完整代码
```python
基于MMClassification训练出的模型进行推理
from XEdu.hub import Workflow as wf
mmedu = wf(task="mmedu",checkpoint="cat_dogs.onnx")
img = 'cat.jpg'
result, new_img =  mmedu.inference(data=img,img_type="pil",show=True)
format_result = mmedu.format_output(lang="zh")
mmedu.show(new_img)
mmedu.save(new_img,"mmedu_img")
```
基于BaseNN导出模型推理
XEduHub现在可以支持使用BaseNN导出的onnx模型进行推理啦！如果你想了解如何将使用BaseNN训练好的模型转换成ONNX格式，可以看这里：BaseNN模型文件格式转换。OK，准备好了ONNX模型，那么就开始使用XEduHub吧！
1. 模型声明
与MMEdu一样，在模型声明时你只需要设置task为"basenn"，而不需要指定是哪种任务；其次，你需要指定你的模型的路径，并传入到checkpoint参数。
python
from XEdu.hub import Workflow as wf
basenn = wf(task="basenn",checkpoint="basenn.pth")
2. 模型推理
python
img = '6.jpg'
result = base.inference(data=img)
result结果如下图所示，是一个一维数组，这代表着每个分类标签的概率。显然可以看到数字为6的标签的置信度最高，是1.0。
mmedu.inference可传入参数：
data：指定待推理数据（数据类型和格式跟模型训练有关）。
注意！基于BaseNN模型推理结果不包含图片！因为大部分使用BaseNN解决的任务只需要输出分类标签、文本或者数组数据等。
3. 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看输出结果。
python
format_output = basenn.format_output(lang='zh')
format_result以字典形式保存了模型的推理结果，包括预测结果，分数（置信度）。
如果此时你有冲动去使用BaseNN完成模型训练到推理，再到转换与应用，快去下文学习BaseNN的相关使用吧！
4. 完整代码
```python
使用BaseNN训练的手写数字识别模型进行推理
from XEdu.hub import Workflow as wf
basenn = wf(task="basenn",checkpoint="basenn.pth")
img = '6.jpg'
result = base.inference(data=img)
format_output = basenn.format_output(lang='zh')
print(format_output)
```
基于BaseML模型推理
XEduHub现在可以支持使用BaseML导出的pkl模型文件进行推理啦！如果你想了解如何将使用BaseML训练模型并保存成.pkl模型文件，可以看这里：BaseML模型保存。OK，准备好了pkl模型，那么就开始使用XEduHub吧！
1. 模型声明
与MMEdu和BaseNN一样，在模型声明时你只需要设置task为"baseml"，而不需要指定是哪种任务；其次，你需要指定你的模型路径，并传入到checkpoint参数。这里我们以鸢尾花聚类项目为例。
python
from XEdu.hub import Workflow as wf
baseml = wf(task='baseml',checkpoint='baseml.pkl')
2. 模型推理
python
data = [[5.1,1.5],[7,4.7]] # 该项目中训练数据只有两维，因此推理时给出两维数据
result= baseml.inference(data=data)
mmdet.inference可传入参数：
data：指定待推理数据（数据类型和格式跟模型训练有关）。
注意！基于BaseML模型推理结果不包含图片！因为大部分使用BaseML解决的任务只需要输出分类标签、文本或者数组数据等。
3. 结果输出
XEduHub提供了一种便捷的方式，能够以标准美观的格式查看输出结果。
python
format_output = baseml.format_output(lang='zh')
format_result以字典形式保存了模型的推理结果，由于使用的是聚类模型，输出结果为这两个特征数据所对应的聚类标签。
如果此时你有冲动去使用BaseML完成模型训练到推理，再到转换与应用，快去下文学习BaseML的相关使用吧！
4. 完整代码
```python
使用BaseML训练的鸢尾花聚类模型推理
from XEdu.hub import Workflow as wf
baseml = wf(task='baseml',checkpoint='baseml.pkl')
data = [[5.1,1.5],[7,4.7]] # 该项目中训练数据只有两维，因此推理时给出两维数据
result= baseml.inference(data=data)
format_output = baseml.format_output(lang='zh')
print(format_output)
```
基于用户自定义ONNX模型推理
XEduHub现在可以支持使用用户自定义的ONNX模型文件进行推理啦！这意味着你可以不仅仅使用MMEdu或者BaseNN训练模型并转换而成的ONNX模型文件进行推理，还可以使用其他各个地方的ONNX模型文件，但是有个重要的前提：你需要会使用这个模型，了解模型输入的训练数据以及模型的输出结果。OK，如果你已经做好了充足的准备，那么就开始使用XEduHub吧！
1. 模型声明
与其他的外置任务一样，在模型声明时你只需要设置task为"custom"，而不需要指定是哪种任务；其次，你需要指定你的模型路径，并传入到checkpoint参数。这里以一个目标检测为例。
python
from XEdu.hub import Workflow as wf
custom = wf(task="custom",checkpoint="custom.onnx")
2. 模型推理
在这里，使用自定义的ONNX模型进行推理的时候，你需要自己的需求实现模型输入数据的前处理以及输出数据的后处理方法，确保在进行模型推理的正常运行。
举一个例子，如果你手中有一个onnx模型文件，这个模型是一个目标检测模型，在训练时的训练数据是将图片读入后进行数字化处理得到的numpy数组，那么你在使用XEduHub时，基于该模型进行推理之前，需要设计对应的前处理方法将图片进行数字化。
同样地，如果你的模型的输出结果是一个一维数组，里面包含了所有类别标签对应的置信度，那么如果你想要输出检测出来的最有可能的类别标签，你就需要设计后处理方法，使得输出的推理结果满足你的需要。
以下是前处理和后处理方法的代码示例，以前文提到的目标检测为例。
```python
import cv2
import numpy as np
def pre(path): # 输入数据（此处为文件路径）前处理的输入参数就是模型推理时的输入参数，这里是图片的路径
    """
    这个前处理方法实现了将待推理的图片读入并进行数字化，调整数据类型、增加维度、调整各维度的顺序。
    """
    img = cv2.imread(path) # 读取图像
    img = img.astype(np.float32) # 调整数据类型
    img = np.expand_dims(img,0) # 增加batch维
    img = np.transpose(img, (0,3,1,2)) # [batch,channel,width,height]
    return img # 输出前处理过的数据（此处为四维numpy数组）
def post(res,data): # 输入推理结果和前处理后的数据
    """
    这个后处理方法实现了获取并返回推理结果中置信度最大的类别标签。
    """
    res = np.argmax(res[0]) # 返回类别索引
    return res # 输出结果
```
在定义好了前处理和后处理函数之后，就可以进行模型推理了！记得要传入前后处理函数的名称到模型参数中。
python
result = custom.inference(data='det.jpg',preprocess=pre,postprocess=post)
print(result)
3. 完整代码
```python
from XEdu.hub import Workflow as wf
import cv2
import numpy as np
custom = wf(task="custom",checkpoint="custom.onnx")
def pre(path): # 输入数据（此处为文件路径）前处理的输入参数就是模型推理时的输入参数，这里是图片的路径
    """
    这个前处理方法实现了将待推理的图片读入并进行数字化，调整数据类型、增加维度、调整各维度的顺序。
    """
    img = cv2.imread(path) # 读取图像
    img = img.astype(np.float32) # 调整数据类型
    img = np.expand_dims(img,0) # 增加batch维
    img = np.transpose(img, (0,3,1,2)) # [batch,channel,width,height]
    return img # 输出前处理过的数据（此处为四维numpy数组）
def post(res,data): # 输入推理结果和前处理后的数据
    """
    这个后处理方法实现了获取并返回推理结果中置信度最大的类别标签。
    """
    res = np.argmax(res[0]) # 返回类别索引
    return res # 输出结果
result = custom.inference(data='det.jpg',preprocess=pre,postprocess=post)
print(result)
```
报错专栏
你是否在使用XEduHub时遇到过报错？是否遇到ERROR时感到无所适从，甚至有点慌张？
没有关系！报错专栏将为你可能在使用过程中出现的错误提供解决方案！
这里收录着使用XEduHub中的常见报错并呈现对应的解决方案，如果你遇到了问题，查看这个专栏，找到类似报错，并且解决它！
正在努力收录中……敬请期待！

在Mind+中使用XEduHub
Mind+中也上线了XEduHub积木块，使用积木也可以玩XEduHub。使用Mind+V1.7.2及以上版本，在python模式用户库中加载此扩展。
Gitee链接：https://gitee.com/liliang9693/ext-xedu-hub
使用说明
第一步：加载积木库
如果联网情况下，打开Mind+用户库粘贴本仓库的链接即可加载：
如果电脑未联网，则可以下载本仓库的文件，然后打开Mind+用户库选择导入用户库，选择.mpext文件即可。
第二步：安装python库
打开库管理，输入xedu-python运行，提示successfully即可。
注：WARNING是提醒，可以忽略；请及时更新xedu-python用户库，以获得更稳定、更强大的模型部署使用体验。
第三步：开始编程！
至此，即可拖动积木块开始快乐编程啦，根据任务类别，可以分为两类：预置任务和通用任务。其中，预置任务指各种内置模型的常见任务，通用任务包含“XEdu”的MMEdu、BaseNN和BaseML等各种工具训练的模型。通用任务也支持其他的ONNX，但前提是需要知道输入的数据格式，需要做前处理。在Mind+中编写预制任务的程序非常简单，例如pose_body任务的运行示例如下：
大家可以举一反三尝试编写各种预制任务的代码，针对MMEdu、BaseNN、BaseML等工具训练及转换并导出的模型，额外将模型文件上传再指定即可。
用这套积木块基本可以完成XEduHub的所有任务，可以做各种小任务，也可以做复杂任务。使用积木完成对一张图片借助XEduHub的相关模型进行人体画面提取、关键点识别，再用BaseNN训练并转换的ONNX模型完成分类模型推理的示例如下。

XEduHub项目案例集
借助XEduHub可以实现应用多元AI模型去解决复杂的问题。
目标检测+关键点检测
以下代码可以将一张图片中所有的手识别出来，并对每一只手提取关键点。这可以提高关键点检测的准确度和实现多目标检测的任务。
具体实现方式为：我们首先使用det_hand进行手目标检测，拿到所有的检测框bbox。随后对每个检测框中的手进行关键点提取。
python
from XEdu.hub import Workflow as wf # 导入库
det  = wf(task='det_hand') # 实例化目标检测模型
hand = wf(task='pose_hand21') # 实例化关键点检测模型
img_path = 'demo/hand3.jpg' # 指定进行推理的图片
bboxs = det.inference(data=img_path) # 目标检测推理
display_img = img_path # 初始化用于显示的图像
for i in bboxs:
    keypoints,display_img = hand.inference(data=display_img,img_type='cv2',bbox=i) # 关键点检测推理
hand.show(display_img) # 结果可视化
实时人体关键点识别
以下代码可以实时检测摄像头中出现的多个人，并对每一个人体提取关键点。
具体实现方式为：我们首先将实时视频中每一帧的图像进行人体目标检测，拿到所有的检测框bbox及其坐标信息，绘制检测框。随后对每个检测框中的人体进行关键点提取。
python
from XEdu.hub import Workflow as wf
import cv2
cap = cv2.VideoCapture(0)
body = wf(task='pose_body17')# 实例化pose模型
det = wf(task='det_body')# 实例化detect模型
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    bboxs = det.inference(data=frame,thr=0.3)
    img = frame
    for i in bboxs:
        keypoints,img =body.inference(data=img,img_type='cv2',bbox=i)
    for [x1,y1,x2,y2] in bboxs: # 画检测框
        cv2.rectangle(img, (int(x1),int(y1)),(int(x2),int(y2)),(0,255,0),2)
    cv2.imshow('video', img)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break    
cap.release()
cv2.destroyAllWindows()
拓展：视频中的人体关键点识别
该项目可以识别视频中出现的人体的关键点。
具体实现方式与上面的代码类似，区别就是从摄像头的实时视频流变成了本地的视频流，对视频每一帧的操作不变。最后，我们还需要将处理好的每一帧的图片再合成为视频。
在这里我们将这个任务分成两步：Step1: 利用关键点识别处理视频的每一帧并保存到本地；Step2: 将本地的视频帧合成为视频。
Step1的代码：
```python
STEP1: 利用关键点识别处理视频的每一帧并保存到本地
import cv2
from XEdu.hub import Workflow as wf
import os
video_path = "data/eason.mp4" # 指定视频路径
output_dir = 'output/' # 指定保存位置
body = wf(task='pose_body17')# 实例化pose模型
det = wf(task='det_body')# 实例化detect模型
cap = cv2.VideoCapture(video_path)
frame_count = 0 # 视频帧的数量
while True:
    ret, frame = cap.read()
    if not ret:
        print('Video read complete!')
        break
    frame_count += 1
    frame_file_name = f'{output_dir}frame_{frame_count:04d}.jpg' # 每一张帧图片的名称
    bboxs = det.inference(data=frame,thr=0.3)
    img = frame
    for i in bboxs:
        keypoints,img =body.inference(data=img,img_type='cv2',bbox=i)
    for [x1,y1,x2,y2] in bboxs: # 画检测框
        cv2.rectangle(img, (int(x1),int(y1)),(int(x2),int(y2)),(0,255,0),2)
    cv2.imshow('video', img)
    body.save(img,frame_file_name)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break    
cap.release()
cv2.destroyAllWindows()
```
Step2的代码：
```python
import cv2
import os
output_video_path = 'output_video.mp4' # 指定合成后视频的名称
output_dir = 'output/' # 指定本地的帧图片的路径
获取推理结果文件列表
result_files = sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.jpg')])
获取第一张图像的尺寸
first_frame = cv2.imread(result_files[0])
frame_height, frame_width, _ = first_frame.shape
设置视频编码器和输出对象
fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 使用 mp4 编码器
out = cv2.VideoWriter(output_video_path, fourcc, 30, (frame_width, frame_height)) # 30 是帧率
print('开始合成视频...')
for image_path in result_files:
    img = cv2.imread(image_path)
    out.write(img)
out.release()
print('视频合成完毕，已保存到：', output_video_path)
```
人脸检测控制舵机方向
以下代码可以运行在Arduino开发板上，实现通过跟随人脸位置来控制舵机方向。具体实现方式为：通过人脸检测模型得到人脸检测框的坐标并计算x轴方向的中心点，根据中心点的位置判断是左转还是右转。通过pinpong库控制舵机的转向。
```python
import cv2
from pinpong.board import Board, Pin, Servo
import numpy as np
from XEdu.hub import Workflow as wf
import time
Board('uno').begin() # 指定Arduino开发板，自动识别COM口
det = wf(task='det_face') # 加载人脸检测模型
ser = Servo(Pin(Pin.D4)) # 初始化舵机，指定舵机接口为D4
cap = cv2.VideoCapture(0) # 打开摄像头
while cap.isOpened():
    ret, frame = cap.read()
    x = 300 # 初始化人脸中心点的x坐标
    if not ret:
        break
    result,img = det.inference(data=frame,img_type='cv2',thr=0.3) # 在CPU上进行推理
    if result is not None and len(result) > 0:
        x = int((result[0][2]+result[0][0])/2) # 计算人脸中心点的x坐标
        print(x)
    if x > 400: # 根据人脸中心点的x坐标控制舵机转动,大于400向左转
        time.sleep(0.05)
        ser.write_angle(0)
        print('left')
    elif x < 200: # 根据人脸中心点的x坐标控制舵机转动，小于200向右转
        time.sleep(0.05)
        ser.write_angle(180)
        print('right')
    cv2.imshow('video', img)
    if cv2.waitKey(1) & 0xFF == ord('q'): # 按q键退出
        break  
cap.release()
cv2.destroyAllWindows()
```
识行小车
我们做了一辆能在驾驶过程中自动阅读交通指令并做出相应运动的智能小车，这辆智能小车具有第一人称视角视角系统，可以将数据发送到服务器进行统一计算处理，根据返回结果执行行进指令。
功能描述：一辆能在驾驶过程中自动阅读交通指令并做出相应运动的小车。
技术实现方式：使用无线摄像头实时获取小车行驶的第一视角，并让一台上位机无线接收图像，再对接收的图像使用XEduHub进行文字识别的离线OCR技术，最后根据识别的文本下发运动指令。
该项目中的小车是JTANK履带车，该小车的无线摄像头和运动指令已经经过封装，由于该小车用的不是普通的ESP32的配置，详细配置请参考JTANK履带车简介。如果你的小车是普通的ESP32，那关于该小车的配置方法可参考科创神器ESP32-CAM小型摄像头模块。识行小车使用的文字识别技术采用的是XEduHub现成模型，模型大小只要10M。通过发送HTTP GET请求来获取视频流和控制小车的运动。上位机的参考代码如下。
```python
导入库
import time
import requests
import cv2
import numpy as np
from XEdu.hub import Workflow as wf
定义控制小车运动函数
def control_car(cmd):
    url = "http://192.168.4.1/state?cmd="#URL，需要根据实际设备或服务器的地址进行修改
    response = requests.get(url + cmd)
    time.sleep(0.3)
    requests.get(url='http://192.168.4.1/state?cmd=S') # 停止URL，需要根据实际设备或服务器的地址进行修改
    if response.status_code == 200:
        print("请求成功！")
        print(response.text)
    else:
        print("请求失败，状态码：", response.status_code)
定义相机流的URL，需要根据实际设备或服务器的地址进行修改
url = 'http://192.168.4.1:81/stream'
发送GET请求来获取视频流
response = requests.get(url, stream=True)
if response.status_code == 200:
    # 使用OpenCV创建一个视频窗口
    cv2.namedWindow('Video', cv2.WINDOW_NORMAL)
    # 实例化模型
    ocr = wf(task='ocr')
    # 设置抽取帧的间隔，例如每25帧抽取一帧
    frame_interval = 25
    frame_count = 0
    max_size=16384 # chunk大小实际情况进行调整 
    # 持续读取视频流，当我们连续获取图像信息就形成了视屏流
    for chunk in response.iter_content(chunk_size=max_size):
        #过滤其他信息，筛选出图像的数据信息
        if len(chunk) >100: 
            if frame_count % frame_interval == 0: # 累计到达相应的帧数对帧进行推理
                # 将数据转换成图像格式
                data = np.frombuffer(chunk, dtype=np.uint8)
                img = cv2.imdecode(data, cv2.IMREAD_COLOR)
                # ocr模型推理
                texts= ocr.inference(data=img) # 进行推理
                # 在窗口中显示图像
                cv2.imshow('Video', img)
                cv2.waitKey(1)
                # 小车控制
                if len(texts)>0:
                    if texts[0][0]=='stop':
                        control_car('S') # 停止
                    elif texts[0][0]=='left':
                        control_car('L') # 左转
                    elif texts[0][0]=='right':
                       control_car('R') # 右转
                    elif texts[0][0]=='go':
                        control_car('F') # 前进
                    elif texts[0][0]=='back':
                        control_car('B') # 后退
            frame_count += 1 
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
关闭窗口和释放资源
cv2.destroyAllWindows()
response.close()
```
此外我们还做了一辆能在驾驶过程中自动跟踪手掌并做出相应运动的小车。
功能描述：一辆能自动跟随手掌，并跟手掌保持一定距离的小车。
技术实现方式：使用无线摄像头实时获取小车行驶的第一视角，并让一台上位机无线接收图像，再对接收的图像使用XEduHub进行目标检测技术，最后根据识别的手掌关键点距离和手与摄像机之间的距离(cm)的映射，以及手掌水平x的位置，来发送运动指令，实现前进、后退、停止、左转和右转的运动。
该项目中的小车是JTANK履带车，该小车的无线摄像头和运动指令已经经过封装，由于该小车用的不是普通的ESP32的配置，详细配置请参考JTANK履带车简介。如果你的小车是普通的ESP32，那关于该小车的配置方法可参考科创神器ESP32-CAM小型摄像头模块。识行小车使用的文字识别技术采用的是XEduHub现成模型，模型大小只要10M。通过发送HTTP GET请求来获取视频流和控制小车的运动。上位机的参考代码如下。
```python
import cv2
from XEdu.hub import Workflow as wf
import math
import numpy as np
import requests
找到手掌间的距离和实际的手与摄像机之间的距离的映射关系
x 代表手掌间的距离(像素距离)，y 代表手和摄像机之间的距离(cm)
x = [300, 245, 200, 170, 145, 130, 112, 103, 93, 87, 80, 75, 70, 67, 62, 59, 57]
y = [20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]
因此我们需要一个类似 y = AX^2 + BX + C 的方程来拟合
coff = np.polyfit(x, y, 2)  #构造二阶多项式方程，coff中存放的是二阶多项式的系数 A,B,C
A, B, C = coff# 拟合的二次多项式的系数保存在coff数组中，即掌间距离和手与相机间的距离的对应关系的系数
requests.get("http://192.168.4.1/state?cmd=T") #低速档位
定义控制小车运动函数
def control_car(cmd):
    url = "http://192.168.4.1/state?cmd="#URL，需要根据实际设备或服务器的地址进行修改
    response = requests.get(url + cmd)
    if response.status_code == 200:
        print("请求成功！")
        print(response.text)
    else:
        print("请求失败，状态码：", response.status_code)
定义相机流的URL，需要根据实际设备或服务器的地址进行修改
url = 'http://192.168.4.1:81/stream'
发送GET请求来获取视频流
response = requests.get(url, stream=True)
if response.status_code == 200:
    # 使用OpenCV创建一个视频窗口
    cv2.namedWindow('Video', cv2.WINDOW_NORMAL)
    # 实例化模型
    detector = wf(task='pose_hand21')
    # 设置抽取帧的间隔，例如每15帧抽取一帧
    frame_interval = 15
    frame_count = 0
    max_size=16384 # chunk大小实际情况进行调整 
    # 持续读取视频流
    for chunk in response.iter_content(chunk_size=max_size):
        #过滤其他信息，筛选出图像的数据信息
        if len(chunk) >100: 
            if frame_count % frame_interval == 0: # 累计到达相应的帧数对帧进行推理
                # 将数据转换成图像格式
                data = np.frombuffer(chunk, dtype=np.uint8)
                img = cv2.imdecode(data, cv2.IMREAD_COLOR)
                # 获取手部关键点信息，绘制关键点和连线后的图像img
                keypoints, img_with_keypoints = detector.inference(data=img, img_type='cv2') # 进行推理
                # 如果检测到手
                if len(keypoints):
                    # 获取食指根部'5'和小指根部'17'的坐标点
                    x1, y1 = keypoints[5] 
                    x2, y2 = keypoints[17]
                    # 勾股定理计算关键点'5'和'17'之间的距离，并变成整型
                    distance = int(math.sqrt((x2-x1)2 + (y2-y1)2))
                    # 得到像素距离转为实际cm距离的公式 y = Ax^2 + Bx + C
                    distanceCM = Adistance2 + Bdistance + C
                    # 勾股定理计算关键点'5'和'17'之间的距离，并变成整型
                    distance = int(math.sqrt((x2-x1)2 + (y2-y1)2))
                    mid=(x1+x2)/2
                else:
                    distanceCM=80
                    mid=150
            print(distanceCM,mid)
            # 小车控制
            if  90 > distanceCM > 70 and 270>mid>50:
                    control_car('S') # 停止
                    print('stop')
            elif distanceCM>=90 and 270>mid>50:
                    control_car('F') # 前进
                    print('go')
            elif distanceCM<=70 and 270>mid>50:
                    control_car('B') # 后退
                    print('back')
            elif 270<mid:
                    control_car('R') # 右转
                    print('right')
            elif 50>mid:
                    control_car('L') # 左转
                    print('left')
            # 在窗口中显示图像
            cv2.imshow('Video', img_with_keypoints)
        frame_count += 1 
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
释放视频资源
cv2.destroyAllWindows()
response.close()
```

快速体验XEduHub
XEduHub是什么？
XEduHub是一个专为快速、便捷地利用最先进的深度学习模型完成任务而设计的工具库。其设计灵感源自PyTorchHub，旨在以工作流的方式，高效地完成深度学习任务。XEduHub的独特之处在于它内置了大量优质的深度学习SOTA模型，无需用户自行进行繁琐的模型训练。用户根据自己的需求，选择不同的特定任务（task），而不同任务对应了不同的内置模型，拿过来就用，轻松进行AI应用实践。
XEduHub的特点
简单易用：XEduhub是个工具箱（有各种螺丝刀、扳手、小刀等），不需要专业知识，只要知道工具的名称和作用，你就可以使用这些AI模型。
无需训练：你不需要自己制作工具，里面就已经有很多常用工具了（为你准备好多AI模型），当然你也可以自己做个新工具放进去。
节省时间：不需要等待，使用XEduHub，选取你需要的模型，然后就可以开始你的AI之旅。
XEduHub的安装和使用
安装方法
pip install XEdu-python或pip install xedu-python
库文件源代码可以从PyPi下载，选择tar.gz格式下载，可用常见解压软件查看源码。
以识别图像中的人体关键点为例，参考代码如下：
```python
步骤一：导入库
from XEdu.hub import Workflow as wf
步骤二：选择你的AI工具
body = wf(task='pose_body17') # 实例化pose模型
步骤三：使用你的AI工具
img = 'body.jpg'
进行推理，同时返回结果和带标注的图片
result,new_img = body.inference(data=img,img_type='cv2')
print(result) # 输出推理结果
body.show(new_img) # 显示带标注图片
```
输出结果：图片加列表，列表如下：
[[ 596.31597222  163.53819444]
 [ 624.65798611  140.86458333]
 [ 576.4765625   149.3671875 ]
 [ 658.66840278  166.37239583]
 [ 553.80295139  177.70920139]
 [ 735.19184028  288.24305556]
 [ 511.28993056  322.25347222]
 [ 871.23350694  387.44010417]
 [ 406.42447917  407.27951389]
 [ 789.04166667  347.76128472]
 [ 369.57986111  381.77170139]
 [ 735.19184028  642.51822917]
 [ 590.64756944  656.68923611]
 [ 831.5546875   832.40972222]
 [ 516.95833333  855.08333333]
 [ 825.88628472 1013.79861111]
 [ 488.61631944 1039.30642361]]
是不是很简单？一般使用步骤是：
步骤1：安装并导入XEduHub库
步骤2：选择你的推理任务
步骤3：输入数据进行推理并得到结果
一旦你安装好XEduHub并导入到代码中后，你就可以查看里面所有的AI模型。看看哪一个是你想要的，然后选择它！查看hub中目前支持的任务代码如下：
```python
from XEdu.hub import Workflow as wf
目前支持的任务
wf.support_task()
```
输出结果：
['pose_body17', 'pose_body17_l', 'pose_body26', 'pose_wholebody133', 'pose_face106', 'pose_hand21', 'det_body', 'det_body_l', 'det_coco', 'det_coco_l', 'det_hand', 'cls_imagenet', 'gen_style', 'nlp_qa', 'drive_perception', 'embedding_image', 'embedding_text', 'gen_color', 'det_face', 'ocr', 'mmedu', 'basenn', 'baseml']
此外我们还可以通过如下语句查看各类任务的文档。注意doc前后是两个下划线。
```python
from XEdu.hub import Workflow as wf
目前支持的任务文档
print(wf.doc)
```
输出结果：
```
Workflow类用于加载预训练模型以解决各类任务。
        目前支持的任务有：
            - pose_body17：人体关键点检测，17个关键点
            - pose_body17_l：人体关键点检测，17个关键点，模型更大
            - pose_body26：人体关键点检测，26个关键点
            - pose_wholebody133：全身关键点检测，包括人体、人脸和手部共133个关键点
            - pose_face106：人脸关键点检测，106个关键点
            - pose_hand21：手部关键点检测，21个关键点
            - det_body：人体检测
            - det_body_l：人体检测，模型更大
            - det_coco：物体检测，80类，基于COCO数据集
            - det_coco_l：物体检测，基于COCO数据集，模型更大
            - det_hand：手部检测
            - det_face：人脸检测
            - cls_imagenet：图像分类，1000类，基于ImageNet数据集
            - gen_style：风格迁移，5种风格
            - nlp_qa：问答系统，基于SQuAD数据集
            - drive_perception：全景驾驶感知系统，包括交通对象检测、可行驶道路区域分割和车道检测任务
            - embedding_image：CLIP图像嵌入
            - embedding_text：CLIP文本嵌入 
            - ocr：光学字符识别，基于rapidocr
            - mmedu：MMEdu模型推理
            - basenn：BaseNN模型推理
            - baseml：BaseML模型推理
    Attributes:
        task：任务类型，可选范围如以上列出。    
        checkpoint：模型权重文件的路径。
        download_path：模型文件即将下载到的路径。
    更多用法及算法详解请参考：https://xedu.readthedocs.io/zh/master/xedu_hub/introduction.html
```
体验XEduhub的更多功能
体验XEduhub的最快速方式是通过OpenInnoLab平台。
OpenInnoLab平台为上海人工智能实验室推出的青少年AI学习平台，满足青少年的AI学习和创作需求，支持在线编程。在“项目”中查看更多，查找“XEduHub”即可找到所有XEduHub相关的体验项目。
XEduHub实例代码合集：https://www.openinnolab.org.cn/pjlab/project?id=65518e1ae79a38197e449843&sc=635638d69ed68060c638f979#public
（用Chrome浏览器打开效果最佳）
用XEduHub玩第一个AI项目！
这里给出一段最简单的XEduHub示例代码，你运行完这段代码就一定会对XEduHub感到惊叹！
python
from XEdu.hub import Workflow as wf
print(wf.support_task())
body = wf(task='pose_body17') # 实例化pose模型
img = 'body.jpg' # 指定进行推理的图片路径
result,new_img = body.inference(data=img,img_type='cv2',show=True) # 进行推理
print(result)
body.show(new_img)
下面对每行代码做一个拆分讲解
第一步：导入XEduHub库
python
from XEdu.hub import Workflow as wf
第二步：选择你的AI工具
你可以查看里面所有的AI模型。看看哪一个是你想要的，然后选择它！可使用如下代码查看目前支持的任务：
```python
查看目前支持的任务
wf.support_task()
```
现在以提取人体关键点任务为例。此时我们可以选择它！
python
body = wf(task='pose_body17') # 实例化pose模型
第三步：使用AI工具
有了模型，你就可以使用它来完成你的任务啦！继续上面的例子，可以用图片识别模型来完成推理：
```python
img = 'body.jpg' # 指定进行推理的图片路径
result,new_img = body.inference(data=img,img_type='cv2',show=True) # 进行推理
print(result)
[[ 596.31597222  163.53819444]
[ 624.65798611  140.86458333]
[ 576.4765625   149.3671875 ]
[ 658.66840278  166.37239583]
[ 553.80295139  177.70920139]
[ 735.19184028  288.24305556]
[ 511.28993056  322.25347222]
[ 871.23350694  387.44010417]
[ 406.42447917  407.27951389]
[ 789.04166667  347.76128472]
[ 369.57986111  381.77170139]
[ 735.19184028  642.51822917]
[ 590.64756944  656.68923611]
[ 831.5546875   832.40972222]
[ 516.95833333  855.08333333]
[ 825.88628472 1013.79861111]
[ 488.61631944 1039.30642361]]
body.show(new_img)
```
是不是觉得XEduHub很酷呢？现在，你可以轻松地使用各种AI模型，就像玩玩具一样简单。希望你在AI的世界里有个愉快的旅程！

